{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPENlVNYWonA++y9bYA75IY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Les LSTM sont un type de réseau de neurones récurrents (RNN) qui sont particulièrement efficaces pour traiter les séquences de données, comme les séquences de texte ou de temps.\n","\n","La cellule LSTM a trois portes principales qui contrôlent le flux d'informations : la porte d'oubli (forget gate), la porte d'entrée (input gate) et la porte de sortie (output gate).\n","\n","1. **Porte d'oubli (Forget Gate)** :\n","   $$\n","   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n","   $$\n","   Cette porte détermine quelles informations de l'état de la cellule précédente doivent être oubliées.\n","\n","2. **Porte d'entrée (Input Gate)** :\n","   $$\n","   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n","   $$\n","   $$\n","   \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n","   $$\n","   Cette porte détermine quelles nouvelles informations doivent être stockées dans l'état de la cellule.\n","\n","3. **Mise à jour de l'état de la cellule** :\n","   $$\n","   C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n","   $$\n","   Cela met à jour l'état de la cellule en combinant les anciennes informations avec les nouvelles.\n","\n","4. **Porte de sortie (Output Gate)** :\n","   $$\n","   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n","   $$\n","   $$\n","   h_t = o_t \\cdot \\tanh(C_t)\n","   $$\n","   Cette porte détermine quelle partie de l'état de la cellule doit être exposée en tant que sortie.\n","\n","Dans ces équations, $x_t$ représente l'entrée à l'instant $t$, $h_t$ est la sortie à l'instant $t$, $C_t$ est l'état de la cellule à l'instant $t$, et $\\sigma$ représente la fonction sigmoïde. $W$ et $b$ sont les poids et les biais qui sont appris pendant l'entraînement du réseau.\n","\n","Les LSTM sont puissants car ils permettent de conserver et d'utiliser des informations sur de longues séquences, ce qui les rend particulièrement utiles dans des tâches comme la traduction automatique, la génération de texte et d'autres domaines liés au traitement de la langue naturelle."],"metadata":{"id":"CzksUV3vakMC"}},{"cell_type":"markdown","source":["### CustomLSTMCell\n","\n","Cette classe représente une seule cellule LSTM. Chaque cellule LSTM a plusieurs portes (porte d'oubli, porte d'entrée, cellule candidate et porte de sortie) qui régulent le flux d'informations à travers la cellule.\n","\n","1. **`__init__`** : Dans le constructeur, nous définissons les composants de la cellule LSTM. Chaque porte a deux couches linéaires : `W_*` qui traite les entrées et `U_*` qui traite l'état de la cellule précédent.\n","\n","2. **`forward`** : Cette méthode définit comment les opérations sont effectuées lors du passage d'une entrée à travers la cellule LSTM.\n","\n","    - On initialise l'état caché `h_t` et l'état de la cellule `c_t` s'ils ne sont pas fournis.\n","    - On calcule les portes d'oubli `f_t`, d'entrée `i_t`, de cellule candidate `g_t` et de sortie `o_t` en utilisant les couches linéaires et des fonctions d'activation appropriées.\n","    - On met à jour l'état de la cellule `c_t`.\n","    - On met à jour l'état caché `h_t`.\n","    - Enfin, on retourne `h_t` et `c_t`."],"metadata":{"id":"E21IOppubIAu"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np"],"metadata":{"id":"rdlGiKFubfa4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class CustomLSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(CustomLSTMCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # Portes d'oubli\n","        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.U_f = nn.Linear(hidden_size, hidden_size)\n","\n","        # Portes d'entrée\n","        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.U_i = nn.Linear(hidden_size, hidden_size)\n","\n","        # Cellule candidate\n","        self.W_c = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.U_c = nn.Linear(hidden_size, hidden_size)\n","\n","        # Porte de sortie\n","        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.U_o = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, x, init_states=None):\n","        bs, _ = x.size()\n","\n","        h_t, c_t = (torch.zeros(self.hidden_size).to(x.device),\n","                    torch.zeros(self.hidden_size).to(x.device)) if init_states is None else init_states\n","\n","        h_t, c_t = h_t.view(bs, -1), c_t.view(bs, -1)\n","\n","        # Calcul des portes d'oubli\n","        f_t = torch.sigmoid(self.W_f(torch.cat([x, h_t], dim=1)) + self.U_f(c_t))\n","\n","        # Calcul des portes d'entrée\n","        i_t = torch.sigmoid(self.W_i(torch.cat([x, h_t], dim=1)) + self.U_i(c_t))\n","\n","        # Calcul de la cellule candidate\n","        g_t = torch.tanh(self.W_c(torch.cat([x, h_t], dim=1)) + self.U_c(c_t))\n","\n","        # Mise à jour de l'état de la cellule\n","        c_t = f_t * c_t + i_t * g_t\n","\n","        # Calcul de la sortie\n","        o_t = torch.sigmoid(self.W_o(torch.cat([x, h_t], dim=1)) + self.U_o(c_t))\n","\n","        # Mise à jour de l'état caché\n","        h_t = o_t * torch.tanh(c_t)\n","\n","        return h_t, c_t\n","\n"],"metadata":{"id":"Hp-qNntYbHKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CustomLSTM\n","\n","Cette classe encapsule le fonctionnement d'une séquence de cellules LSTM. Elle itère sur les éléments de la séquence en utilisant la cellule LSTM que nous avons définie précédemment.\n","\n","1. **`__init__`** : Le constructeur de la classe initialise les dimensions de l'entrée et de l'état caché, puis crée une instance de `CustomLSTMCell`.\n","\n","2. **`forward`** : Cette méthode définit comment les opérations sont effectuées lors du passage de l'ensemble d'une séquence à travers le LSTM.\n","\n","    - On initialise l'état caché `h_t` et l'état de la cellule `c_t` à zéro.\n","    - On itère sur les éléments de la séquence (`seq_len`) en appliquant la cellule LSTM sur chaque élément. On met à jour `h_t` et `c_t` à chaque itération.\n","    - On retourne la sortie `h_t`."],"metadata":{"id":"SbJ6_w3DbRkw"}},{"cell_type":"code","source":["\n","\n","class CustomLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(CustomLSTM, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.lstm_cell = CustomLSTMCell(input_size, hidden_size)\n","\n","    def forward(self, x):\n","        bs, seq_len, _ = x.size()\n","\n","        h_t, c_t = (torch.zeros(self.hidden_size).to(x.device),\n","                    torch.zeros(self.hidden_size).to(x.device))\n","\n","        h_t, c_t = h_t.view(1,-1).repeat(bs,1), c_t.view(1,-1).repeat(bs,1)\n","\n","        h_t_list = []\n","        for t in range(seq_len):\n","            h_t, c_t = self.lstm_cell(x[:, t, :], (h_t, c_t))\n","            h_t_list.append(h_t.unsqueeze(1))\n","\n","        h_t = torch.cat(h_t_list, dim=1)\n","\n","        return h_t\n","\n"],"metadata":{"id":"Am7OQWbPbURE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Utilisation\n","\n","Pour utiliser cette implémentation personnalisée de LSTM, vous pouvez créer une instance de `CustomLSTM` et l'utiliser dans le processus d'entraînement de votre modèle. Assurez-vous de fournir les dimensions d'entrée et de l'état caché appropriées lors de la création de l'instance.\n"],"metadata":{"id":"LYLrjcMWbcSZ"}},{"cell_type":"code","source":["# Paramètres\n","input_size = 5\n","hidden_size = 8\n","num_layers = 2\n","output_size = 1\n","num_samples = 100\n","sequence_length = 10"],"metadata":{"id":"MxYH3w8YbwDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Générer des séquences aléatoires\n","X = np.random.rand(num_samples, sequence_length, input_size)\n","y = np.sum(X, axis=2)  # La sortie est la somme des entrées\n","\n","# Convertir les données en tenseurs PyTorch\n","X = torch.Tensor(X)\n","y = torch.Tensor(y).unsqueeze(-1)"],"metadata":{"id":"prFHn8q7byJV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Initialiser le modèle et définir la fonction de coût et l'optimiseur\n","model = CustomLSTM(input_size, hidden_size)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Entraînement\n","for epoch in range(100):\n","    optimizer.zero_grad()\n","    outputs = model(X)\n","\n","    loss = criterion(outputs, y)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch+1) % 10 == 0:\n","        print(f'Époque [{epoch+1}/100], Perte : {loss.item():.4f}')\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MfsbkfxbbbVr","executionInfo":{"status":"ok","timestamp":1698482658109,"user_tz":-120,"elapsed":931,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"90fa3a9a-d6c6-469e-aa47-6309c9181b35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([100, 10, 1])) that is different to the input size (torch.Size([100, 10, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Époque [10/100], Perte : 6.4849\n","Époque [20/100], Perte : 6.3146\n","Époque [30/100], Perte : 6.1468\n","Époque [40/100], Perte : 5.9610\n","Époque [50/100], Perte : 5.7067\n","Époque [60/100], Perte : 5.3495\n","Époque [70/100], Perte : 4.9441\n","Époque [80/100], Perte : 4.5781\n","Époque [90/100], Perte : 4.2935\n","Époque [100/100], Perte : 4.0701\n"]}]},{"cell_type":"code","source":["\n","\n","# Définir une classe pour le LSTM sous pytorch\n","class LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n","\n","        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n","\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","model=LSTM()"],"metadata":{"id":"UFzIhcbCbiSf"},"execution_count":null,"outputs":[]}]}