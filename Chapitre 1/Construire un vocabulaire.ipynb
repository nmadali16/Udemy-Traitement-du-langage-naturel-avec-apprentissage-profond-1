{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNogcnsrAj3VVwL4YeffwRv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Création d'un vocabulaire de mot unique:\n","\n","1. **Prendre un texte** : Commencez par choisir le texte que vous souhaitez traiter.\n","\n","2. **Tokenisation** : La tokenisation consiste à diviser le texte en mots ou en tokens. Vous pouvez utiliser des espaces pour cela.\n","\n","   ```python\n","   texte = \"Votre texte ici.\"\n","   tokens = texte.split()\n","   ```\n","\n","3. **Mise en minuscules** : Convertissez tous les tokens en minuscules pour éviter les problèmes de casse.\n","\n","   ```python\n","   tokens = [mot.lower() for mot in tokens]\n","   ```\n","\n","4. **Suppression de la ponctuation** : Éliminez la ponctuation du texte. Vous pouvez utiliser la bibliothèque `string` pour cela.\n","\n","   ```python\n","   # Compiler l'expression régulière pour la ponctuation\n","    regex = re.compile(f\"[{re.escape(string.punctuation)}]\")\n","\n","    # Suppression de la ponctuation\n","    tokens = [regex.sub('', mot) for mot in tokens]\n","   ```\n","5. **Suppression des Stop words** : Les mots vides sont des mots courants qui ne portent pas beaucoup d'information.\n","```python\n","   stop_words = set(stopwords.words('french'))  # Obtenir la liste des mots vides en français\n","\n","  filtered_tokens = [word for word in tokens if word not in stop_words]  # Supprimer les mots vides\n","   ```\n","\n","6. **Stemming** : Le stemming consiste à réduire les mots à leur forme de base (le radical). Vous pouvez utiliser une bibliothèque comme NLTK pour cela.\n","\n","   ```python\n","   from nltk.stem import SnowballStemmer\n","\n","   stemmer = SnowballStemmer('french')\n","   stemmed_tokens = [stemmer.stem(mot) for mot in tokens]\n","   ```\n","\n","6. **Création du vocabulaire** : Créez un vocabulaire à partir des tokens uniques.\n","\n","   ```python\n","   vocabulaire = set(stemmed_tokens)\n","   ```\n","\n"],"metadata":{"id":"fH26DSZeXuZb"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","import string\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ftk47Cy-akaS","executionInfo":{"status":"ok","timestamp":1696166777024,"user_tz":-120,"elapsed":1801,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"5bfe2ada-a4e4-40d4-a91c-1b3afe186b59"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"GbkXpN5FcB5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Étape 1: Prétraitement du texte\n","texte = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n","               the world have come and invaded us, captured our lands, conquered our minds.\n","               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n","               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n","               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n","               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n","               I see four milestones in my career\"\"\"\n","texte = texte.lower()  # Convertir en minuscules\n","# Compiler l'expression régulière pour la ponctuation\n","regex = re.compile(f\"[{re.escape(string.punctuation)}]\")\n","\n","# Suppression de la ponctuation\n","texte = regex.sub('', texte)\n","\n","# Étape 2: Tokenisation\n","tokens = word_tokenize(texte, language='english')  # Tokenisation du texte\n","\n","# Étape 3: Suppression des mots vides\n","stop_words = set(stopwords.words('english'))  # Obtenir la liste des mots vides en français\n","tokens_sans_mots_vides = [word for word in tokens if word not in stop_words]  # Supprimer les mots vides\n","\n","# Étape 4: Stemming\n","stemmer = SnowballStemmer('english')  # Utilisation d'un stemmer pour le français\n","tokens_stemmed = [stemmer.stem(word) for word in tokens_sans_mots_vides]  # Appliquer le stemming\n","\n","# Afficher les résultats\n","print(\"Texte prétraité :\", texte)\n","print(\"Tokens :\", tokens)\n","print(\"Tokens sans mots vides :\", tokens_sans_mots_vides)\n","print(\"Tokens après stemming :\", tokens_stemmed)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3aqlufIwagd_","executionInfo":{"status":"ok","timestamp":1696167201005,"user_tz":-120,"elapsed":6,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"10ec1081-8006-4cee-ee84-245369267169"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Texte prétraité : i have three visions for india in 3000 years of our history people from all over \n","               the world have come and invaded us captured our lands conquered our minds \n","               strong not only as a military power but also as an economic power both must go handinhand \n","               my good fortune was to have worked with three great minds dr vikram sarabhai of the dept of \n","               space professor satish dhawan who succeeded him and dr brahm prakash father of nuclear material\n","               i was lucky to have worked with all three of them closely and consider this the great opportunity of my life \n","               i see four milestones in my career\n","Tokens : ['i', 'have', 'three', 'visions', 'for', 'india', 'in', '3000', 'years', 'of', 'our', 'history', 'people', 'from', 'all', 'over', 'the', 'world', 'have', 'come', 'and', 'invaded', 'us', 'captured', 'our', 'lands', 'conquered', 'our', 'minds', 'strong', 'not', 'only', 'as', 'a', 'military', 'power', 'but', 'also', 'as', 'an', 'economic', 'power', 'both', 'must', 'go', 'handinhand', 'my', 'good', 'fortune', 'was', 'to', 'have', 'worked', 'with', 'three', 'great', 'minds', 'dr', 'vikram', 'sarabhai', 'of', 'the', 'dept', 'of', 'space', 'professor', 'satish', 'dhawan', 'who', 'succeeded', 'him', 'and', 'dr', 'brahm', 'prakash', 'father', 'of', 'nuclear', 'material', 'i', 'was', 'lucky', 'to', 'have', 'worked', 'with', 'all', 'three', 'of', 'them', 'closely', 'and', 'consider', 'this', 'the', 'great', 'opportunity', 'of', 'my', 'life', 'i', 'see', 'four', 'milestones', 'in', 'my', 'career']\n","Tokens sans mots vides : ['three', 'visions', 'india', '3000', 'years', 'history', 'people', 'world', 'come', 'invaded', 'us', 'captured', 'lands', 'conquered', 'minds', 'strong', 'military', 'power', 'also', 'economic', 'power', 'must', 'go', 'handinhand', 'good', 'fortune', 'worked', 'three', 'great', 'minds', 'dr', 'vikram', 'sarabhai', 'dept', 'space', 'professor', 'satish', 'dhawan', 'succeeded', 'dr', 'brahm', 'prakash', 'father', 'nuclear', 'material', 'lucky', 'worked', 'three', 'closely', 'consider', 'great', 'opportunity', 'life', 'see', 'four', 'milestones', 'career']\n","Tokens après stemming : ['three', 'vision', 'india', '3000', 'year', 'histori', 'peopl', 'world', 'come', 'invad', 'us', 'captur', 'land', 'conquer', 'mind', 'strong', 'militari', 'power', 'also', 'econom', 'power', 'must', 'go', 'handinhand', 'good', 'fortun', 'work', 'three', 'great', 'mind', 'dr', 'vikram', 'sarabhai', 'dept', 'space', 'professor', 'satish', 'dhawan', 'succeed', 'dr', 'brahm', 'prakash', 'father', 'nuclear', 'materi', 'lucki', 'work', 'three', 'close', 'consid', 'great', 'opportun', 'life', 'see', 'four', 'mileston', 'career']\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","# Étape 5: Création du vocabulaire et comptage des fréquences\n","vocabulaire = Counter(tokens_stemmed)  # Créer un vocabulaire avec les tokens stemmés et leurs fréquences"],"metadata":{"id":"QwjTXj8RcdHL","executionInfo":{"status":"ok","timestamp":1696167221842,"user_tz":-120,"elapsed":2,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Trouver le token avec la fréquence la plus élevée\n","mot_plus_frequent, frequence = vocabulaire.most_common(1)[0]\n","\n","# Afficher le vocabulaire et le token le plus fréquent\n","print(\"Vocabulaire :\", vocabulaire)\n","print(\"Longeur du vocabulaire :\", len(vocabulaire))\n","print(f\"Le token le plus fréquent est '{mot_plus_frequent}' avec une fréquence de {frequence} fois.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_lo3eLkaiwC","executionInfo":{"status":"ok","timestamp":1696167263251,"user_tz":-120,"elapsed":3,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"e6bda4fe-de48-4e64-cf97-cb26b28799ba"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulaire : Counter({'three': 3, 'mind': 2, 'power': 2, 'work': 2, 'great': 2, 'dr': 2, 'vision': 1, 'india': 1, '3000': 1, 'year': 1, 'histori': 1, 'peopl': 1, 'world': 1, 'come': 1, 'invad': 1, 'us': 1, 'captur': 1, 'land': 1, 'conquer': 1, 'strong': 1, 'militari': 1, 'also': 1, 'econom': 1, 'must': 1, 'go': 1, 'handinhand': 1, 'good': 1, 'fortun': 1, 'vikram': 1, 'sarabhai': 1, 'dept': 1, 'space': 1, 'professor': 1, 'satish': 1, 'dhawan': 1, 'succeed': 1, 'brahm': 1, 'prakash': 1, 'father': 1, 'nuclear': 1, 'materi': 1, 'lucki': 1, 'close': 1, 'consid': 1, 'opportun': 1, 'life': 1, 'see': 1, 'four': 1, 'mileston': 1, 'career': 1})\n","Longeur du vocabulaire : 50\n","Le token le plus fréquent est 'three' avec une fréquence de 3 fois.\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"HHF56y38bK8f"}}]}