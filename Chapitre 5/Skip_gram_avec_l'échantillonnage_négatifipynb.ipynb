{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My6aStird1T0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYj6UgJ5eUR5"
   },
   "source": [
    "# Skip-gram avec l'échantillonnage négatif :\n",
    "Skip-gram est un modèle qui vise à prédire les mots contextuels donnés un mot cible. L'échantillonnage négatif est une technique utilisée pour rendre l'entraînement plus efficace en échantillonnant des exemples \"négatifs\" (mots qui ne font pas partie du contexte) ainsi que les vrais exemples positifs (mots faisant partie du contexte). L'objectif est de maximiser la probabilité d'observer les vrais mots contextuels tout en minimisant la probabilité d'observer des mots négatifs échantillonnés au hasard.\n",
    "\n",
    "Le modèle skip-gram définit deux ensembles de vecteurs de mots :\n",
    "1. Les vecteurs de mots d'entrée (cibles), notés $v(w_i)$ pour le mot cible $w_i$.\n",
    "2. Les vecteurs de mots de sortie (contextuels), notés $u(w_j)$ pour le mot contextuel $w_j$.\n",
    "\n",
    "La probabilité d'observer un mot contextuel $w_j$ étant donné un mot cible $w_i$ est modélisée à l'aide de la fonction sigmoïde :\n",
    "\n",
    "$$\n",
    "P(D = 1 | w_i, w_j) = \\sigma(u(w_j) \\cdot v(w_i))\n",
    "$$\n",
    "\n",
    "Où $\\sigma(x)$ est la fonction sigmoïde :\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Pour entraîner le modèle, on cherche à maximiser la vraisemblance d'observer des mots contextuels lorsque l'on a un exemple positif (un vrai mot contextuel) et à la minimiser pour les exemples négatifs échantillonnés au hasard.\n",
    "\n",
    "La fonction de perte pour un seul exemple positif (mot contextuel) et $K$ exemples négatifs est définie comme suit :\n",
    "\n",
    "$$\n",
    "L = -\\log(P(D = 1 | w_i, w_j)) - \\sum_{k=1}^K \\log(P(D = 0 | w_i, w_k))\n",
    "$$\n",
    "\n",
    "Où $P(D = 0 | w_i, w_k)$ est la probabilité d'observer un mot exemple négatif $w_k$.\n",
    "\n",
    "Optimisation par descente de gradient :\n",
    "Pour optimiser le modèle Word2Vec en utilisant la descente de gradient, vous devez calculer les gradients de la fonction de perte par rapport aux vecteurs de mots $u(w_j)$ et $v(w_i)$, puis mettre à jour ces vecteurs en conséquence.\n",
    "\n",
    "1. Calculez les gradients par rapport à $u(w_j)$ et $v(w_i)$ :\n",
    "   - Gradient pour un exemple positif :\n",
    "\n",
    "     $ \\nabla u(w_j) = \\frac{\\partial L}{\\partial u(w_j)} = (P(D = 1 | w_i, w_j) - 1) \\cdot v(w_i)$\n",
    "     \n",
    "     $\\nabla v(w_i) = \\frac{\\partial L}{\\partial v(w_i)} = (P(D = 1 | w_i, w_j) - 1) \\cdot u(w_j)$\n",
    "\n",
    "   - Gradient pour les exemples négatifs ($w_k$) :\n",
    "\n",
    "     $\\nabla u(w_k) = \\frac{\\partial L}{\\partial u(w_k)} = -P(D = 0 | w_i, w_k) \\cdot v(w_i) $\n",
    "\n",
    "     $\\nabla v(w_i) = \\frac{\\partial L}{\\partial v(w_i)} = -P(D = 0 | w_i, w_k) \\cdot u(w_k)$\n",
    "\n",
    "2. Mettez à jour les vecteurs de mots en utilisant les gradients et un taux d'apprentissage ($\\alpha$) :\n",
    "   - Règle de mise à jour pour $u(w_j)$ et $v(w_i)$ pour un exemple positif :\n",
    "\n",
    "     $u(w_j) \\leftarrow u(w_j) - \\alpha \\cdot \\nabla u(w_j)$\n",
    "\n",
    "     $v(w_i) \\leftarrow v(w_i) - \\alpha \\cdot \\nabla v(w_i)$\n",
    "\n",
    "   - Règle de mise à jour pour $u(w_k)$ et $v(w_i)$ pour les exemples négatifs ($w_k$) :\n",
    "\n",
    "     $u(w_k) \\leftarrow u(w_k) - \\alpha \\cdot \\nabla u(w_k)$\n",
    "\n",
    "     $v(w_i) \\leftarrow v(w_i) - \\alpha \\cdot \\nabla v(w_i)$\n",
    "\n",
    "3. Répétez ce processus sur plusieurs itérations (époques) et sur l'ensemble de votre jeu de données pour entraîner le modèle Word2Vec.\n",
    "\n",
    "La descente de gradient permet au modèle d'apprendre des embeddings de mots qui capturent les relations sémantiques entre les mots dans le corpus donné. Le processus d'optimisation affine les vecteurs de mots pour maximiser la vraisemblance d'observer des mots contextuels tout en minimisant la vraisemblance d'observer des exemples négatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Homn3jwMh8e9"
   },
   "outputs": [],
   "source": [
    "#!bash get_datasets.sh ou #wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1j2YudEgz2H"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.treebank import StanfordSentiment\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9z9uydrlAA4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from utils.gradcheck import gradcheck_naive\n",
    "from utils.utils import normalizeRows, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWyIDcX3hafV"
   },
   "outputs": [],
   "source": [
    "# Réinitialiser la graine aléatoire pour s'assurer que tout le monde obtient les mêmes résultats\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcOiSaZ3i-aq",
    "outputId": "938483be-f377-4d5e-eba1-9a3ddab24f42"
   },
   "outputs": [],
   "source": [
    "print('Longueur du vocabulaire :',nWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hn9fO_y1iDuc",
    "outputId": "166706a4-4bf7-4870-f4d1-cd8d42a1a552"
   },
   "outputs": [],
   "source": [
    "dataset.sentences()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvPYx13kidmU"
   },
   "outputs": [],
   "source": [
    "# Nous allons former des vecteurs à 10 dimensions pour cette mission.\n",
    "dimVectors = 10\n",
    "\n",
    "# Taille du contexte\n",
    "C = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvwlNrTXk5ob"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calcule la fonction sigmoïde pour l'entrée ici.\n",
    "    Arguments :\n",
    "    x -- Un scalaire ou un tableau numpy.\n",
    "    Retourne :\n",
    "    s -- sigmoïde(x)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "\n",
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Fonction de perte et de gradient Softmax naïf pour les modèles word2vec\n",
    "\n",
    "    Implémente la perte naïve softmax et les gradients entre l'intégration d'un mot central et l'intégration d'un mot extérieur.\n",
    "    d'un mot central et l'intégration d'un mot extérieur. Ce sera le bloc de construction\n",
    "    pour nos modèles word2vec.\n",
    "\n",
    "    Arguments :\n",
    "    centerWordVec -- numpy ndarray, intégration du mot central\n",
    "                    (v_c dans le document pdf)\n",
    "    outsideWordIdx -- entier, l'index du mot extérieur\n",
    "                    (o de u_o dans le document pdf)\n",
    "    outsideVectors -- vecteurs extérieurs (lignes de la matrice) pour tous les mots du vocabulaire\n",
    "                      (U dans le document pdf)\n",
    "    dataset -- nécessaire pour l'échantillonnage négatif, inutilisé ici.\n",
    "\n",
    "    Retourne :\n",
    "    loss -- perte naïve softmax\n",
    "    gradCenterVec -- le gradient par rapport au vecteur central du mot\n",
    "                     (dJ / dv_c dans le document pdf)\n",
    "    gradOutsideVecs -- le gradient par rapport à tous les vecteurs de mots extérieurs\n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    y_hat = softmax(np.dot(centerWordVec, outsideVectors.T))\n",
    "    delta = y_hat.copy()\n",
    "    delta[outsideWordIdx] -= 1\n",
    "\n",
    "    loss = -np.log(y_hat)[outsideWordIdx]\n",
    "    gradCenterVec = np.dot(delta, outsideVectors)\n",
    "    gradOutsideVecs = np.dot(delta[:, np.newaxis], centerWordVec[np.newaxis, :])\n",
    "\n",
    "\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "\n",
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Échantillons K index qui ne sont pas l'outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n",
    "\n",
    "\n",
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Fonction de perte d'échantillonnage négative pour les modèles word2vec\n",
    "\n",
    "    Implémenter la perte d'échantillonnage négative et les gradients pour un vecteur de mots centerWordVec\n",
    "    et un vecteur de mots outsideWordIdx en tant que bloc de construction pour les modèles word2vec\n",
    "    pour les modèles de type word2vec. K est le nombre d'échantillons négatifs à prélever.\n",
    "\n",
    "    Remarque : le même mot peut être échantillonné négativement plusieurs fois. Par exemple\n",
    "    Par exemple, si un mot extérieur est échantillonné deux fois, vous devrez\n",
    "    compter deux fois le gradient par rapport à ce mot. Trois fois si\n",
    "    s'il a été échantillonné trois fois, et ainsi de suite.\n",
    "\n",
    "    Arguments/spécifications de retour : les mêmes que pour naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "    gradCenterVec = np.zeros(centerWordVec.shape)\n",
    "    loss = 0.0\n",
    "    #L = -\\log(P(D = 1 | w_i, w_j)) - \\sum_{k=1}^K \\log(P(D = 0 | w_i, w_k))\n",
    "\n",
    "    z = sigmoid(np.dot(outsideVectors[outsideWordIdx], centerWordVec))\n",
    "    loss -= np.log(z)\n",
    "\n",
    "    # \\frac{\\partial L}{\\partial u(w_j)} = (P(D = 1 | w_i, w_j) - 1) \\cdot v(w_i)$\n",
    "\n",
    "    #\\frac{\\partial L}{\\partial v(w_i)} = (P(D = 1 | w_i, w_j) - 1) \\cdot u(w_j)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gradOutsideVecs[outsideWordIdx] += centerWordVec * (z - 1.0)\n",
    "    gradCenterVec += outsideVectors[outsideWordIdx] * (z - 1.0)\n",
    "\n",
    "    #$\\frac{\\partial L}{\\partial u(w_k)} = -P(D = 0 | w_i, w_k) \\cdot v(w_i) $\n",
    "\n",
    "    #$ = \\frac{\\partial L}{\\partial v(w_i)} = -P(D = 0 | w_i, w_k) \\cdot u(w_k)$\n",
    "    u_k = outsideVectors[negSampleWordIndices]\n",
    "    z = sigmoid(-np.dot(u_k,centerWordVec))\n",
    "    loss += np.sum(- np.log(z))\n",
    "    gradCenterVec += np.dot((z-1),u_k)*(-1)\n",
    "    gradOutsideVecs[negSampleWordIndices] += np.outer((z-1),centerWordVec)*(-1)\n",
    "\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "\n",
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Modèle de skip-gram dans word2vec\n",
    "\n",
    "    Implémente le modèle skip-gram dans cette fonction.\n",
    "\n",
    "    Arguments :\n",
    "    currentCenterWord -- une chaîne du mot central actuel\n",
    "    windowSize -- entier, taille de la fenêtre de contexte\n",
    "    outsideWords -- liste de 2*windowSize au maximum, les mots extérieurs\n",
    "    word2Ind -- un dictionnaire qui associe les mots à leurs indices dans la\n",
    "              dans la liste des vecteurs de mots\n",
    "    centerWordVectors -- vecteurs de mots centraux (en tant que lignes) pour tous les mots du vocabulaire\n",
    "                        (V dans le document pdf)\n",
    "    outsideVectors -- vecteurs de mots extérieurs (sous forme de lignes) pour tous les mots du vocabulaire\n",
    "                    (U dans le document pdf)\n",
    "    word2vecLossAndGradient -- la fonction de perte et de gradient pour\n",
    "                               un vecteur de prédiction étant donné les vecteurs de mots outsideWordIdx\n",
    "                               vecteurs de mots, peut être l'une des deux fonctions de perte\n",
    "                               fonctions de perte que vous avez implémentées ci-dessus.\n",
    "\n",
    "    Retourne :\n",
    "    loss -- la valeur de la fonction de perte pour le modèle skip-gram\n",
    "            (J dans le document pdf)\n",
    "    gradCenterVecs -- le gradient par rapport aux vecteurs de mots centraux\n",
    "            (dJ / dV dans le document pdf)\n",
    "    gradOutsideVectors -- le gradient par rapport aux vecteurs de mots extérieurs\n",
    "                        (dJ / dU dans le document pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "\n",
    "    currentCenterWordIdx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[currentCenterWordIdx]\n",
    "\n",
    "    for outsideWord in outsideWords:\n",
    "        outsideWordIdx = word2Ind[outsideWord]\n",
    "        (l, gradCenter, gradOutside) = word2vecLossAndGradient(\n",
    "            centerWordVec, outsideWordIdx, outsideVectors, dataset)\n",
    "        loss += l\n",
    "        gradCenterVecs[currentCenterWordIdx] += gradCenter\n",
    "        gradOutsideVectors += gradOutside\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjfsC0Rdkmmb"
   },
   "outputs": [],
   "source": [
    "windowSize=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG-E8o79kP--"
   },
   "outputs": [],
   "source": [
    "x0, step, iterations, postprocessing,PRINT_EVERY=wordVectors, 0.3, 40000, lambda x: x, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD8WrF5ijoDJ"
   },
   "outputs": [],
   "source": [
    "ANNEAL_EVERY = 20000\n",
    "start_iter=0\n",
    "x = x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_WZKutW5jtE4",
    "outputId": "015af9db-438d-4593-e90d-b6fe9af025fe"
   },
   "outputs": [],
   "source": [
    "exploss = None\n",
    "batchsize = 50\n",
    "for iter in range(start_iter + 1, iterations + 1):\n",
    "\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        loss = 0.0\n",
    "        grad = np.zeros(wordVectors.shape)\n",
    "        N = wordVectors.shape[0]\n",
    "        centerWordVectors = wordVectors[:int(N/2),:]\n",
    "        outsideVectors = wordVectors[int(N/2):,:]\n",
    "        for i in range(batchsize):\n",
    "            windowSize1 = random.randint(1, windowSize)\n",
    "            centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "            c, gin, gout = skipgram(\n",
    "                centerWord, windowSize1, context,  tokens, centerWordVectors,\n",
    "                outsideVectors, dataset, negSamplingLossAndGradient\n",
    "            )\n",
    "            loss += c / batchsize\n",
    "            grad[:int(N/2), :] += gin / batchsize\n",
    "            grad[int(N/2):, :] += gout / batchsize\n",
    "        x -= step * grad\n",
    "\n",
    "        x = postprocessing(x)\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not exploss:\n",
    "                exploss = loss\n",
    "            else:\n",
    "                exploss = .95 * exploss + .05 * loss\n",
    "            print(\"iter %d: %f\" % (iter, exploss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZNBDNwMmPsG"
   },
   "outputs": [],
   "source": [
    "# concaténer les vecteurs de mots d'entrée et de sortie\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqBDi9tvmQuF"
   },
   "outputs": [],
   "source": [
    "\n",
    "visualizeWords = [\n",
    "    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "    \"hail\", \"coffee\", \"tea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmqfsjokmTWU"
   },
   "outputs": [],
   "source": [
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0crh-aKmULa"
   },
   "outputs": [],
   "source": [
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X31-LhYv-ZUy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "86w5-wa8mVvF",
    "outputId": "70b5cfc3-0ddd-4ba3-dc29-1e50bc0f5d50"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
