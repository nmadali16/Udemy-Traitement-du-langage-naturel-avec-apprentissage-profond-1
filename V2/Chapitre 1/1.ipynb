{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8337f61f-84a4-48b5-85fc-f2ebf7d417ca",
   "metadata": {},
   "source": [
    "# Introduction to Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be individual words, phrases, or even characters, depending on the level of granularity required. In the context of natural language processing (NLP), tokenization is one of the foundational steps, enabling machines to process and understand text efficiently.\n",
    "\n",
    "## Why Tokenization is Important\n",
    "1. **Simplifies Text Processing**: By breaking text into manageable pieces, tokenization simplifies the process of analyzing and understanding text data.\n",
    "2. **Facilitates NLP Tasks**: Most NLP tasks, such as text classification, sentiment analysis, or machine translation, require text to be tokenized as a preprocessing step.\n",
    "3. **Reduces Redundancy**: Tokenization helps reduce redundant words and phrases by enabling the use of standardized tokens. For instance, different forms of the same word (e.g., \"running,\" \"runs,\" \"ran\") can be tokenized and further normalized to a base form (a process known as stemming or lemmatization).\n",
    "4. **Improves Model Efficiency**: Tokenization creates structured input for machine learning models, enhancing their ability to learn patterns and relationships within the data.\n",
    "\n",
    "In this notebook, we will explore various tokenization techniques, from simple approaches like whitespace splitting to more advanced library-based methods. We'll also see how tokenization helps in reducing redundancy and preparing text for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7e6971-8e50-4dc3-8541-a3f8cd8d2fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b89f7a6-d96e-4c51-814e-cc71feebc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '''\n",
    "Concentration of Risk: Credit Risk\n",
    "Financial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\n",
    "restricted cash, accounts receivable, convertible note hedges, and interest rate swaps. Our cash balances are primarily invested in money market funds\n",
    "or on deposit at high credit quality financial institutions in the U.S. These deposits are typically in excess of insured limits. As of September 30, 2021\n",
    "and December 31, 2020, no entity represented 10% or more of our total accounts receivable balance. The risk of concentration for our convertible note\n",
    "hedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\n",
    "Concentration of Risk: Supply Risk\n",
    "We are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\n",
    "products in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\n",
    "suppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\n",
    "'''\n",
    "\n",
    "text2 = '''\n",
    "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information \n",
    "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers \n",
    "for tesla related news,\n",
    "https://twitter.com/teslarati\n",
    "https://twitter.com/dummy_tesla\n",
    "https://twitter.com/dummy_2_tesla\n",
    "'''\n",
    "\n",
    "text3='codebasics: Hello, I am having an issue with my order # 412889912'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c2298-d376-488d-983a-a687b27917fd",
   "metadata": {},
   "source": [
    "## Naive Tokenization: Splitting Text by Spaces\n",
    "\n",
    "One of the simplest ways to tokenize text is by splitting it based on spaces. While this approach does not handle punctuation or account for complex language structures, it serves as a good starting point for understanding tokenization.\n",
    "\n",
    "Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0726599e-c254-4d4f-b421-1e54e023f3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConcentration of Risk: Credit Risk\\nFinancial instruments that potentially subject us to a concentra'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b658cd7-a7cd-4b57-a26f-08fc95596993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ad371a-9a99-4071-8527-c065a31b02e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text1.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8de8e01-4b9c-42ca-8098-bcf5e0d4d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}\n",
    "for token in text1.split():\n",
    "    if token in vocab:\n",
    "        vocab[token]+=1\n",
    "    else:\n",
    "        vocab[token]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19aef086-f8e6-4914-abd1-f67ef96387ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f87b2-4c10-4ee7-9a95-2e56f6dbdb37",
   "metadata": {},
   "source": [
    "## Importance of Lowercasing Before Tokenization\n",
    "\n",
    "Text data often contains words in mixed cases, such as \"Apple,\" \"apple,\" or \"APPLE.\" Without normalization, these variations are treated as distinct tokens, which can lead to redundancy and inconsistencies in downstream tasks.\n",
    "\n",
    "Lowercasing is a simple yet effective preprocessing step that ensures case insensitivity in the tokenization process.\n",
    "\n",
    "Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0cf9b79-a5f6-47e0-85a1-317ca8742f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConcentration of Risk: Credit Risk\\nFinancial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\\nrestricted cash, accounts receivable, convertible note hedges, and interest rate swaps. Our cash balances are primarily invested in money market funds\\nor on deposit at high credit quality financial institutions in the U.S. These deposits are typically in excess of insured limits. As of September 30, 2021\\nand December 31, 2020, no entity represented 10% or more of our total accounts receivable balance. The risk of concentration for our convertible note\\nhedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\\nConcentration of Risk: Supply Risk\\nWe are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\\nproducts in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\\nsuppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b37a96e-0b9f-4a3a-838a-98b7176d76c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconcentration of risk: credit risk\\nfinancial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\\nrestricted cash, accounts receivable, convertible note hedges, and interest rate swaps. our cash balances are primarily invested in money market funds\\nor on deposit at high credit quality financial institutions in the u.s. these deposits are typically in excess of insured limits. as of september 30, 2021\\nand december 31, 2020, no entity represented 10% or more of our total accounts receivable balance. the risk of concentration for our convertible note\\nhedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\\nconcentration of risk: supply risk\\nwe are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\\nproducts in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\\nsuppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a85c76f-e935-4607-b03c-ea516022c5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text1.lower().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879a4df-d05a-47fd-9743-ebfaa92d4f88",
   "metadata": {},
   "source": [
    "### Importance of Stop Word Removal During Tokenization\n",
    "\n",
    "In Natural Language Processing (NLP), tokenization is a crucial first step in preparing text for analysis. It involves breaking down a large body of text into smaller chunks, typically words or sub-words, which can then be processed further. However, one aspect of tokenization that plays a pivotal role in improving the efficiency and relevance of downstream tasks is **stop word removal**.\n",
    "\n",
    "### What Are Stop Words?\n",
    "\n",
    "Stop words are common words that appear frequently in a language but carry very little meaningful information on their own. Examples of stop words include:\n",
    "\n",
    "- Articles: \"the\", \"a\", \"an\"\n",
    "- Prepositions: \"in\", \"on\", \"at\"\n",
    "- Pronouns: \"he\", \"she\", \"it\"\n",
    "- Conjunctions: \"and\", \"but\", \"or\"\n",
    "\n",
    "These words, while essential for grammar and sentence structure, are often removed during tokenization because they don't contribute significantly to the meaning of a text in most NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23f9fea7-8bb5-4b1d-994e-5314df64044e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nmadali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e301757c-96d6-4372-aac5-8aa634eb24ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb12859f-d806-4994-b495-7d9561db70ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1.lower().split() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6482582-23b4-42b0-ba43-12955c1c0b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([token for token in text1.lower().split() if not token in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e987e0c-ae06-491b-98b9-5e0f29a670d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([token for token in text1.lower().split() if not token in stopwords.words('english')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b8bd9-a434-404f-a90c-38b95a390db6",
   "metadata": {},
   "source": [
    "### Importance of Regular Expressions for Removing Inadequate Tokens (e.g., HTML Tags) During Tokenization\n",
    "\n",
    "In Natural Language Processing (NLP), one of the challenges when processing raw text is dealing with \"inadequate tokens\" that do not add meaningful information to the analysis. These tokens might include special characters, formatting elements, or structural components such as HTML tags. To clean the text and prepare it for further analysis, we often use **regular expressions (regex)** to remove these types of tokens efficiently.\n",
    "\n",
    "### What Are Inadequate Tokens?\n",
    "\n",
    "Inadequate tokens refer to elements in the text that do not contribute to its core meaning. Examples of such tokens include:\n",
    "\n",
    "- **HTML Tags**: `<div>`, `<p>`, `<a>`, etc.\n",
    "- **CSS Classes/IDs**: `class=\"content\"`, `id=\"main\"`, etc.\n",
    "- **JavaScript Code**: `<script>`, `onclick=\"function()\"`, etc.\n",
    "- **URLs and Email Addresses**: `https://www.example.com`, `someone@example.com`\n",
    "- **Special Characters**: Punctuation, extra spaces, non-alphanumeric characters that do not serve a semantic purpose.\n",
    "\n",
    "These elements are often artifacts of how the text is structured or formatted on the web and do not add to the meaning of the text in most NLP applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a538068-4459-4c4f-a4d9-38d751299514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65281171-587b-4369-b806-ebc80d1e852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codebasics', 'hello', 'i', 'am', 'having', 'an', 'issue', 'with', 'my', 'order']\n"
     ]
    }
   ],
   "source": [
    "res = re.sub(r'[^a-zA-Z]', ' ', text3)\n",
    "print(res.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3a2355-ef8c-48df-9829-1db03abe1b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codebasics: Hello, I am having an issue with my order # 412889912'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec1914-16d4-4203-bd09-43f74d01122c",
   "metadata": {},
   "source": [
    "### Lemmatization in Natural Language Processing\n",
    "\n",
    "Lemmatization is an essential step in Natural Language Processing (NLP) that focuses on reducing words to their base or dictionary form, known as the **lemma**. Unlike stemming, which simply truncates words to remove suffixes, lemmatization considers the context and meaning of words to produce their correct base form. This step is crucial for normalizing words and improving the quality of text analysis in many NLP tasks.\n",
    "\n",
    "### What is Lemmatization?\n",
    "\n",
    "Lemmatization involves the process of mapping a word to its canonical form or root form. The lemma of a word is its base or dictionary form, which is often found in the dictionary. For example:\n",
    "\n",
    "- **\"running\"** → **\"run\"**\n",
    "- **\"better\"** → **\"good\"**\n",
    "- **\"geese\"** → **\"goose\"**\n",
    "\n",
    "Lemmatization typically uses part-of-speech (POS) tagging to understand the context of the word and accurately transform it into its base form. For instance, \"running\" may be lemmatized to \"run\" if it’s a verb, but \"better\" may be lemmatized to \"good\" if it’s an adjective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6cce545-d1f0-4162-add0-4cfa6f0f148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38f406c1-2b2e-4121-a6b0-02c0089fdc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['concentration',\n",
       " 'of',\n",
       " 'risk:',\n",
       " 'credit',\n",
       " 'risk',\n",
       " 'financial',\n",
       " 'instruments',\n",
       " 'that',\n",
       " 'potentially',\n",
       " 'subject',\n",
       " 'us',\n",
       " 'to',\n",
       " 'a',\n",
       " 'concentration',\n",
       " 'of',\n",
       " 'credit',\n",
       " 'risk',\n",
       " 'consist',\n",
       " 'of',\n",
       " 'cash,',\n",
       " 'cash',\n",
       " 'equivalents,',\n",
       " 'marketable',\n",
       " 'securities,',\n",
       " 'restricted',\n",
       " 'cash,',\n",
       " 'accounts',\n",
       " 'receivable,',\n",
       " 'convertible',\n",
       " 'note',\n",
       " 'hedges,',\n",
       " 'and',\n",
       " 'interest',\n",
       " 'rate',\n",
       " 'swaps.',\n",
       " 'our',\n",
       " 'cash',\n",
       " 'balances',\n",
       " 'are',\n",
       " 'primarily',\n",
       " 'invested',\n",
       " 'in',\n",
       " 'money',\n",
       " 'market',\n",
       " 'funds',\n",
       " 'or',\n",
       " 'on',\n",
       " 'deposit',\n",
       " 'at',\n",
       " 'high',\n",
       " 'credit',\n",
       " 'quality',\n",
       " 'financial',\n",
       " 'institutions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'u.s.',\n",
       " 'these',\n",
       " 'deposits',\n",
       " 'are',\n",
       " 'typically',\n",
       " 'in',\n",
       " 'excess',\n",
       " 'of',\n",
       " 'insured',\n",
       " 'limits.',\n",
       " 'as',\n",
       " 'of',\n",
       " 'september',\n",
       " '30,',\n",
       " '2021',\n",
       " 'and',\n",
       " 'december',\n",
       " '31,',\n",
       " '2020,',\n",
       " 'no',\n",
       " 'entity',\n",
       " 'represented',\n",
       " '10%',\n",
       " 'or',\n",
       " 'more',\n",
       " 'of',\n",
       " 'our',\n",
       " 'total',\n",
       " 'accounts',\n",
       " 'receivable',\n",
       " 'balance.',\n",
       " 'the',\n",
       " 'risk',\n",
       " 'of',\n",
       " 'concentration',\n",
       " 'for',\n",
       " 'our',\n",
       " 'convertible',\n",
       " 'note',\n",
       " 'hedges',\n",
       " 'and',\n",
       " 'interest',\n",
       " 'rate',\n",
       " 'swaps',\n",
       " 'is',\n",
       " 'mitigated',\n",
       " 'by',\n",
       " 'transacting',\n",
       " 'with',\n",
       " 'several',\n",
       " 'highly-rated',\n",
       " 'multinational',\n",
       " 'banks.',\n",
       " 'concentration',\n",
       " 'of',\n",
       " 'risk:',\n",
       " 'supply',\n",
       " 'risk',\n",
       " 'we',\n",
       " 'are',\n",
       " 'dependent',\n",
       " 'on',\n",
       " 'our',\n",
       " 'suppliers,',\n",
       " 'including',\n",
       " 'single',\n",
       " 'source',\n",
       " 'suppliers,',\n",
       " 'and',\n",
       " 'the',\n",
       " 'inability',\n",
       " 'of',\n",
       " 'these',\n",
       " 'suppliers',\n",
       " 'to',\n",
       " 'deliver',\n",
       " 'necessary',\n",
       " 'components',\n",
       " 'of',\n",
       " 'our',\n",
       " 'products',\n",
       " 'in',\n",
       " 'a',\n",
       " 'timely',\n",
       " 'manner',\n",
       " 'at',\n",
       " 'prices,',\n",
       " 'quality',\n",
       " 'levels',\n",
       " 'and',\n",
       " 'volumes',\n",
       " 'acceptable',\n",
       " 'to',\n",
       " 'us,',\n",
       " 'or',\n",
       " 'our',\n",
       " 'inability',\n",
       " 'to',\n",
       " 'efficiently',\n",
       " 'manage',\n",
       " 'these',\n",
       " 'components',\n",
       " 'from',\n",
       " 'these',\n",
       " 'suppliers,',\n",
       " 'could',\n",
       " 'have',\n",
       " 'a',\n",
       " 'material',\n",
       " 'adverse',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'our',\n",
       " 'business,',\n",
       " 'prospects,',\n",
       " 'financial',\n",
       " 'condition',\n",
       " 'and',\n",
       " 'operating',\n",
       " 'results.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b77e26-f8c2-446d-abf4-81cbf2cf97cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'concentr'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('concentration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1271743-abcd-4489-8000-e96b91b5005c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([stemmer.stem(token) for token in text1.lower().split() if not token in stopwords.words('english')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980da48-8540-4363-9f42-d4befcb9cf47",
   "metadata": {},
   "source": [
    "### Using `en_core_web_sm` for Natural Language Processing\n",
    "\n",
    "`en_core_web_sm` is a pre-trained small-sized model from **spaCy**, a popular open-source library for natural language processing (NLP) in Python. The model is used to perform various NLP tasks like tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more. The `sm` in the model name stands for \"small\", indicating it is designed for efficient use with smaller computational resources.\n",
    "\n",
    "## Key Features of `en_core_web_sm`\n",
    "\n",
    "1. **Tokenization**: Breaks text into individual words, punctuation marks, and symbols.\n",
    "2. **Part-of-Speech (POS) Tagging**: Identifies the grammatical category of each word in a sentence (e.g., noun, verb, adjective).\n",
    "3. **Named Entity Recognition (NER)**: Identifies and classifies entities in text, such as persons, organizations, and locations.\n",
    "4. **Dependency Parsing**: Analyzes the syntactic structure of a sentence and identifies the relationships between words.\n",
    "5. **Lemmatization**: Reduces words to their base form (e.g., \"running\" becomes \"run\").\n",
    "6. **Sentence Segmentation**: Splits text into sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db3dc937-9194-4b48-88cd-9b28a145adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "603bb011-d300-460d-97ea-5f021ebf55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "# No tokenizer assignment, everything is fine\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp( text1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "506c1428-536d-4b98-9349-5ce2708eb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "for token in doc:\n",
    "    if not (token.is_stop or token.is_punct):\n",
    "        vocab.append(token.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "394348b1-c8e8-4955-8b61-38cec1aa4ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ec01d-d510-4535-8406-e55de2108d64",
   "metadata": {},
   "source": [
    "### Creating a Vocabulary of Unique Words After Tokenization\n",
    "\n",
    "Once the text has been properly tokenized and unnecessary elements like stop words or HTML tags have been removed, the next important step in Natural Language Processing (NLP) is to create a **vocabulary**. A vocabulary is essentially a collection of unique words that appear in a given corpus or dataset. This step is crucial for transforming raw text into a structured form that can be used by machine learning models.\n",
    "\n",
    "### What is a Vocabulary?\n",
    "\n",
    "A vocabulary is a set of distinct words that have been extracted from the tokenized text. These words can then be used as features in various NLP tasks such as text classification, sentiment analysis, or machine translation. The vocabulary represents all the terms that the model will recognize and work with.\n",
    "\n",
    "For example, consider the following tokenized text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70285f34-9027-46f7-b855-fe439f15f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tmp=[]\n",
    "for text in [text3]:\n",
    "    doc = nlp( text.lower())\n",
    "    for token in doc:\n",
    "        if not (token.is_stop or token.is_punct):\n",
    "            vocab_tmp.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7bc8c155-2b16-4a97-8c49-0f744afed5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'codebasics' in vocab_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79b15e6b-0150-47d4-9295-98b1049a74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={k:v for v,k in enumerate(set(vocab_tmp))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8d6cf97-68c1-45c4-8dd8-35b021b82ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv={v:k for k,v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e515ef9c-405e-495e-83fd-cb948eaf6ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hedges'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "558f2572-f4c6-44ce-9eae-3c9583df650c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hedges': 0,\n",
       " 'https://twitter.com/elonmusk': 1,\n",
       " 'receivable': 2,\n",
       " 'having': 3,\n",
       " 'funds': 4,\n",
       " 'https://twitter.com/teslarati': 5,\n",
       " 'financial': 6,\n",
       " 'necessary': 7,\n",
       " 'prospects': 8,\n",
       " 'single': 9,\n",
       " 'consist': 10,\n",
       " 'september': 11,\n",
       " 'hello': 12,\n",
       " 'total': 13,\n",
       " 'related': 14,\n",
       " 'business': 15,\n",
       " 'volumes': 16,\n",
       " 'codebasics': 17,\n",
       " 'dependent': 18,\n",
       " 'excess': 19,\n",
       " 'products': 20,\n",
       " 'balances': 21,\n",
       " 'effect': 22,\n",
       " 'order': 23,\n",
       " 'highly': 24,\n",
       " '2020': 25,\n",
       " 'musk': 26,\n",
       " 'issue': 27,\n",
       " 'typically': 28,\n",
       " 'acceptable': 29,\n",
       " 'twitter': 30,\n",
       " 'entity': 31,\n",
       " 'components': 32,\n",
       " 'efficiently': 33,\n",
       " 'inability': 34,\n",
       " 'material': 35,\n",
       " 'rate': 36,\n",
       " 'deposit': 37,\n",
       " 'market': 38,\n",
       " '30': 39,\n",
       " 'deliver': 40,\n",
       " 'marketable': 41,\n",
       " '31': 42,\n",
       " 'insured': 43,\n",
       " 'limits': 44,\n",
       " 'balance': 45,\n",
       " 'convertible': 46,\n",
       " 'invested': 47,\n",
       " 'leading': 48,\n",
       " 'subject': 49,\n",
       " 'banks': 50,\n",
       " 'high': 51,\n",
       " 'https://twitter.com/dummy_2_tesla': 52,\n",
       " 'manage': 53,\n",
       " 'interest': 54,\n",
       " '412889912': 55,\n",
       " 'note': 56,\n",
       " 'mitigated': 57,\n",
       " 'concentration': 58,\n",
       " 'risk': 59,\n",
       " 'levels': 60,\n",
       " 'multinational': 61,\n",
       " 'cash': 62,\n",
       " 'condition': 63,\n",
       " 'https://www.tesla.com/.': 64,\n",
       " 'news': 65,\n",
       " 'follow': 66,\n",
       " 'securities': 67,\n",
       " 'timely': 68,\n",
       " 'transacting': 69,\n",
       " 'source': 70,\n",
       " 'potentially': 71,\n",
       " 'primarily': 72,\n",
       " 'supply': 73,\n",
       " 'https://twitter.com/dummy_tesla': 74,\n",
       " 'influencers': 75,\n",
       " 'operating': 76,\n",
       " 'accounts': 77,\n",
       " 'u.s': 78,\n",
       " 'deposits': 79,\n",
       " 'manner': 80,\n",
       " 'including': 81,\n",
       " 'money': 82,\n",
       " 'prices': 83,\n",
       " '2021': 84,\n",
       " 'tesla': 85,\n",
       " '10': 86,\n",
       " 'december': 87,\n",
       " 'information': 88,\n",
       " 'restricted': 89,\n",
       " 'represented': 90,\n",
       " 'leader': 91,\n",
       " 'suppliers': 92,\n",
       " 'credit': 93,\n",
       " 'quality': 94,\n",
       " 'instruments': 95,\n",
       " 'adverse': 96,\n",
       " 'swaps': 97,\n",
       " 'rated': 98,\n",
       " 'found': 99,\n",
       " 'institutions': 100,\n",
       " '\\n': 101,\n",
       " 'equivalents': 102,\n",
       " 'results': 103,\n",
       " 'elon': 104}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2b49c32-97e5-4508-a51e-2eae7d1e02f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25415606-d222-40b0-8854-235680b64d9a",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) Using NLTK\n",
    "\n",
    "The **Bag of Words (BoW)** model is one of the simplest and most widely used techniques for text representation in Natural Language Processing (NLP). It involves converting text documents into a matrix of token counts or frequencies. In the BoW model, each document is represented by a vector of words, where the frequency of each word is stored in the vector. This model is called \"bag of words\" because it disregards the order and structure of words in the text, focusing solely on the frequency of words.\n",
    "\n",
    "In this section, we'll explore how to build a Bag of Words model using **NLTK** functions.\n",
    "\n",
    "### Steps to Implement Bag of Words Using NLTK\n",
    "\n",
    "1. **Tokenization**:  \n",
    "   The first step is to tokenize the text, breaking it down into individual words or tokens. NLTK provides the `word_tokenize` function for this purpose.\n",
    "\n",
    "2. **Removing Stop Words**:  \n",
    "   Once the text is tokenized, we often remove stop words (common words like \"the\", \"is\", etc.) because they don't provide useful information for analysis. NLTK provides a predefined list of stop words that we can filter out.\n",
    "\n",
    "3. **Creating the Vocabulary**:  \n",
    "   The next step is to create the vocabulary (the list of unique words) from the text. This vocabulary will serve as the basis for our Bag of Words representation.\n",
    "\n",
    "4. **Vectorizing the Text**:  \n",
    "   After preparing the vocabulary, each document is represented as a vector, where each element in the vector corresponds to the frequency of a specific word in the vocabulary.\n",
    "\n",
    "### Example: Building a Bag of Words Model with NLTK\n",
    "\n",
    "Let's go through an example of creating a Bag of Words model for a set of text documents using NLTK.\n",
    "\n",
    "#### 1. Import Necessary Libraries and Download Resources\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc0f7452-935e-4a1a-bf59-df78cf815ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codebasics:',\n",
       " 'hello,',\n",
       " 'i',\n",
       " 'am',\n",
       " 'having',\n",
       " 'an',\n",
       " 'issue',\n",
       " 'with',\n",
       " 'my',\n",
       " 'order',\n",
       " '#',\n",
       " '412889912']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e762565-f2aa-437a-960f-62c138c717d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text3.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ba4a3f30-7520-48ff-9529-0b5a82b68431",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=np.zeros(len(vocab))\n",
    "\n",
    "tokens = [token.text  for token in nlp( text3.lower())  if not (token.is_stop or token.is_punct)]\n",
    "for key in vocab: \n",
    "\n",
    "    if key in tokens:   \n",
    "        vec[vocab[key]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eedb152b-2af8-4eba-a7f8-1844670f0fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'codebasics' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e354b967-4c9d-431c-9663-9354bdb2c5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['codebasics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7f22074b-f0c7-46fd-bf5a-b5f29bac0195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codebasics:',\n",
       " 'hello,',\n",
       " 'i',\n",
       " 'am',\n",
       " 'having',\n",
       " 'an',\n",
       " 'issue',\n",
       " 'with',\n",
       " 'my',\n",
       " 'order',\n",
       " '#',\n",
       " '412889912']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "80b939f7-c9c2-413a-aa5a-73c55b3e4027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codebasics: Hello, I am having an issue with my order # 412889912'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4fc112a1-5277-46ed-8b83-dc13829cc4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1b7be0d5-bb9d-4116-8146-68895135868c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12, 17, 23, 27, 55])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vec==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "150def68-ae5f-43eb-a185-d33e6d70ab07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inv[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "479b6695-cf4c-4c3a-80de-62d5a270db13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['having', 'hello', 'codebasics', 'order', 'issue', '412889912']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab_inv[idx] for idx in  np.where(vec==1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644408e4-7426-4186-a2dc-0ee3422dada9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974de5b-ec3b-4014-9591-70a75590d587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63234f-52f7-4257-8f08-fb9cb5d559e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
