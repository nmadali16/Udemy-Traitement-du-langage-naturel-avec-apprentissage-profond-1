{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f52740-0736-4b01-a007-4f9857b4dfad",
   "metadata": {},
   "source": [
    "# The Datasets Library and Hugging Face\n",
    "\n",
    "The **`datasets`** library is an open-source Python library developed by **Hugging Face**, designed to provide seamless access to a wide variety of datasets for machine learning and natural language processing (NLP). It simplifies the process of downloading, preprocessing, and managing datasets, enabling developers and researchers to focus more on modeling and experimentation.\n",
    "\n",
    "## Key Features\n",
    "- **Wide Range of Datasets**: The library includes a large collection of popular datasets like GLUE, SQuAD, IMDb, and many others for various ML tasks such as text classification, machine translation, and question answering.\n",
    "- **Ease of Use**: Datasets can be loaded with a single line of code, and the library handles downloading, caching, and efficient data loading.\n",
    "- **Interoperability**: Fully compatible with Hugging Face's Transformer models, making it easy to train and fine-tune models on datasets directly.\n",
    "- **Dataset Processing**: Offers built-in tools for preprocessing, filtering, and mapping functions across datasets, which is particularly useful for preparing data for machine learning pipelines.\n",
    "- **Community Contributions**: Users can contribute their datasets to Hugging Faceâ€™s hub, making them available for others in the community.\n",
    "\n",
    "# Hyperparameters of `load_dataset`\n",
    "\n",
    "The `load_dataset` function from the Hugging Face `datasets` library provides several parameters (hyperparameters) to customize the loading and processing of datasets. Below is a breakdown of its key parameters:\n",
    "\n",
    "## Function Signature\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path,\n",
    "    name=None,\n",
    "    data_dir=None,\n",
    "    data_files=None,\n",
    "    split=None,\n",
    "    cache_dir=None,\n",
    "    features=None,\n",
    "    download_config=None,\n",
    "    download_mode=None,\n",
    "    ignore_verifications=False,\n",
    "    save_infos=False,\n",
    "    revision=None,\n",
    "    token=None,\n",
    "    use_auth_token=None,\n",
    "    streaming=False,\n",
    "    num_proc=None,\n",
    "    **config_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52c12d-f762-4158-92f9-4121812482a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4fc5e-812d-45ca-9eb3-969f64506511",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Key Parameters\n",
    "\n",
    "1. **`path` (str)**:\n",
    "   - The identifier for the dataset. This can be the dataset's name (e.g., `\"squad\"`, `\"imdb\"`) from the Hugging Face Hub or a local dataset script path.\n",
    "   - Example:\n",
    "     ```python\n",
    "     dataset = load_dataset(\"squad\")\n",
    "     ```\n",
    "\n",
    "2. **`name` (str, optional)**:\n",
    "   - Specifies the configuration for datasets with multiple configurations (e.g., `\"glue\"` tasks like `\"sst2\"`, `\"cola\"`).\n",
    "   - Example:\n",
    "     ```python\n",
    "     dataset = load_dataset(\"glue\", name=\"sst2\")\n",
    "     ```\n",
    "\n",
    "3. **`data_dir` (str, optional)**:\n",
    "   - Specifies the directory containing dataset files when using a local dataset.\n",
    "   - Example:\n",
    "     ```python\n",
    "     dataset = load_dataset(\"path/to/script\", data_dir=\"my_data\")\n",
    "     ```\n",
    "\n",
    "4. **`data_files` (str or dict, optional)**:\n",
    "   - Specifies the file(s) to use as the dataset. Supports single files, lists, or split-specific dictionaries.\n",
    "   - Example:\n",
    "     ```python\n",
    "     dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"})\n",
    "     ```\n",
    "\n",
    "5. **`split` (str or list, optional)**:\n",
    "   - Specifies the dataset split(s) to load (e.g., `\"train\"`, `\"test\"`, `\"validation\"`).\n",
    "   - Example:\n",
    "     ```python\n",
    "     dataset = load_dataset(\"squad\", split=\"train\")\n",
    "     ```\n",
    "\n",
    "6. **`cache_dir` (str, optional)**:\n",
    "   - Specifies where to store cached datasets locally.\n",
    "   - Example:\n",
    "     ```python\n",
    "     dataset = load_dataset(\"squad\", cache_dir=\"./cache\")\n",
    "     ```\n",
    "\n",
    "7. **`features` (Features, optional)**:\n",
    "   - Defines the feature types (e.g., string, integer) for the dataset. Useful for preprocessing or schema enforcement.\n",
    "\n",
    "8. **`download_mode` (str, optional)**:\n",
    "   - Controls the download behavior:\n",
    "     - `\"reuse_dataset_if_exists\"` (default): Uses cached data if available.\n",
    "     - `\"force_redownload\"`: Forces re-downloading the dataset.\n",
    "\n",
    "9. **`streaming` (bool, optional)**:\n",
    "   - Enables streaming mode for lazy data loading, ideal for large datasets.\n",
    "   - Example:\n",
    "     ```python\n",
    "     dataset = load_dataset(\"large_dataset\", streaming=True)\n",
    "     ```\n",
    "\n",
    "10. **`use_auth_token` or `token` (str or bool, optional)**:\n",
    "    - Provides an authentication token to access private datasets on the Hugging Face Hub.\n",
    "\n",
    "11. **`num_proc` (int, optional)**:\n",
    "    - Specifies the number of processes for data preprocessing, improving speed on large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8d150-84d3-42dc-80e2-9df33f7b2035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
