{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "baf7f300-6026-439b-b70a-f36c968a79da",
      "metadata": {
        "id": "baf7f300-6026-439b-b70a-f36c968a79da"
      },
      "source": [
        "\n",
        "# LSTM Language Modeling with IMDB Data\n",
        "\n",
        "In this notebook, we will train an LSTM model for language modeling on the IMDB dataset. We will cover the following steps:\n",
        "\n",
        "1. **Dataset Loading**\n",
        "2. **Tokenization and Vocabulary Creation**\n",
        "3. **Dataset Preparation**\n",
        "4. **DataLoader Creation**\n",
        "5. **PyTorch Model Creation**\n",
        "6. **Optimizer and Loss Function**\n",
        "7. **Model Training and Loss Monitoring**\n",
        "8. **Model Evaluation**\n",
        "\n",
        "The main goal is to create a language model that can predict the next word in a sequence of words. This involves training the LSTM model to minimize the cross-entropy loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f50f87-87ac-4d23-96d2-1495439c6068",
      "metadata": {
        "id": "39f50f87-87ac-4d23-96d2-1495439c6068"
      },
      "source": [
        "## 1. Dataset Loading\n",
        "\n",
        "We use the `IMDB` dataset from TorchText. The dataset contains labeled movie reviews. However, for language modeling, we only use the text data and ignore the labels.\n",
        "\n",
        "The dataset is split into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkwebxwe-xYz",
        "outputId": "d87bb11e-500d-41bf-b1c8-9438f650a46f"
      },
      "id": "zkwebxwe-xYz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YpWCQloI_W1",
        "outputId": "1dd4c563-0849-4362-a6e3-fb491a228e5e"
      },
      "id": "_YpWCQloI_W1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.11.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0383f4-9cc8-48b6-982d-8cee084def41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c0383f4-9cc8-48b6-982d-8cee084def41",
        "outputId": "61712955-cbc6-4645-b8b4-904c8b32aa35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import datasets\n",
        "\n",
        "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text=[sample['text'] for sample in train_data]"
      ],
      "metadata": {
        "id": "LcsWqgs7-iAu"
      },
      "id": "LcsWqgs7-iAu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a88b47-e3d6-42b6-981e-4604de2666a1",
      "metadata": {
        "id": "d6a88b47-e3d6-42b6-981e-4604de2666a1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4ca4dfbc-cdd2-45a5-8ff9-413a01662f24",
      "metadata": {
        "id": "4ca4dfbc-cdd2-45a5-8ff9-413a01662f24"
      },
      "source": [
        "## 2. Tokenization and Vocabulary Creation\n",
        "\n",
        "Tokenization is the process of splitting text into smaller units (tokens). We use TorchText's built-in tokenizer for this purpose.\n",
        "\n",
        "We then build a vocabulary from the tokenized dataset. The vocabulary maps each token to a unique integer index. It also contains special tokens:\n",
        "- `<unk>` for unknown tokens\n",
        "- `<pad>` for padding sequences\n",
        "- `<bos>` for the beginning of a sequence\n",
        "- `<eos>` for the end of a sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410fdbf4-1427-4498-950e-1ca2ad0dddeb",
      "metadata": {
        "id": "410fdbf4-1427-4498-950e-1ca2ad0dddeb"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "class Tokenizer:\n",
        "  def __init__(self, stop_words, puncts, truncation_size=256  ):\n",
        "\n",
        "    self.stop_words=stop_words\n",
        "    self.puncts=puncts\n",
        "    self.df = {}\n",
        "    self.truncation_size=truncation_size\n",
        "  def format_string(self, text):\n",
        "      tokens=[ token for token  in text.lower().split() if not ((token in self.stop_words) or  (token in self.puncts))   ]\n",
        "      return tokens\n",
        "\n",
        "  def tokenize(self, text, truncation=False):\n",
        "      tokens=self.format_string(text)\n",
        "      tmp=[]\n",
        "      for token in tokens:\n",
        "          if token in self.w2i :\n",
        "              tmp.append(self.w2i[token])\n",
        "          else:\n",
        "              tmp.append(self.w2i['<unk>'])\n",
        "\n",
        "      if truncation:\n",
        "          tmp=tmp[: self.truncation_size]\n",
        "          output= np.ones(self.truncation_size +2 )*self.w2i['<pad>']\n",
        "          output[0]=self.w2i['<bos>']\n",
        "          output[-1]=self.w2i['<eos>']\n",
        "          output[1:len(tmp)+1]=tmp\n",
        "          return list(output)\n",
        "      else:\n",
        "          return tmp\n",
        "  def detokenize(self, idxs):\n",
        "      words=[self.i2w[idx] for idx in idxs]\n",
        "\n",
        "      return ''.join(word+' ' for word in words )\n",
        "  def fit(self, train_text):\n",
        "    for text in train_text:\n",
        "        tokens=self.format_string(text)\n",
        "        for token, count in Counter(tokens).items():\n",
        "            if token in self.df:\n",
        "                self.df[token]+=count\n",
        "            else:\n",
        "                self.df[token]=count\n",
        "    self.w2i={}\n",
        "    idx_count=0\n",
        "    for  (k,v) in  self.df.items():\n",
        "        if v>5:\n",
        "          self.w2i[k]=idx_count\n",
        "          idx_count+=1\n",
        "\n",
        "    for k in ['<unk>', '<pad>', '<bos>', '<eos>']:\n",
        "      self.w2i[k]=idx_count\n",
        "      idx_count+=1\n",
        "\n",
        "\n",
        "    self.i2w = { v:k for (k,v) in  self.w2i .items() }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words= []\n",
        "puncts=  []"
      ],
      "metadata": {
        "id": "e_2zBpk--tUN"
      },
      "id": "e_2zBpk--tUN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(stop_words , puncts)"
      ],
      "metadata": {
        "id": "9KA5kR4v-uem"
      },
      "id": "9KA5kR4v-uem",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit(train_text)"
      ],
      "metadata": {
        "id": "qrbVLsu5-viZ"
      },
      "id": "qrbVLsu5-viZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(train_text[4], truncation=True)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtcCnXhPAaxP",
        "outputId": "71f83aa7-f57a-421b-ef21-701b256f5984"
      },
      "id": "VtcCnXhPAaxP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[40133.0, 377.0, 40131.0, 378.0, 64.0, 33.0, 379.0, 153.0, 44.0, 380.0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.w2i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc1QZrHiBarD",
        "outputId": "5c2cde29-8833-49d7-8d12-41dd83ebc915"
      },
      "id": "Dc1QZrHiBarD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40135"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.detokenize(tokenizer.tokenize(train_text[4], truncation=True)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_Qo9PTIUAI3f",
        "outputId": "ab4e63bd-6fc5-4116-fa3a-ddc01cce4ecc"
      },
      "id": "_Qo9PTIUAI3f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos> oh, <unk> hearing about this ridiculous film for umpteen '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(train_text[4], truncation=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rTRBzEVAQm6",
        "outputId": "3830870c-39a0-4785-d90d-e82e7eb7fe41"
      },
      "id": "4rTRBzEVAQm6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "258"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad71f2e-b53e-41ca-b07a-a8ec811db4d2",
      "metadata": {
        "id": "4ad71f2e-b53e-41ca-b07a-a8ec811db4d2"
      },
      "source": [
        "**Mathematical Representation:**\n",
        "\n",
        "Given a sequence of tokens $ \\{w_1, w_2, \\dots, w_T\\} $, the vocabulary maps each token $ w_i $ to an integer index $ v_i $.\n",
        "\n",
        "\n",
        "## 3. Dataset Preparation\n",
        "\n",
        "For language modeling, we split the tokenized dataset into input-target pairs:\n",
        "\n",
        "$$\n",
        "(x_1, x_2, \\dots, x_{T-1}) \\to (x_2, x_3, \\dots, x_T)\n",
        "$$\n",
        "\n",
        "This means the model will learn to predict the next word in the sequence.\n",
        "\n",
        "Sequences are padded to ensure equal length for batching."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, train_data, tokenizer, return_type='Ids'):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with data and targets.\n",
        "        Args:\n",
        "            data: The input data (e.g., features).\n",
        "            targets: The corresponding labels or targets.\n",
        "        \"\"\"\n",
        "        self.train_data = train_data\n",
        "        self.tokenizer = tokenizer\n",
        "        self. return_type=return_type # BoG, Tf-Idf Ids\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the total number of samples.\n",
        "        \"\"\"\n",
        "        return len(self.train_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a sample and its target at the given index.\n",
        "        \"\"\"\n",
        "        text,label= self.train_data[idx]['text'], self.train_data[idx]['label']\n",
        "\n",
        "        if self.return_type=='Ids'  :\n",
        "\n",
        "            idxs= np.array(self.tokenizer.tokenize(text, truncation=True)).astype('int32')\n",
        "            return idxs, label\n",
        "        elif self.return_type=='BoG':\n",
        "            vec=np.zeros(len(self.tokenizer.w2i))\n",
        "            idxs= self.tokenizer.tokenize(text)\n",
        "            for idx in idxs:\n",
        "                vec[idx]+=1\n",
        "            return vec, label\n",
        "        else:\n",
        "            tf=np.zeros(len(self.tokenizer.w2i))\n",
        "            idxs= self.tokenizer.tokenize(text)\n",
        "            for idx in idxs:\n",
        "                tf[idx]+=1\n",
        "            tf/=len(idxs)\n",
        "\n",
        "            for i in range(len(tf)):\n",
        "                tf[i]*=self.tokenizer.idf[i]\n",
        "            return tf, label"
      ],
      "metadata": {
        "id": "MdT89WOTCBZ_"
      },
      "id": "MdT89WOTCBZ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a33e14-db3c-4d72-90a5-03d3652cc9e0",
      "metadata": {
        "id": "83a33e14-db3c-4d72-90a5-03d3652cc9e0"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_data, tokenizer)\n",
        "test_dataset = CustomDataset(test_data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcObzlsXCtI5",
        "outputId": "615a13f7-c2c9-4bb3-f66f-a902510f2a12"
      },
      "id": "bcObzlsXCtI5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([40133,     0,     1,     0,     2, 40131,     3,     4,     5,\n",
              "           6], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.detokenize(train_dataset[0][0][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fsxv0rqTCGCK",
        "outputId": "e4139fad-c3a8-47b2-9cec-959fb21d739f"
      },
      "id": "fsxv0rqTCGCK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos> i rented i am <unk> from my video store '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55962eef-7e80-47f7-86cd-03d358b9f51a",
      "metadata": {
        "id": "55962eef-7e80-47f7-86cd-03d358b9f51a"
      },
      "source": [
        "## 4. DataLoader Creation\n",
        "\n",
        "The DataLoader is used to batch, shuffle, and efficiently load the dataset during training. Padding ensures that all sequences in a batch have the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14fe578-7f99-4147-af6a-ac1e716e25ca",
      "metadata": {
        "id": "e14fe578-7f99-4147-af6a-ac1e716e25ca"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25072779-1620-40f4-9c1a-b9728d03515b",
      "metadata": {
        "id": "25072779-1620-40f4-9c1a-b9728d03515b"
      },
      "source": [
        "\n",
        "\n",
        "## 5. PyTorch Model Creation\n",
        "\n",
        "We define an LSTM-based language model using PyTorch. The model consists of:\n",
        "- An Embedding layer: Converts token indices into dense vectors.\n",
        "- An LSTM layer: Processes the sequence data.\n",
        "- A Linear layer: Maps the LSTM output to vocabulary size for prediction.\n",
        "\n",
        "The LSTM updates its hidden states $ h_t $ and cell states $ c_t $ at each time step $ t $:\n",
        "\n",
        "$$\n",
        "(h_t, c_t) = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d6fce50-f974-4fbd-b26d-5b4f562745f8",
      "metadata": {
        "id": "9d6fce50-f974-4fbd-b26d-5b4f562745f8"
      },
      "outputs": [],
      "source": [
        "class LM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_size, vocab_size, hidden_size, pad_index):\n",
        "        super(LM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, padding_idx=pad_index)\n",
        "        self.lstm= nn.LSTM(embedding_size, hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "        self.cls= nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idxs):\n",
        "        x = self.embedding (idxs)\n",
        "        hidden_states, (hn, cn)= self.lstm(x)\n",
        "        predictions= self.cls(hidden_states)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size=256\n",
        "embedding_size=300\n",
        "vocab_size=len(tokenizer.w2i)\n",
        "pad_index=tokenizer.w2i['<pad>']\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model= LM( embedding_size, vocab_size, hidden_size, pad_index).to(device)\n"
      ],
      "metadata": {
        "id": "s0qopUVzDXCV"
      },
      "id": "s0qopUVzDXCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0,1,2,3\n",
        "1,2,3\n",
        "loss_fn(idxs[1:],outputs[:-1])"
      ],
      "metadata": {
        "id": "eob0nenKGTJM"
      },
      "id": "eob0nenKGTJM"
    },
    {
      "cell_type": "markdown",
      "id": "743a36fe-caad-4203-b000-7a1fc0f51b50",
      "metadata": {
        "id": "743a36fe-caad-4203-b000-7a1fc0f51b50"
      },
      "source": [
        "## 6. Optimizer and Loss Function\n",
        "\n",
        "We use the Adam optimizer and CrossEntropyLoss for training:\n",
        "- **Adam Optimizer:** An adaptive learning rate optimization algorithm.\n",
        "- **CrossEntropyLoss:** Computes the loss between predicted and target token distributions."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YoQXNmsWG7_a"
      },
      "id": "YoQXNmsWG7_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.text import Perplexity"
      ],
      "metadata": {
        "id": "BFnJ_LQXGz5G"
      },
      "id": "BFnJ_LQXGz5G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7623697-9af9-44d1-b989-a5a0356d5ed9",
      "metadata": {
        "id": "a7623697-9af9-44d1-b989-a5a0356d5ed9"
      },
      "outputs": [],
      "source": [
        "# Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_fn= torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "perp=Perplexity().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "int -> [int]\n",
        "\n",
        "(bs,seq_len,v) ->(-1, v)\n",
        "(-1)"
      ],
      "metadata": {
        "id": "d6qlL4KaHcto"
      },
      "id": "d6qlL4KaHcto"
    },
    {
      "cell_type": "markdown",
      "id": "2ca8e63f-fee9-430e-a12c-194e0589fdfe",
      "metadata": {
        "id": "2ca8e63f-fee9-430e-a12c-194e0589fdfe"
      },
      "source": [
        "### 7. Model Training\n",
        "\n",
        "Training involves minimizing the loss over multiple epochs. For each batch:\n",
        "1. Forward pass through the model.\n",
        "2. Compute the loss.\n",
        "3. Backward pass to compute gradients.\n",
        "4. Update model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769d8d5d-1194-4c3b-a124-666b314056b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "769d8d5d-1194-4c3b-a124-666b314056b6",
        "outputId": "4e5e62fb-fff2-4e27-cb95-66548aef0eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 782/782 [04:15<00:00,  3.06batch/s, loss=4.47, perplexity=545]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 0  loss 4.47069331965483  perplexity 545.1595719915522 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [04:14<00:00,  3.07batch/s, loss=3.93, perplexity=53.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1  loss 3.9329817526785615  perplexity 53.094930312212774 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [04:14<00:00,  3.07batch/s, loss=3.76, perplexity=44.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 2  loss 3.7567684644323482  perplexity 44.35045300847124 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [04:15<00:00,  3.07batch/s, loss=3.63, perplexity=39.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 3  loss 3.633431990128344  perplexity 39.140930290417295 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 782/782 [04:15<00:00,  3.06batch/s, loss=3.54, perplexity=35.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 4  loss 3.5391940518718243  perplexity 35.47881779097535 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 782/782 [04:14<00:00,  3.07batch/s, loss=3.46, perplexity=32.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 5  loss 3.460298712601137  perplexity 32.79753287917818 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 782/782 [04:16<00:00,  3.05batch/s, loss=3.39, perplexity=30.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 6  loss 3.394742081537271  perplexity 30.614711317564826 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 782/782 [04:14<00:00,  3.07batch/s, loss=3.34, perplexity=28.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 7  loss 3.3352974320921445  perplexity 28.837434916240174 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 782/782 [04:14<00:00,  3.07batch/s, loss=3.28, perplexity=27.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 8  loss 3.2830165286198296  perplexity 27.324046828862652 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 782/782 [04:14<00:00,  3.07batch/s, loss=3.23, perplexity=26.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 9  loss 3.2347928800851182  perplexity 26.064371563894365 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "n_epochs=10\n",
        "for epoch in range(n_epochs):\n",
        "  running_perplexity=[]\n",
        "  running_loss=[]\n",
        "  with tqdm.tqdm(train_dataloader, unit='batch') as tepoch:\n",
        "    for batch in tepoch:\n",
        "      tepoch.set_description(f'Epoch {epoch}')\n",
        "\n",
        "      idxs, _=batch\n",
        "      idxs=idxs.to(device).long()\n",
        "      # Zero your gradients for every batch!\n",
        "      optimizer.zero_grad()\n",
        "      outputs=model(idxs)\n",
        "\n",
        "      batch_size, seq_len, vocab_size=outputs.shape\n",
        "\n",
        "      outputs = outputs[:,:-1,:].reshape(-1, vocab_size)\n",
        "      targets = idxs[:,1:].reshape(-1)\n",
        "\n",
        "      # Create a mask for non-padding tokens\n",
        "      mask = targets != pad_index\n",
        "\n",
        "      # Apply mask to filter out padding tokens\n",
        "      filtered_outputs = outputs[mask]\n",
        "      filtered_targets = targets[mask]\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      perplexity = perp(outputs.reshape(batch_size, (seq_len-1), vocab_size), targets.reshape(batch_size, (seq_len-1)))\n",
        "\n",
        "      running_perplexity.append(perplexity.item())\n",
        "      running_loss.append(loss.item())\n",
        "      tepoch.set_postfix(loss=np.mean(running_loss), perplexity = np.mean(running_perplexity) )\n",
        "  #print(' Epoch {}  loss {}  perplexity {} '.format(epoch, np.mean(running_loss),np.mean(running_perplexity)) )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'pretrained_lstm.pth')"
      ],
      "metadata": {
        "id": "Qd2HyERYUnLo"
      },
      "id": "Qd2HyERYUnLo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6ab118d6-5f2a-44e5-9ee8-6b5f1e376228",
      "metadata": {
        "id": "6ab118d6-5f2a-44e5-9ee8-6b5f1e376228"
      },
      "source": [
        "## 8. Model Evaluation\n",
        "\n",
        "To evaluate the model, we compute the perplexity, a common metric for language models. Perplexity is the exponential of the average loss:\n",
        "\n",
        "$$\n",
        "PPL = e^{\\frac{1}{N} \\sum_{i=1}^N \\text{Loss}(x_i, y_i)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_perplexity=[]\n",
        "running_loss=[]\n",
        "with tqdm.tqdm(test_dataloader, unit='batch') as tepoch:\n",
        "    for batch in tepoch:\n",
        "      tepoch.set_description(f'Epoch {epoch}')\n",
        "\n",
        "      idxs, _=batch\n",
        "      idxs=idxs.to(device).long()\n",
        "      with torch.no_grad():\n",
        "        outputs=model(idxs)\n",
        "\n",
        "      batch_size, seq_len, vocab_size=outputs.shape\n",
        "\n",
        "      outputs = outputs[:,:-1,:].reshape(-1, vocab_size)\n",
        "      targets = idxs[:,1:].reshape(-1)\n",
        "\n",
        "      # Create a mask for non-padding tokens\n",
        "      mask = targets != pad_index\n",
        "\n",
        "      # Apply mask to filter out padding tokens\n",
        "      filtered_outputs = outputs[mask]\n",
        "      filtered_targets = targets[mask]\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      perplexity = perp(outputs.reshape(batch_size, (seq_len-1), vocab_size), targets.reshape(batch_size, (seq_len-1)))\n",
        "\n",
        "      running_perplexity.append(perplexity.item())\n",
        "      running_loss.append(loss.item())\n",
        "      tepoch.set_postfix(loss=np.mean(running_loss), perplexity = np.mean(running_perplexity) )\n",
        "print(' Epoch {}  loss {}  perplexity {} '.format(epoch, np.mean(running_loss),np.mean(running_perplexity)) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgxTEz8ueapJ",
        "outputId": "34f69a41-7353-4789-9009-dbdcc802da6f"
      },
      "id": "hgxTEz8ueapJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 782/782 [01:44<00:00,  7.46batch/s, loss=3.6, perplexity=39.4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 9  loss 3.5995045534485137  perplexity 39.377061341424735 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Generating Sentences\n",
        "To generate a sentence using a pretrained LSTM (Long Short-Term Memory) model, we typically begin by providing an initial token, often referred to as the \"beginning of sequence\" (`<bos>`) token. The process involves feeding the model this token and using its output to iteratively generate subsequent tokens until an \"end of sequence\" (`<eos>`) token is produced, or a predefined sentence length is reached.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2pLRr_9DYDYn"
      },
      "id": "2pLRr_9DYDYn"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bRbHK3MBYLPi"
      },
      "id": "bRbHK3MBYLPi"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='<bos>'\n",
        "list_idxs=tokenizer.tokenize(prompt)\n",
        "\n",
        "max_len=20\n",
        "generated_text=''\n",
        "for it in range(max_len):\n",
        "  idxs=torch.from_numpy(np.array(list_idxs)).to(device).long().unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "          outputs=model(idxs)[:,-1].detach().cpu()\n",
        "          proba=torch.nn.functional.softmax(outputs,1).numpy()\n",
        "\n",
        "          next_word =\"<unk>\"\n",
        "          while next_word ==\"<unk>\":\n",
        "            next_word_idx=np.random.choice( proba.shape[1], size=1, p=proba[0])\n",
        "            next_word =tokenizer.i2w[next_word_idx[0]]\n",
        "\n",
        "          if next_word_idx[0] == tokenizer.w2i['<eos>']:\n",
        "            break\n",
        "          else:\n",
        "            list_idxs.append(next_word_idx[0])\n",
        "            generated_text+=tokenizer.i2w[next_word_idx[0]]+ ' '\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "allNLsugftsM",
        "outputId": "c5b535c7-5328-4cc0-a83d-ee20cd12d747"
      },
      "id": "allNLsugftsM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i have to say that it is grim. the sets in history; never came.<br /><br />here go, the film introduces \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Notion of Temperature in Text Generation\n",
        "\n",
        "In the context of text generation, **temperature** is a parameter that controls the level of randomness or creativity in the generated output. It is especially important in models like LSTMs, GPT, and other autoregressive language models. Temperature is applied to the model’s probability distribution during the sampling process, influencing how the next token in the sequence is chosen.\n",
        "\n",
        "#### **How Temperature Affects Text Generation**\n",
        "\n",
        "When generating text, a model predicts the next word or token based on the previous tokens. The model generates a **probability distribution** for each potential next token, which is derived from the raw outputs (logits). The temperature parameter modifies this distribution by scaling the logits before applying the **softmax** function, which converts them into probabilities.\n",
        "\n",
        "Mathematically, the temperature $T$ modifies the logits as follows:\n",
        "\n",
        "$$\n",
        "P_{\\text{adjusted}}(i) = \\frac{e^{\\frac{logits(i)}{T}}}{\\sum_{i=1}^{N} e^{\\frac{logits(i)}{T}}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $logits(i)$ are the raw model outputs for token $i$,\n",
        "- $T$ is the temperature,\n",
        "- $N$ is the total number of possible tokens.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-lC0fXqAYP93"
      },
      "id": "-lC0fXqAYP93"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='<bos> the movie was'\n",
        "list_idxs=tokenizer.tokenize(prompt)\n",
        "T=0.3\n",
        "max_len=20\n",
        "generated_text=''\n",
        "for it in range(max_len):\n",
        "  idxs=torch.from_numpy(np.array(list_idxs)).to(device).long().unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "          outputs=model(idxs)[:,-1].detach().cpu()\n",
        "          proba=torch.nn.functional.softmax(outputs/T,1).numpy()\n",
        "\n",
        "          next_word =\"<unk>\"\n",
        "          while next_word ==\"<unk>\":\n",
        "            next_word_idx=np.random.choice( proba.shape[1], size=1, p=proba[0])\n",
        "            next_word =tokenizer.i2w[next_word_idx[0]]\n",
        "\n",
        "          if next_word_idx[0] == tokenizer.w2i['<eos>']:\n",
        "            break\n",
        "          else:\n",
        "            list_idxs.append(next_word_idx[0])\n",
        "            generated_text+=tokenizer.i2w[next_word_idx[0]]+ ' '\n",
        "print(prompt+ ' '+ generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvyzzp1qi9jX",
        "outputId": "25ea721d-a4c0-4ca3-eb30-30ca9585cb01"
      },
      "id": "zvyzzp1qi9jX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos> the movie was a bit too much to be a very good film and i have to say that i was a fan \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}