{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ee8d4a-aed9-4e31-9d66-96407fd45448",
   "metadata": {},
   "source": [
    "## Understanding the Need for Sequence Modeling in NLP\n",
    "\n",
    "Natural Language Processing (NLP) tasks often involve working with sequential data, such as sentences, documents, or dialogues. Unlike static data, sequences have an inherent order and dependencies between elements. For example, the meaning of a word in a sentence often depends on the context provided by preceding or succeeding words. Traditional machine learning models struggle to capture these dependencies effectively, which limits their applicability in tasks like language modeling, machine translation, and sentiment analysis.\n",
    "\n",
    "### Limitations of Conventional Approaches\n",
    "1. **Fixed Input Size**: Standard neural networks like feedforward networks are designed for fixed-size inputs and outputs, making them unsuitable for variable-length sequences.\n",
    "2. **Context Independence**: They process inputs independently, ignoring the relationships between elements of the sequence.\n",
    "3. **Memory Constraints**: There is no mechanism to retain or leverage the context of prior inputs for decision-making.\n",
    "\n",
    "To address these limitations, **Recurrent Neural Networks (RNNs)** were developed, offering a robust framework for sequence modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad650dc-c2ed-4443-a81b-2baf244756fd",
   "metadata": {},
   "source": [
    "bs, seq_len, emb_size=2,256,300\n",
    "case1 : Pooling features 2, 300\n",
    "case2 : No Poolling 2,256*300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6b594-58c6-4c72-976f-175970efa3b5",
   "metadata": {},
   "source": [
    "f((bs, seq_len, emb_size)) -> (bs, seq_len, emb_size)\n",
    "f((bs, seq_len, emb_size)) -> (bs, sent_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214df322-1700-4823-9017-f7e2852316a4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are a type of neural network designed to process sequential data by maintaining a hidden state that captures information from previous elements of the sequence. This makes them well-suited for modeling sequences in NLP and other domains like speech recognition and time-series forecasting.\n",
    "\n",
    "### How RNNs Work\n",
    "At each time step $ t$, the RNN processes the current input $ x_t$ and the hidden state from the previous step $ h_{t-1}$, producing an updated hidden state $ h_t$. This process can be described mathematically as:\n",
    "\n",
    "$$\n",
    "h_t = f(W_h h_{t-1} + W_x x_t + b)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ W_h$ and $ W_x$ are weight matrices.\n",
    "- $ b$ is the bias term.\n",
    "- $ h_t$ is the hidden state at time $ t$, which serves as the \"memory\" of the network.\n",
    "\n",
    "The output $ y_t$ at each time step can be computed as:\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + c)\n",
    "$$\n",
    "\n",
    "Where $ W_y$ is the output weight matrix, $ c$ is the output bias, and $ g$ is the output activation function.\n",
    "\n",
    "RNNs use backpropagation through time (BPTT) to update weights, enabling them to learn dependencies over time. However, they may struggle with long-term dependencies due to issues like vanishing or exploding gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416402f-0d94-4ed9-935a-e3d075fa72a1",
   "metadata": {},
   "source": [
    "## Implementing an RNN Model in PyTorch\n",
    "\n",
    "### Step 1: Generate Random Data\n",
    "We'll create random sequential data for demonstration purposes. This data will consist of input sequences and their corresponding outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6951dde5-5e7b-4704-b7fb-378cdc8db23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "torch.Size([2, 10, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "bs, seq_len, emb_size=2,10,300\n",
    "xt=torch.rand((bs, seq_len, emb_size))\n",
    "print(xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1e0297-9bb4-4ee4-9cd7-65dafa07d04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "hidden_dim=256\n",
    "\n",
    "h0=torch.rand((bs, hidden_dim))\n",
    "print(h0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca48ff-6139-4990-8677-7d492085451f",
   "metadata": {},
   "source": [
    "### Step 2: Define an RNN Model\n",
    "Using PyTorch, we'll implement a basic RNN model to process the generated data. The model will consist of an embedding layer (for input processing), a recurrent layer, and a fully connected layer (for output generation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9625c7-12ac-4bd6-ac47-12b45691a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim ,hidden_size, output_dim):\n",
    "        super(MyRNN, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim  + hidden_size, hidden_size)\n",
    "        self.activation= torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, output_dim)\n",
    "    def forward(self, xt, h):\n",
    "        \n",
    "        bs, seq_len, emb_size=xt.shape\n",
    "        \n",
    "        hidden_outputs=[]\n",
    "        for t in range(seq_len):\n",
    "            \n",
    "            x=torch.cat([xt[:,t],h], 1)\n",
    "            h = self.linear1(x)\n",
    "            h= self.activation(h)\n",
    "            hidden_outputs.append(h)\n",
    "        hidden_states=torch.stack(hidden_outputs).permute(1,0,2) #seq_len, bs, hidden_dim     \n",
    "        predictions= self.linear2(hidden_states)    \n",
    "        return hidden_states, h , predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0057609-37de-4719-a199-3d7f38cf94a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn=MyRNN(emb_size,hidden_dim, 1)\n",
    "hidden_states, h,predictions= rnn(xt,h0)\n",
    "hidden_states.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7768376-c3ee-424a-bdaa-b6b4b52ff302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb3722-0f01-4e78-8e2f-da72080d0c4c",
   "metadata": {},
   "source": [
    "### Step 3:  Defining an RNN  in PyTorch\n",
    "Defining an RNN (Recurrent Neural Network) in PyTorch involves a few steps, typically leveraging `torch.nn` modules and classes. Here's a structured explanation:\n",
    "\n",
    "\n",
    "####  Understand Key Components\n",
    "\n",
    "1. **RNN Layer**:\n",
    "   - `nn.RNN(input_size, hidden_size, num_layers, batch_first)`: \n",
    "     - `input_size`: Number of features in the input.\n",
    "     - `hidden_size`: Number of features in the hidden state.\n",
    "     - `num_layers`: Number of stacked RNN layers.\n",
    "     - `batch_first`: If `True`, the input and output tensors are shaped as `(batch, seq, feature)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1dc9d7-7b28-4951-b8d8-1e694d890f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch first= BS, seq_len, embding\n",
    "#not batch first= seq_len, BS, embding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae46e9f-aac7-4c49-bc5e-2219ca9cbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(input_size=emb_size, hidden_size=hidden_dim, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24bb483a-4447-4b10-93c4-c556acab20a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rnn(xt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48f0c1c4-f51c-4e26-a6b0-454406959bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=rnn(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200b099f-e8e2-4a3c-86f5-3cbe8a09f30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7773f21a-304b-4e70-b9ca-85d300fd79d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b02917-e098-4b5e-91f1-6e1eab32376f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
