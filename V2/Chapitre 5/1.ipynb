{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ba56b3-fbf9-4e5d-8a95-569f6390c8bf",
   "metadata": {},
   "source": [
    "# The Vanishing Gradient Problem\n",
    "\n",
    "In standard RNNs, gradients are propagated backward through time. The gradient of the loss function with respect to earlier layers is calculated using repeated multiplications by the Jacobian matrix. When the eigenvalues of this matrix are less than 1, the gradients shrink exponentially as they propagate back, leading to the **vanishing gradient problem**.\n",
    "\n",
    "### Simplified Gradient Expression for RNN:\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial h_T} \\prod_{t=1}^T \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "If $\\|\\frac{\\partial h_t}{\\partial h_{t-1}}\\| < 1$, gradients diminish as $t$ increases.\n",
    "\n",
    "## How LSTMs Mitigate Vanishing Gradients:\n",
    "\n",
    "1. **Cell State ($C_t$):** LSTMs maintain a nearly constant error flow by introducing a cell state, which is updated additively rather than multiplicatively, preserving information over long time steps.\n",
    "2. **Gated Mechanisms:** The gates control the flow of information, allowing the model to selectively update and forget information, avoiding the uncontrolled growth or shrinkage of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740e09e-8222-4a85-a650-4e9ad0fa113a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9558e-ae13-4e87-a076-f440ee85675c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d632a0-56b7-4de3-a296-19479870658c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Long Short-Term Memory (LSTM) Cell\n",
    "\n",
    "LSTM is a type of recurrent neural network (RNN) designed to address the problem of long-term dependency, which standard RNNs struggle with due to the vanishing gradient problem. LSTMs achieve this using a unique architecture involving three main gates:\n",
    "\n",
    "1. **Forget Gate** ($f_t$): Decides what information to discard from the cell state.\n",
    "2. **Input Gate** ($i_t$): Determines what information to update in the cell state.\n",
    "3. **Output Gate** ($o_t$): Controls what information is sent to the output.\n",
    "\n",
    "## LSTM Cell Equations:\n",
    "\n",
    "### Forget Gate:\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "### Input Gate:\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "### Cell State Update:\n",
    "$$C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t$$\n",
    "\n",
    "### Output Gate:\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\cdot \\tanh(C_t)$$\n",
    "\n",
    "Here, $\\sigma$ represents the sigmoid activation function, and $\\tanh$ represents the hyperbolic tangent activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0050063a-f42d-48c7-ad2a-cad0f6515856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "bs, seq_len, embd_size, hidden_size = 2, 10, 300, 256\n",
    "\n",
    "xt=torch.rand((bs,seq_len,embd_size))\n",
    "h0=torch.rand((bs, hidden_size))\n",
    "c0=torch.rand((bs, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "802b91c5-2070-43de-a1f1-9fb0d2bd5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim ):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        self.f = torch.nn.Linear(input_dim + hidden_dim,  hidden_dim)\n",
    "        self.i = torch.nn.Linear(input_dim + hidden_dim,  hidden_dim)\n",
    "        self.c_ = torch.nn.Linear(input_dim + hidden_dim,  hidden_dim)\n",
    "        self.o = torch.nn.Linear(input_dim + hidden_dim,  hidden_dim)\n",
    "      \n",
    "    def forward(self, x,h,c):\n",
    "        hidden_states=[]\n",
    "        bs, seq_len, embd_size=x.shape\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_h=torch.cat([x[:, t], h], 1)\n",
    "            f_t=torch.sigmoid(self.f(x_h))\n",
    "            i_t=torch.sigmoid(self.f(x_h))\n",
    "            o_t=torch.sigmoid(self.f(x_h))\n",
    "            c__t=torch.tanh(self.f(x_h))\n",
    "            \n",
    "            c=(f_t*c)+ (i_t*c__t)\n",
    "            h=o_t*torch.tanh(c)\n",
    "            hidden_states.append(c)\n",
    "\n",
    "        hidden_states=torch.stack(hidden_states).permute(1,0,2)\n",
    "      \n",
    "        return hidden_states, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "740a7a3a-8c4d-433c-b74b-0f6cfcd83b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm=MyLSTM(embd_size, hidden_size)\n",
    "hidden_states, (hn,cn)=lstm(xt,h0,c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87cd741e-f233-4577-b60a-2c21de4fbb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa0ecb7d-1670-4104-ab69-f5509b29a970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256]), torch.Size([2, 256]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a92548c5-beb0-4568-8509-53813351b6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((hn,cn),1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c16470-41a1-4d35-8a81-d7702bcd47fc",
   "metadata": {},
   "source": [
    "## Pytorch LSTM Layer\n",
    "Creating an LSTM in PyTorch involves understanding both the built-in `nn.LSTM` module and how to use it for sequence modeling tasks. Here's a step-by-step guide:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Understand the `nn.LSTM` Module**\n",
    "The `nn.LSTM` module in PyTorch implements a multi-layer LSTM. It automatically handles the forward propagation of LSTM cells for entire sequences.\n",
    "\n",
    "Key parameters:\n",
    "- **`input_size`**: The number of features in the input.\n",
    "- **`hidden_size`**: The number of features in the hidden state.\n",
    "- **`num_layers`**: Number of stacked LSTM layers.\n",
    "- **`batch_first`**: If `True`, input/output tensors have shape `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Define the Model**\n",
    "An LSTM model in PyTorch typically consists of:\n",
    "- An embedding layer (optional for NLP tasks).\n",
    "- An LSTM layer (`nn.LSTM`).\n",
    "- A fully connected layer to map the LSTM output to the desired prediction size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bb158c9-e114-4253-af14-98df9ed48340",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm= torch.nn.LSTM(embd_size, hidden_size,1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d098a860-b7d2-47cd-b377-1e84843d3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, (hn,cn)=lstm(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "166a241a-830a-4fcd-97a4-0adc28bc2213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 256]), torch.Size([1, 2, 256]), torch.Size([1, 2, 256]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape, hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028e1c1-3c14-4145-a1c2-d1b6eb6033b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
