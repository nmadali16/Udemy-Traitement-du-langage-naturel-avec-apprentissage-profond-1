{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eef18bf-9fd7-4e96-b67b-12b48caffc90",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) and Bidirectional LSTM\n",
    "\n",
    "**Named Entity Recognition (NER)** is a natural language processing (NLP) task that involves identifying and classifying entities in text into predefined categories, such as:\n",
    "- Person names (e.g., \"John Doe\")\n",
    "- Organizations (e.g., \"OpenAI\")\n",
    "- Locations (e.g., \"New York\")\n",
    "- Dates and time (e.g., \"January 1, 2023\")\n",
    "\n",
    "### NER with Bidirectional LSTM\n",
    "\n",
    "A Bidirectional Long Short-Term Memory (BiLSTM) network is an effective architecture for NER because it considers the context of words in both forward and backward directions. This enables the model to capture dependencies from both preceding and succeeding words in the sequence, which is crucial for understanding context in text.\n",
    "\n",
    "#### Key Components:\n",
    "1. **Input Representation**: Words are represented as word embeddings, such as Word2Vec, GloVe, or contextual embeddings like BERT.\n",
    "2. **Bidirectional LSTM**: Processes the sequence of embeddings in both forward and backward directions, producing context-aware representations for each word.\n",
    "3. **Output Layer**: A dense layer followed by a softmax activation is used to classify each word into its respective entity class.\n",
    "\n",
    "#### Mathematical Explanation\n",
    "\n",
    "Let $ x_t $ be the embedding of the word at position $ t $ in a sequence of length $ T $.\n",
    "\n",
    "1. **Forward LSTM**:\n",
    "   The hidden state at time $ t $ is computed as:\n",
    "   $$\n",
    "   \\overrightarrow{h_t} = \\text{LSTM}(x_t, \\overrightarrow{h_{t-1}})\n",
    "  $$\n",
    "2. **Backward LSTM**:\n",
    "   Similarly, the hidden state for the backward LSTM is:\n",
    "   $$\n",
    "   \\overleftarrow{h_t} = \\text{LSTM}(x_t, \\overleftarrow{h_{t+1}})\n",
    "  $$\n",
    "3. **Concatenation**:\n",
    "   The final hidden state for each word combines both directions:\n",
    "   $$\n",
    "   h_t = \\text{concat}(\\overrightarrow{h_t}, \\overleftarrow{h_t})\n",
    "  $$\n",
    "\n",
    "4. **Classification**:\n",
    "   The output for each word is computed by applying a dense layer with softmax activation:\n",
    "   $$\n",
    "   y_t = \\text{softmax}(W \\cdot h_t + b)\n",
    "  $$\n",
    "   where $ W $ and $ b $ are trainable parameters, and $ y_t $ represents the probabilities for each entity class.\n",
    "\n",
    "#### Workflow:\n",
    "1. Preprocess the text and tokenize it.\n",
    "2. Convert tokens into embeddings.\n",
    "3. Pass embeddings through the BiLSTM.\n",
    "4. Use the output states for each token to predict entity classes.\n",
    "\n",
    "By training the BiLSTM model on annotated datasets (e.g., CoNLL-2003), the model learns to effectively recognize entities in text based on context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97e013-9593-495e-b9f5-4f9cb66a5768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf79963-8d57-49c6-a7b7-252d15f4b2a6",
   "metadata": {},
   "source": [
    "## CoNLL-2003 \n",
    "The **CoNLL-2003 dataset** is one of the most widely used datasets for Named Entity Recognition (NER). It was introduced as part of the Conference on Natural Language Learning (CoNLL) shared task in 2003. This dataset is annotated for four types of entities:\n",
    "\n",
    "1. **PER**: Person names\n",
    "2. **ORG**: Organizations\n",
    "3. **LOC**: Locations\n",
    "4. **MISC**: Miscellaneous entities that do not fit into the other categories\n",
    "\n",
    "### Dataset Structure\n",
    "The dataset consists of text data in multiple languages (e.g., English, German), but the English portion is most commonly used. It contains:\n",
    "- **Tokens**: Words from sentences.\n",
    "- **Part-of-Speech (POS) tags**: POS tags for each word.\n",
    "- **Chunk tags**: Information about syntactic chunks.\n",
    "- **NER labels**: Entity labels in the BIO format (Begin-Inside-Outside).\n",
    "\n",
    "#### Example:\n",
    "| Token   | POS   | Chunk   | NER   |\n",
    "|---------|-------|---------|-------|\n",
    "| EU      | NNP   | I-NP    | B-ORG |\n",
    "| rejects | VBZ   | I-VP    | O     |\n",
    "| German  | JJ    | I-NP    | B-MISC|\n",
    "| call    | NN    | I-NP    | O     |\n",
    "| to      | TO    | I-VP    | O     |\n",
    "| boycott | VB    | I-VP    | O     |\n",
    "| British | JJ    | I-NP    | B-MISC|\n",
    "| lamb    | NN    | I-NP    | O     |\n",
    "| .       | .     | O       | O     |\n",
    "\n",
    "### BIO Format\n",
    "The dataset uses the **BIO tagging scheme**:\n",
    "- **B**: Beginning of an entity.\n",
    "- **I**: Inside an entity.\n",
    "- **O**: Outside of any entity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fdca4-8892-4cb5-b48e-245113c0e7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8e5712-3be2-47bd-8c73-f952c52d1ae0",
   "metadata": {},
   "source": [
    "### 1. Dataset and Dataloader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a394b8-4624-4a8d-b840-874823bdb52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a35c6a7-6cfc-40c3-9324-ee7ee1189a44",
   "metadata": {},
   "source": [
    "### 2. Model Creation\n",
    "\n",
    "We use a Bidirectional LSTM (BiLSTM) model with a CRF layer or a Transformer-based model like BERT for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db4ed5-d3a2-4a65-bb7f-284a8981fa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc1be129-803e-43dc-8302-3f88ff3145a7",
   "metadata": {},
   "source": [
    "### 3. Training Loop\n",
    "\n",
    "The training loop involves forward propagation, loss computation, and backpropagation. Use `CrossEntropyLoss` for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752034c8-9747-4bdb-9906-cbd765f6b400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60458675-5e73-47da-9d2f-a384f05f0251",
   "metadata": {},
   "source": [
    "### 4. Evaluation Process\n",
    "\n",
    "Evaluate the model on the test set using metrics like accuracy, precision, recall, and F1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20364c87-24ea-4471-b92a-305573b47eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
