{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ba56b3-fbf9-4e5d-8a95-569f6390c8bf",
   "metadata": {},
   "source": [
    "# The Vanishing Gradient Problem\n",
    "\n",
    "In standard RNNs, gradients are propagated backward through time. The gradient of the loss function with respect to earlier layers is calculated using repeated multiplications by the Jacobian matrix. When the eigenvalues of this matrix are less than 1, the gradients shrink exponentially as they propagate back, leading to the **vanishing gradient problem**.\n",
    "\n",
    "### Simplified Gradient Expression for RNN:\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial h_T} \\prod_{t=1}^T \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "If $\\|\\frac{\\partial h_t}{\\partial h_{t-1}}\\| < 1$, gradients diminish as $t$ increases.\n",
    "\n",
    "## How LSTMs Mitigate Vanishing Gradients:\n",
    "\n",
    "1. **Cell State ($C_t$):** LSTMs maintain a nearly constant error flow by introducing a cell state, which is updated additively rather than multiplicatively, preserving information over long time steps.\n",
    "2. **Gated Mechanisms:** The gates control the flow of information, allowing the model to selectively update and forget information, avoiding the uncontrolled growth or shrinkage of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9558e-ae13-4e87-a076-f440ee85675c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d632a0-56b7-4de3-a296-19479870658c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Long Short-Term Memory (LSTM) Cell\n",
    "\n",
    "LSTM is a type of recurrent neural network (RNN) designed to address the problem of long-term dependency, which standard RNNs struggle with due to the vanishing gradient problem. LSTMs achieve this using a unique architecture involving three main gates:\n",
    "\n",
    "1. **Forget Gate** ($f_t$): Decides what information to discard from the cell state.\n",
    "2. **Input Gate** ($i_t$): Determines what information to update in the cell state.\n",
    "3. **Output Gate** ($o_t$): Controls what information is sent to the output.\n",
    "\n",
    "## LSTM Cell Equations:\n",
    "\n",
    "### Forget Gate:\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "### Input Gate:\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "### Cell State Update:\n",
    "$$C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t$$\n",
    "\n",
    "### Output Gate:\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\cdot \\tanh(C_t)$$\n",
    "\n",
    "Here, $\\sigma$ represents the sigmoid activation function, and $\\tanh$ represents the hyperbolic tangent activation function.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
