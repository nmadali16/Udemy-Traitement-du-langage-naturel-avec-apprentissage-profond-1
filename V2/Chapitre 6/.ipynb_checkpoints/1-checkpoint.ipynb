{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183786c4-e4c5-44de-b1a9-1f7f02ff5176",
   "metadata": {},
   "source": [
    "## Seq2Seq LSTM Model in PyTorch\n",
    "This lab demonstrates the implementation of a Sequence-to-Sequence (Seq2Seq) model using LSTM layers in PyTorch.\n",
    "\n",
    "The Seq2Seq model architecture consists of an encoder and a decoder. The encoder processes the input sequence and outputs a context vector, which the decoder uses to generate the output sequence.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Let $ x = (x_1, x_2, \\dots, x_T) $ be the input sequence, and $ y = (y_1, y_2, \\dots, y_T') $ be the target sequence. The encoder computes hidden states $ h_t $ as follows:\n",
    "$$\n",
    "h_t = f(x_t, h_{t-1})\n",
    "$$\n",
    "where $ f $ is an LSTM cell.\n",
    "\n",
    "The decoder predicts $ y_t $ based on the context vector $ c $ and previous outputs:\n",
    "$$\n",
    "y_t = g(y_{t-1}, s_t, c)\n",
    "$$\n",
    "where $ s_t $ is the decoder's hidden state, and $ g $ is another LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c9b43-6a10-4095-a4e7-5912e948db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hidden_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
    "\n",
    "        return torch.softmax(attention, dim=1)  # [batch_size, src_len]\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [src_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded: [src_len, batch_size, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # outputs: [src_len, batch_size, hidden_dim]\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # cell: [n_layers, batch_size, hidden_dim]\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim + emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        # input: [batch_size]\n",
    "        # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "        input = input.unsqueeze(0)  # [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch_size, emb_dim]\n",
    "\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)  # [batch_size, src_len]\n",
    "        attn_weights = attn_weights.unsqueeze(1)  # [batch_size, 1, src_len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hidden_dim]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs).permute(1, 0, 2)  # [1, batch_size, hidden_dim]\n",
    "\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)  # [1, batch_size, hidden_dim + emb_dim]\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        # output: [1, batch_size, hidden_dim]\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(0), context.squeeze(0)), dim=1))\n",
    "        # prediction: [batch_size, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [src_len, batch_size]\n",
    "        # trg: [trg_len, batch_size]\n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]  # First input to the decoder is the <sos> token\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)  # Get the highest probability token\n",
    "\n",
    "            input = trg[t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_DIM = 1000  # Size of the source vocabulary\n",
    "OUTPUT_DIM = 1000  # Size of the target vocabulary\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# Instantiate models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "attention = Attention(HIDDEN_DIM)\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT, attention)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg in iterator:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        # output: [trg_len, batch_size, output_dim]\n",
    "        # trg: [trg_len, batch_size]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Example usage:\n",
    "# Create your data iterators, initialize training loop, and start training the Seq2Seq model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668627b-7dc5-437d-88b5-cab92f9e6d32",
   "metadata": {},
   "source": [
    "### Generate synthetic sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff484ff1-0dc9-40a2-a75c-27332fb6babe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1951bd13-d488-4452-8806-2e1fcf4eee59",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80239249-6894-4815-9d71-5b7545ff5851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08b594-9ed5-45ef-b53a-fba175530a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74b00536-9ccf-471d-b925-19cd54da04a7",
   "metadata": {},
   "source": [
    "### Model, loss, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55f78f-b5c4-49cc-812c-a6cc3bc2a2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
