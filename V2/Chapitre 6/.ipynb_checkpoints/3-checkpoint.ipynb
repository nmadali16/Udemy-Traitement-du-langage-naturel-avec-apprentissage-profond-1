{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd965aec-636e-4321-93cc-bb1224cad2bc",
   "metadata": {
    "id": "cd965aec-636e-4321-93cc-bb1224cad2bc"
   },
   "source": [
    "### Machine Translation with Seq2Seq Model in PyTorch\n",
    "\n",
    "# Machine Translation\n",
    "Machine translation (MT) refers to the automatic translation of text from one language to another using computational models. Neural Machine Translation (NMT) systems often use Seq2Seq architectures with encoder-decoder structures and attention mechanisms.\n",
    "\n",
    "## What is Machine Translation?\n",
    "\n",
    "Machine Translation (MT) is a subfield of Natural Language Processing (NLP) focused on the automatic translation of text or speech from one language to another. Given a source sentence $ X = (x_1, x_2, \\dots, x_n) $ in a source language, the goal is to generate a target sentence $ Y = (y_1, y_2, \\dots, y_m) $ in a target language that conveys the same meaning.\n",
    "\n",
    "In mathematical terms, the task can be modeled as finding the most probable translation $ Y^* $:\n",
    "\n",
    "$$\n",
    "Y^* = \\underset{Y}{\\text{argmax}} \\; P(Y|X)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ P(Y|X) $ is the conditional probability of the target sequence given the source sequence.\n",
    "\n",
    "Seq2Seq models break this task into two steps:\n",
    "1. **Encoding:** The source sentence $ X $ is encoded into a fixed-length context vector $ C $, which represents the meaning of the source sentence.\n",
    "2. **Decoding:** The target sentence $ Y $ is generated word-by-word based on the context vector $ C $ and the previously generated words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qcRHoM8JtoFR",
   "metadata": {
    "id": "qcRHoM8JtoFR"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3Ulmi9y01TDQ",
   "metadata": {
    "id": "3Ulmi9y01TDQ"
   },
   "outputs": [],
   "source": [
    "#!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6436eb4-7d5d-48d3-ab33-771ba55b98db",
   "metadata": {
    "id": "e6436eb4-7d5d-48d3-ab33-771ba55b98db"
   },
   "source": [
    "This lab demonstrates how to:\n",
    "1. Load a translation dataset from Hugging Face's `datasets` library.\n",
    "2. Create a PyTorch Dataset and DataLoader.\n",
    "3. Build a Seq2Seq model with an encoder-decoder architecture.\n",
    "4. Train the model.\n",
    "5. Evaluate the model using BLEU score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e55611-9c47-4255-8074-9e93d1e1b7ca",
   "metadata": {
    "id": "f3e55611-9c47-4255-8074-9e93d1e1b7ca"
   },
   "source": [
    "### 1. Load  Dataset\n",
    "The **ManyThings English-French dataset** is a popular dataset used for machine translation tasks, particularly for English-to-French translations. It is simple, lightweight, and primarily intended for beginner-level experimentation with machine translation models. Below is an explanation of its key aspects:\n",
    "\n",
    "**Content:**\n",
    "   - The dataset consists of parallel sentences, meaning that each English sentence is paired with its corresponding translation in French.\n",
    "   - The translations are often short, conversational, and simple, making it ideal for introductory machine translation tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b1b175-e7e2-45e5-9362-fd80e7b2e01e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76b1b175-e7e2-45e5-9362-fd80e7b2e01e",
    "outputId": "e0c2a4b7-4d8a-48b0-9cc3-e8b469da95eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"avitri/eng-fra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "OlKWBvistv66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlKWBvistv66",
    "outputId": "a2bb4f09-96ec-416f-d2a5-d49f7de14f85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 135842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "YUcVdC0DttKX",
   "metadata": {
    "id": "YUcVdC0DttKX"
   },
   "outputs": [],
   "source": [
    "split_ds=ds[\"train\"].train_test_split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ZvaD29vat2L-",
   "metadata": {
    "id": "ZvaD29vat2L-"
   },
   "outputs": [],
   "source": [
    "train_data, test_data= split_ds['train'], split_ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "xhnAW-ByuWhX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhnAW-ByuWhX",
    "outputId": "cb665a34-21c0-4b7d-8c4a-9f0601e85bdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are taller than me.', 'Vous Ãªtes plus grands que moi.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['text'].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f1e1d7-9f2c-4220-a5a0-cfc263ab1873",
   "metadata": {
    "id": "e0f1e1d7-9f2c-4220-a5a0-cfc263ab1873"
   },
   "outputs": [],
   "source": [
    "train_tokens_A=[token for sentence_pair in  train_data for token in sentence_pair['text'].split('\\t')[0].lower().split()]\n",
    "train_tokens_B=[token for sentence_pair in  train_data for token in sentence_pair['text'].split('\\t')[1].lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4k_hJdKum1b",
   "metadata": {
    "id": "d4k_hJdKum1b"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_A=Counter(train_tokens_A)\n",
    "vocab_B=Counter(train_tokens_B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "PbQT83V4vBBK",
   "metadata": {
    "id": "PbQT83V4vBBK"
   },
   "outputs": [],
   "source": [
    "w2i_A={k: (i+4) for i, (k,v) in enumerate(vocab_A.items())}\n",
    "w2i_B={k: (i+4) for i, (k,v) in enumerate(vocab_B.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "-KXMDeafvI8s",
   "metadata": {
    "id": "-KXMDeafvI8s"
   },
   "outputs": [],
   "source": [
    "for i, k in enumerate(['<pad>','<bos>','<eos>','<unk>']):\n",
    "     w2i_A[k]=i\n",
    "     w2i_B[k]=i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704934e-0c0a-45e0-a534-fd4757190b64",
   "metadata": {
    "id": "3704934e-0c0a-45e0-a534-fd4757190b64"
   },
   "source": [
    "### 2. Dataset and Dataloader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "_4Utp4u4vy0V",
   "metadata": {
    "id": "_4Utp4u4vy0V"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train_data, w2i_A, w2i_B):\n",
    "        self.train_data = train_data\n",
    "        self.w2i_A = w2i_A\n",
    "        self.i2w_A = {v:k for (k,v) in w2i_A.items()}\n",
    "\n",
    "        self.w2i_B = w2i_B\n",
    "        self.i2w_B = {v:k for (k,v) in w2i_B.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.train_data[idx]\n",
    "        sent_A, sent_B= sample['text'].split('\\t')\n",
    "\n",
    "\n",
    "        tokens_A=[w2i_A['<bos>']]\n",
    "        for token in sent_A.lower().split():\n",
    "          if token in w2i_A:\n",
    "            tokens_A.append(w2i_A[token])\n",
    "          else:\n",
    "             tokens_A.append(w2i_A['<unk>'])\n",
    "        tokens_A.append(w2i_A['<eos>'])\n",
    "\n",
    "        tokens_B=[w2i_B['<bos>']]\n",
    "        for token in sent_B.lower().split():\n",
    "          if token in w2i_B:\n",
    "            tokens_B.append(w2i_B[token])\n",
    "          else:\n",
    "             tokens_B.append(w2i_B['<unk>'])\n",
    "        tokens_B.append(w2i_B['<eos>'])\n",
    "\n",
    "        return tokens_A, tokens_B, len(tokens_A) , len(tokens_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ZfcpUB4qxTI0",
   "metadata": {
    "id": "ZfcpUB4qxTI0"
   },
   "outputs": [],
   "source": [
    "train_dataset=CustomDataset(train_data, w2i_A, w2i_B)\n",
    "test_dataset=CustomDataset(test_data, w2i_A, w2i_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "qPBUekvbyM0-",
   "metadata": {
    "id": "qPBUekvbyM0-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for the DataLoader to handle variable-length sequences.\n",
    "    Pads input and target sequences to the maximum length in the batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list of dicts): Each element contains `source_ids`, `source_mask`, and `target_ids`.\n",
    "\n",
    "    Returns:\n",
    "        dict: Padded and batched inputs and targets.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Separate source and target sequences\n",
    "    source_ids = [torch.tensor(item[0]) for item in batch]\n",
    "    target_ids = [torch.tensor(item[1]) for item in batch]\n",
    "\n",
    "    source_lentghs=torch.tensor( [item[2] for item in batch])\n",
    "    target_lentghs = torch.tensor( [item[3] for item in batch])\n",
    "    # Pad the sequences to the maximum length in the batch\n",
    "    padded_source_ids = pad_sequence(source_ids, batch_first=True, padding_value=0)\n",
    "    padded_target_ids = pad_sequence(target_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    return padded_source_ids, padded_target_ids , source_lentghs, target_lentghs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v03t7-w2x5Y3",
   "metadata": {
    "id": "v03t7-w2x5Y3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn= my_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a3867-77d9-4960-9067-13d2cc38ed9b",
   "metadata": {
    "id": "757a3867-77d9-4960-9067-13d2cc38ed9b"
   },
   "source": [
    "### 3. Seq2Seq Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1750f4b4-35fd-4802-85f4-9d12ccbd48ad",
   "metadata": {
    "id": "1750f4b4-35fd-4802-85f4-9d12ccbd48ad"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "-dHXvxu5zsqu",
   "metadata": {
    "id": "-dHXvxu5zsqu"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, padding_idx_A, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=padding_idx_A)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3birAnS2zt_v",
   "metadata": {
    "id": "3birAnS2zt_v"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, num_embeddings_A, num_embeddings_B, hidden_size, padding_idx_A, padding_idx_B , dropout_rate=0.1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder= EncoderRNN(num_embeddings_A, hidden_size, padding_idx_A, dropout_rate)\n",
    "        self.embedding = nn.Embedding(num_embeddings_B, hidden_size, padding_idx=padding_idx_B)\n",
    "\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, num_embeddings_B)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "\n",
    "        encoder_outputs, encoder_hidden=self.encoder(input_tensor)\n",
    "        batch_size,seq_len = target_tensor.shape\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "\n",
    "            decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "847c2771-3a61-4cdd-b511-42bcb7ed774f",
   "metadata": {
    "id": "847c2771-3a61-4cdd-b511-42bcb7ed774f"
   },
   "outputs": [],
   "source": [
    "# Initialize encoder and decoder\n",
    "num_embeddings_A=len(w2i_A)\n",
    "padding_idx_A=w2i_A['<pad>']\n",
    "\n",
    "\n",
    "num_embeddings_B=len(w2i_B)\n",
    "padding_idx_B=w2i_B['<pad>']\n",
    "\n",
    "hidden_size= 256\n",
    "dropout_rate=0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model= Seq2Seq(num_embeddings_A, num_embeddings_B ,hidden_size,padding_idx_A, padding_idx_B,dropout_rate).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd911399-8528-430f-9952-7ba264885f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://drive.google.com/file/d/1p7bsrla6DGgtJNOAuqOmAn6FQ5YgrHDH/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "XkiEdene3PTp",
   "metadata": {
    "id": "XkiEdene3PTp"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'pretrained_mt.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02b197-689e-4220-ac41-427e0bd63f5f",
   "metadata": {
    "id": "0e02b197-689e-4220-ac41-427e0bd63f5f"
   },
   "source": [
    "### 5. Evaluation with BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6XFl74tv5xis",
   "metadata": {
    "id": "6XFl74tv5xis"
   },
   "source": [
    "### BLEU Score Explanation\n",
    "\n",
    "The **BLEU (Bilingual Evaluation Understudy)** score is a metric for evaluating the quality of text that has been machine-translated compared to a reference translation. It is based on the following key components:\n",
    "\n",
    "1. **N-gram Precision**: The BLEU score calculates the precision of n-grams (sequences of $n$ words) in the candidate translation compared to the reference translation.\n",
    "\n",
    "2. **Brevity Penalty (BP)**: A penalty is applied if the candidate translation is shorter than the reference translation, to discourage overly short translations.\n",
    "\n",
    "The BLEU score formula is as follows:\n",
    "\n",
    "$$\n",
    "\\text{BLEU} = \\text{BP} \\cdot \\exp\\left(\\sum_{n=1}^N w_n \\cdot \\log p_n\\right)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $p_n$ is the precision for n-grams of size $n$.  \n",
    "- $w_n$ is the weight for n-grams of size $n$, typically $w_n = \\frac{1}{N}$ (equal weights).  \n",
    "- $N$ is the maximum size of n-grams considered (e.g., 4 for BLEU-4).  \n",
    "- $\\text{BP}$ is the brevity penalty, defined as:  \n",
    "\n",
    "$$\n",
    "\\text{BP} =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } c > r \\\\\n",
    "e^{1 - \\frac{r}{c}}, & \\text{if } c \\leq r\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $c$ is the length of the candidate translation.  \n",
    "- $r$ is the length of the reference translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26f645-ec14-4c0c-b476-f31b3c4e1768",
   "metadata": {
    "id": "1f26f645-ec14-4c0c-b476-f31b3c4e1768"
   },
   "outputs": [],
   "source": [
    "#from nltk.translate.bleu_score import sentence_bleu"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
