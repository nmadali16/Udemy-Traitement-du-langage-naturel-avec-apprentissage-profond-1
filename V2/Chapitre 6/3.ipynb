{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd965aec-636e-4321-93cc-bb1224cad2bc",
      "metadata": {
        "id": "cd965aec-636e-4321-93cc-bb1224cad2bc"
      },
      "source": [
        "### Machine Translation with Seq2Seq Model in PyTorch\n",
        "\n",
        "# Machine Translation\n",
        "Machine translation (MT) refers to the automatic translation of text from one language to another using computational models. Neural Machine Translation (NMT) systems often use Seq2Seq architectures with encoder-decoder structures and attention mechanisms.\n",
        "\n",
        "## What is Machine Translation?\n",
        "\n",
        "Machine Translation (MT) is a subfield of Natural Language Processing (NLP) focused on the automatic translation of text or speech from one language to another. Given a source sentence $ X = (x_1, x_2, \\dots, x_n) $ in a source language, the goal is to generate a target sentence $ Y = (y_1, y_2, \\dots, y_m) $ in a target language that conveys the same meaning.\n",
        "\n",
        "In mathematical terms, the task can be modeled as finding the most probable translation $ Y^* $:\n",
        "\n",
        "$$\n",
        "Y^* = \\underset{Y}{\\text{argmax}} \\; P(Y|X)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ P(Y|X) $ is the conditional probability of the target sequence given the source sequence.\n",
        "\n",
        "Seq2Seq models break this task into two steps:\n",
        "1. **Encoding:** The source sentence $ X $ is encoded into a fixed-length context vector $ C $, which represents the meaning of the source sentence.\n",
        "2. **Decoding:** The target sentence $ Y $ is generated word-by-word based on the context vector $ C $ and the previously generated words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "qcRHoM8JtoFR",
      "metadata": {
        "id": "qcRHoM8JtoFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c05e227-c062-4f5a-8588-2994b6690de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "3Ulmi9y01TDQ",
      "metadata": {
        "id": "3Ulmi9y01TDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a371ee8-495a-4ed8-a4d6-7779d630e787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.11.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6436eb4-7d5d-48d3-ab33-771ba55b98db",
      "metadata": {
        "id": "e6436eb4-7d5d-48d3-ab33-771ba55b98db"
      },
      "source": [
        "This lab demonstrates how to:\n",
        "1. Load a translation dataset from Hugging Face's `datasets` library.\n",
        "2. Create a PyTorch Dataset and DataLoader.\n",
        "3. Build a Seq2Seq model with an encoder-decoder architecture.\n",
        "4. Train the model.\n",
        "5. Evaluate the model using BLEU score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e55611-9c47-4255-8074-9e93d1e1b7ca",
      "metadata": {
        "id": "f3e55611-9c47-4255-8074-9e93d1e1b7ca"
      },
      "source": [
        "### 1. Load  Dataset\n",
        "The **ManyThings English-French dataset** is a popular dataset used for machine translation tasks, particularly for English-to-French translations. It is simple, lightweight, and primarily intended for beginner-level experimentation with machine translation models. Below is an explanation of its key aspects:\n",
        "\n",
        "**Content:**\n",
        "   - The dataset consists of parallel sentences, meaning that each English sentence is paired with its corresponding translation in French.\n",
        "   - The translations are often short, conversational, and simple, making it ideal for introductory machine translation tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "76b1b175-e7e2-45e5-9362-fd80e7b2e01e",
      "metadata": {
        "id": "76b1b175-e7e2-45e5-9362-fd80e7b2e01e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"avitri/eng-fra\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "OlKWBvistv66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlKWBvistv66",
        "outputId": "aa8e4350-60b3-44f0-e997-9d0c4dc54778"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 135842\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "YUcVdC0DttKX",
      "metadata": {
        "id": "YUcVdC0DttKX"
      },
      "outputs": [],
      "source": [
        "split_ds=ds[\"train\"].train_test_split(0.1, seed=33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "ZvaD29vat2L-",
      "metadata": {
        "id": "ZvaD29vat2L-"
      },
      "outputs": [],
      "source": [
        "train_data, test_data= split_ds['train'], split_ds['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "xhnAW-ByuWhX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhnAW-ByuWhX",
        "outputId": "335f6092-3797-45f2-8c26-638bf6723853"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['They bought a box of cookies.', 'Ils ont acheté une boîte de biscuits.']"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "train_data[0]['text'].split('\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "e0f1e1d7-9f2c-4220-a5a0-cfc263ab1873",
      "metadata": {
        "id": "e0f1e1d7-9f2c-4220-a5a0-cfc263ab1873"
      },
      "outputs": [],
      "source": [
        "train_tokens_A=[token for sentence_pair in  train_data for token in sentence_pair['text'].split('\\t')[0].lower().split()]\n",
        "train_tokens_B=[token for sentence_pair in  train_data for token in sentence_pair['text'].split('\\t')[1].lower().split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "d4k_hJdKum1b",
      "metadata": {
        "id": "d4k_hJdKum1b"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "vocab_A=Counter(train_tokens_A)\n",
        "vocab_B=Counter(train_tokens_B)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "PbQT83V4vBBK",
      "metadata": {
        "id": "PbQT83V4vBBK"
      },
      "outputs": [],
      "source": [
        "w2i_A={k: (i+4) for i, (k,v) in enumerate(vocab_A.items())}\n",
        "w2i_B={k: (i+4) for i, (k,v) in enumerate(vocab_B.items())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "-KXMDeafvI8s",
      "metadata": {
        "id": "-KXMDeafvI8s"
      },
      "outputs": [],
      "source": [
        "for i, k in enumerate(['<pad>','<bos>','<eos>','<unk>']):\n",
        "     w2i_A[k]=i\n",
        "     w2i_B[k]=i"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(w2i_A),len(w2i_B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvOhHb-4JqWQ",
        "outputId": "1f9e8b8b-65ac-45c2-e072-c6e5337dbd59"
      },
      "id": "dvOhHb-4JqWQ",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22603, 37272)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3704934e-0c0a-45e0-a534-fd4757190b64",
      "metadata": {
        "id": "3704934e-0c0a-45e0-a534-fd4757190b64"
      },
      "source": [
        "### 2. Dataset and Dataloader Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "_4Utp4u4vy0V",
      "metadata": {
        "id": "_4Utp4u4vy0V"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, train_data, w2i_A, w2i_B):\n",
        "        self.train_data = train_data\n",
        "        self.w2i_A = w2i_A\n",
        "        self.i2w_A = {v:k for (k,v) in w2i_A.items()}\n",
        "\n",
        "        self.w2i_B = w2i_B\n",
        "        self.i2w_B = {v:k for (k,v) in w2i_B.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.train_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.train_data[idx]\n",
        "        sent_A, sent_B= sample['text'].split('\\t')\n",
        "\n",
        "\n",
        "        tokens_A=[w2i_A['<bos>']]\n",
        "        for token in sent_A.lower().split():\n",
        "          if token in w2i_A:\n",
        "            tokens_A.append(w2i_A[token])\n",
        "          else:\n",
        "             tokens_A.append(w2i_A['<unk>'])\n",
        "        tokens_A.append(w2i_A['<eos>'])\n",
        "\n",
        "        tokens_B=[w2i_B['<bos>']]\n",
        "        for token in sent_B.lower().split():\n",
        "          if token in w2i_B:\n",
        "            tokens_B.append(w2i_B[token])\n",
        "          else:\n",
        "             tokens_B.append(w2i_B['<unk>'])\n",
        "        tokens_B.append(w2i_B['<eos>'])\n",
        "\n",
        "        return tokens_A, tokens_B, len(tokens_A) , len(tokens_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "ZfcpUB4qxTI0",
      "metadata": {
        "id": "ZfcpUB4qxTI0"
      },
      "outputs": [],
      "source": [
        "train_dataset=CustomDataset(train_data, w2i_A, w2i_B)\n",
        "test_dataset=CustomDataset(test_data, w2i_A, w2i_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "qPBUekvbyM0-",
      "metadata": {
        "id": "qPBUekvbyM0-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for the DataLoader to handle variable-length sequences.\n",
        "    Pads input and target sequences to the maximum length in the batch.\n",
        "\n",
        "    Args:\n",
        "        batch (list of dicts): Each element contains `source_ids`, `source_mask`, and `target_ids`.\n",
        "\n",
        "    Returns:\n",
        "        dict: Padded and batched inputs and targets.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Separate source and target sequences\n",
        "    source_ids = [torch.tensor(item[0]) for item in batch]\n",
        "    target_ids = [torch.tensor(item[1]) for item in batch]\n",
        "\n",
        "    source_lentghs=torch.tensor( [item[2] for item in batch])\n",
        "    target_lentghs = torch.tensor( [item[3] for item in batch])\n",
        "    # Pad the sequences to the maximum length in the batch\n",
        "    padded_source_ids = pad_sequence(source_ids, batch_first=True, padding_value=0)\n",
        "    padded_target_ids = pad_sequence(target_ids, batch_first=True, padding_value=0)\n",
        "\n",
        "\n",
        "    return padded_source_ids, padded_target_ids , source_lentghs, target_lentghs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "v03t7-w2x5Y3",
      "metadata": {
        "id": "v03t7-w2x5Y3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn= my_collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn= my_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757a3867-77d9-4960-9067-13d2cc38ed9b",
      "metadata": {
        "id": "757a3867-77d9-4960-9067-13d2cc38ed9b"
      },
      "source": [
        "### 3. Seq2Seq Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "1750f4b4-35fd-4802-85f4-9d12ccbd48ad",
      "metadata": {
        "id": "1750f4b4-35fd-4802-85f4-9d12ccbd48ad"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "-dHXvxu5zsqu",
      "metadata": {
        "id": "-dHXvxu5zsqu"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, padding_idx_A, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=padding_idx_A)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "3birAnS2zt_v",
      "metadata": {
        "id": "3birAnS2zt_v"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, num_embeddings_A, num_embeddings_B, hidden_size, padding_idx_A, padding_idx_B , dropout_rate=0.1):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder= EncoderRNN(num_embeddings_A, hidden_size, padding_idx_A, dropout_rate)\n",
        "        self.embedding = nn.Embedding(num_embeddings_B, hidden_size, padding_idx=padding_idx_B)\n",
        "\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, num_embeddings_B)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input_tensor, target_tensor, test_phase=False):\n",
        "\n",
        "        encoder_outputs, encoder_hidden=self.encoder(input_tensor)\n",
        "\n",
        "        batch_size = input_tensor.size(0)\n",
        "\n",
        "        decoder_input= torch.ones((batch_size, 1)).long().to(input_tensor.device)\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        max_length=target_tensor.size(1)\n",
        "\n",
        "        for i in range(max_length):\n",
        "\n",
        "\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            if test_phase:\n",
        "                _, top_index= decoder_output.squeeze(1).topk(1)# bs, 1, V\n",
        "                decoder_input= top_index.detach() #bs, 1\n",
        "            else:\n",
        "              decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "847c2771-3a61-4cdd-b511-42bcb7ed774f",
      "metadata": {
        "id": "847c2771-3a61-4cdd-b511-42bcb7ed774f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7e7ec8-6144-439c-9d89-0230a6c5b8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-35f4cf4d7da3>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('pretrained_mt.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "# Initialize encoder and decoder\n",
        "num_embeddings_A=len(w2i_A)\n",
        "padding_idx_A=w2i_A['<pad>']\n",
        "\n",
        "\n",
        "num_embeddings_B=len(w2i_B)\n",
        "padding_idx_B=w2i_B['<pad>']\n",
        "\n",
        "hidden_size= 256\n",
        "dropout_rate=0.1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model= Seq2Seq(num_embeddings_A, num_embeddings_B ,hidden_size,padding_idx_A, padding_idx_B,dropout_rate).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown 1a3nVKJ8QVUb2dkevAbT6sG8Oi3UPIhSW"
      ],
      "metadata": {
        "id": "xrjpKDkASyoe"
      },
      "id": "xrjpKDkASyoe",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('pretrained_mt.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2E73HP-S6Cc",
        "outputId": "97c660cb-6704-4c48-c9cf-a364b47a9a3d"
      },
      "id": "r2E73HP-S6Cc",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-124-30c48d22b986>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('pretrained_mt.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e02b197-689e-4220-ac41-427e0bd63f5f",
      "metadata": {
        "id": "0e02b197-689e-4220-ac41-427e0bd63f5f"
      },
      "source": [
        "### 5. Evaluation with BLEU Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6XFl74tv5xis",
      "metadata": {
        "id": "6XFl74tv5xis"
      },
      "source": [
        "### BLEU Score Explanation\n",
        "\n",
        "The **BLEU (Bilingual Evaluation Understudy)** score is a metric for evaluating the quality of text that has been machine-translated compared to a reference translation. It is based on the following key components:\n",
        "\n",
        "1. **N-gram Precision**: The BLEU score calculates the precision of n-grams (sequences of $n$ words) in the candidate translation compared to the reference translation.\n",
        "\n",
        "2. **Brevity Penalty (BP)**: A penalty is applied if the candidate translation is shorter than the reference translation, to discourage overly short translations.\n",
        "\n",
        "The BLEU score formula is as follows:\n",
        "\n",
        "$$\n",
        "\\text{BLEU} = \\text{BP} \\cdot \\exp\\left(\\sum_{n=1}^N w_n \\cdot \\log p_n\\right)\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $p_n$ is the precision for n-grams of size $n$.  \n",
        "- $w_n$ is the weight for n-grams of size $n$, typically $w_n = \\frac{1}{N}$ (equal weights).  \n",
        "- $N$ is the maximum size of n-grams considered (e.g., 4 for BLEU-4).  \n",
        "- $\\text{BP}$ is the brevity penalty, defined as:  \n",
        "\n",
        "$$\n",
        "\\text{BP} =\n",
        "\\begin{cases}\n",
        "1, & \\text{if } c > r \\\\\n",
        "e^{1 - \\frac{r}{c}}, & \\text{if } c \\leq r\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $c$ is the length of the candidate translation.  \n",
        "- $r$ is the length of the reference translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoded_sent(idxs, i2w, max_length):\n",
        "  idxs=idxs[:  max_length]\n",
        "  return [i2w[idx] for idx in idxs if idx >=4]"
      ],
      "metadata": {
        "id": "orY2QGfXpQII"
      },
      "id": "orY2QGfXpQII",
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
      ],
      "metadata": {
        "id": "Mabhx-joqsCE"
      },
      "id": "Mabhx-joqsCE",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "sent_blue=[]\n",
        "gt=[]\n",
        "pred=[]\n",
        "for batch in tqdm(test_dataloader):\n",
        "    padded_source_ids, padded_target_ids , source_lentghs, target_lentghs=batch\n",
        "\n",
        "    padded_source_ids, padded_target_ids= padded_source_ids.to(device), padded_target_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      decoder_outputs, attentions= model(padded_source_ids, padded_target_ids)\n",
        "      decoder_outputs= np.argmax(torch.nn.functional.softmax(decoder_outputs,-1).detach().cpu().numpy(),-1)\n",
        "\n",
        "    padded_target_ids= padded_target_ids.detach().cpu().numpy()\n",
        "    target_lentghs = target_lentghs.detach().cpu().numpy()\n",
        "\n",
        "    for i in range(len(target_lentghs)):\n",
        "      gt.append(decoded_sent(padded_target_ids[i], train_dataset.i2w_B, target_lentghs[i]))\n",
        "      pred.append(decoded_sent(decoder_outputs[i], train_dataset.i2w_B, target_lentghs[i]))\n",
        "\n",
        "\n",
        "      sent_blue.append(sentence_bleu([gt[-1]], pred[-1], weights=(1,0,0,0)))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI0XCCirof_E",
        "outputId": "500b695c-c1ec-4d51-946e-40be4c58d3aa"
      },
      "id": "nI0XCCirof_E",
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/213 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 213/213 [00:44<00:00,  4.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(sent_blue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHXFemJ1sF5b",
        "outputId": "a709e468-b46a-4723-a6d8-9279a440088f"
      },
      "id": "MHXFemJ1sF5b",
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5369103053320133"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "vEfpVM8Uq9s_",
        "outputId": "a4572830-d0a7-420e-86f9-b07b8105880c"
      },
      "id": "vEfpVM8Uq9s_",
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "('aide-moi',)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-0a891d3ec8cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# denominator for the corpus-level modified precision.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_weight_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mp_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodified_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0mp_numerators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mp_denominators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mmodified_precision\u001b[0;34m(references, hypothesis, n)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;31m# Assigns the intersection between hypothesis and references' counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     clipped_counts = {\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mngram\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;31m# Assigns the intersection between hypothesis and references' counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     clipped_counts = {\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mngram\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m     }\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('aide-moi',)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtred_idxs=[i for i, sent in enumerate(gt) if len(sent)>10]"
      ],
      "metadata": {
        "id": "vq8POLous0EU"
      },
      "id": "vq8POLous0EU",
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bleu([gt[idxs] for idxs in filtred_idxs], [pred[idxs] for idxs in filtred_idxs] , weights=(1,0,0,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0u9WlivtEzT",
        "outputId": "e1f94f12-8940-485d-d824-c48bdac4f059"
      },
      "id": "p0u9WlivtEzT",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03663932975396779"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"translation\", model=\"hopkins/mbart-finetuned-eng-fra\", src_lang='fr', tgt_lang=\"eng\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBWSR2fit5px",
        "outputId": "c7bd636a-9a20-40a9-9efe-2edb4692ce64"
      },
      "id": "PBWSR2fit5px",
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= train_data[1]['text']"
      ],
      "metadata": {
        "id": "MnuuL7rvuY5T"
      },
      "id": "MnuuL7rvuY5T",
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split('\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO5wgMqQusrg",
        "outputId": "8e646ecf-419f-44a0-fb6b-39d3609e8dfc"
      },
      "id": "yO5wgMqQusrg",
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tom signed the documents.', 'Tom signa les documents.']"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iVULS0WunV8"
      },
      "id": "8iVULS0WunV8",
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S3Iyhwvcu61a",
        "outputId": "ceb4ffca-1971-4628-aa6e-ee7cc371c5d6"
      },
      "id": "S3Iyhwvcu61a",
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tom signa les documents.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(target)[0]['translation_text'].split()[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJxixt6qv2kI",
        "outputId": "ca48a76e-cec0-4a09-9c5d-a9a44cc1267a"
      },
      "id": "vJxixt6qv2kI",
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tom', 'signed', 'the', 'documents.']"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0DJaSnav4Z5",
        "outputId": "cabf79d7-d47a-4793-d636-9fda7c161b94"
      },
      "id": "v0DJaSnav4Z5",
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tom', 'signed', 'the', 'documents.']"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPSbYWk0t6_7",
        "outputId": "3518293a-25d3-492f-9c84-974e23eb4701"
      },
      "id": "KPSbYWk0t6_7",
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_blue=[]\n",
        "for sample in tqdm(test_data):\n",
        "  text=sample['text']\n",
        "  text= train_data[1]['text']\n",
        "  target, source =text.split('\\t')\n",
        "\n",
        "  score= sentence_bleu([target.split()], pipe(source)[0]['translation_text'].split()[1:])\n",
        "  sent_blue.append(score)\n",
        "\n",
        "print(np.mean(sent_blue))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "LWQ-TpjVwEes",
        "outputId": "e80c2fb1-f33a-49dd-9bb1-fe692128da22"
      },
      "id": "LWQ-TpjVwEes",
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/13585 [00:01<1:45:38,  2.14it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  0%|          | 23/13585 [00:09<1:29:06,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-dc8e5f9009b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'translation_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0msent_blue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m               \u001b[0mtoken\u001b[0m \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         if (\n\u001b[1;32m    169\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             )\n\u001b[1;32m   1300\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m             \u001b[0;31m# 13. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2283\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2284\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3503\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mdecoder_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_tokens_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1611\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1497\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                 )\n\u001b[1;32m   1354\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    666\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# get query proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;31m# get key, value proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;31m# `past_key_value[0].shape[2] == key_value_states.shape[1]`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_blue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR_Zk6WkwgYr",
        "outputId": "98eae431-e67b-46b0-937a-6ecba87c8af2"
      },
      "id": "JR_Zk6WkwgYr",
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(sent_blue))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz5mLAoAwc1s",
        "outputId": "3a95107e-76af-4dc7-d17a-1180c3dc1dde"
      },
      "id": "dz5mLAoAwc1s",
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0B6u82snpDNI"
      },
      "id": "0B6u82snpDNI"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}