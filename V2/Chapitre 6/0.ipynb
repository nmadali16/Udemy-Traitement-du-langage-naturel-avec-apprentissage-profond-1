{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68be2034-bb52-4407-9d43-a2a4b70e0985",
   "metadata": {},
   "source": [
    "### Explanation of Encoder-Decoder Architecture with LSTM\n",
    "\n",
    "The **Encoder-Decoder architecture** is widely used in NLP tasks like machine translation. This architecture typically uses **LSTM** networks to model sequential data effectively.\n",
    "\n",
    "#### Encoder\n",
    "The encoder processes the input sequence $ X = \\{x_1, x_2, \\ldots, x_T\\} $, where $ T $ is the length of the input sequence. Each token $ x_t $ is passed through an embedding layer and then into the LSTM network:\n",
    "\n",
    "$$\n",
    "h_t, c_t = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ h_t $: Hidden state at time step $ t $\n",
    "- $ c_t $: Cell state at time step $ t $\n",
    "- $ x_t $: Input at time step $ t $\n",
    "\n",
    "The final hidden state and cell state $(h_T, c_T)$ summarize the input sequence and serve as the initial state for the decoder.\n",
    "\n",
    "#### Decoder\n",
    "The decoder generates the output sequence $ Y = \\{y_1, y_2, \\ldots, y_{T'}\\} $, where $ T' $ is the length of the output sequence. At each time step $ t' $, the decoder takes the previous token $ y_{t'-1} $, the hidden state $ h_{t'-1} $, and the cell state $ c_{t'-1} $ as inputs:\n",
    "\n",
    "$$\n",
    "h_{t'}, c_{t'} = \\text{LSTM}(y_{t'-1}, (h_{t'-1}, c_{t'-1}))\n",
    "$$\n",
    "\n",
    "The output $ o_{t'} $ at each time step is computed as:\n",
    "\n",
    "$$\n",
    "o_{t'} = \\text{softmax}(W \\cdot h_{t'} + b)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ W $ and $ b $: Learnable weight matrix and bias\n",
    "- $ o_{t'} $: Probability distribution over the vocabulary for the next word\n",
    "\n",
    "#### Loss Function\n",
    "The model is trained using the cross-entropy loss, defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^{T'} y_t^{(i)} \\log(\\hat{y}_t^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ N $: Number of training examples\n",
    "- $ y_t^{(i)} $: Ground truth word at time $ t $ for example $ i $\n",
    "- $ \\hat{y}_t^{(i)} $: Predicted probability for $ y_t^{(i)} $\n",
    "\n",
    "This equation measures the difference between the true and predicted distributions over the target vocabulary.\n",
    "\n",
    "#### Applications\n",
    "The Encoder-Decoder framework is powerful for tasks like:\n",
    "- **Machine Translation**: E.g., translating \"I love programming\" (English) to \"J'aime programmer\" (French).\n",
    "- **Text Summarization**: Converting long articles into concise summaries.\n",
    "- **Speech Recognition**: Transforming audio signals into textual transcriptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9346f3-594b-4b06-afac-41c8e4d58aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "806a61e3-295f-4570-bef8-16a25f046c49",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Dataset Creation\n",
    "Before training an Encoder-Decoder model for tasks like machine translation, it is crucial to preprocess the data and prepare it in a format suitable for the model. This involves tokenizing, encoding, and splitting the data into training and validation sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d23da7-0dbc-4c5a-b130-c93ed7db24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English-French sentence pairs\n",
    "sentence_pairs = [\n",
    "    (\"I love programming.\", \"J'aime programmer.\"),\n",
    "    (\"The weather is nice today.\", \"Il fait beau aujourd'hui.\"),\n",
    "    (\"Can you help me?\", \"Peux-tu m'aider ?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa98c3ff-3cbc-4659-aa4c-a3377d894048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67a96a5-a5d8-4d74-b52b-a9f8271a97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_A=[token for sentence_pair in  sentence_pairs for token in sentence_pair[0].lower().split()]\n",
    "train_tokens_B=[token for sentence_pair in  sentence_pairs for token in sentence_pair[1].lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d16adfd-3dd2-4e46-9aa9-c24c9b1d4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_A=Counter(train_tokens_A)\n",
    "vocab_B=Counter(train_tokens_B)\n",
    "\n",
    "vocab_A['<pad>']=1\n",
    "vocab_B['<pad>']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf9fbb7-3575-4b4b-9362-8aee7fb5e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_A={k:i for i, (k,v) in enumerate(vocab_A.items())}\n",
    "w2i_B={k:i for i, (k,v) in enumerate(vocab_B.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e71983-e79f-4668-9511-673593332daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs_A=[torch.tensor([w2i_A[token] for token in sentence_pair[0].lower().split()]) for sentence_pair in sentence_pairs ]\n",
    "inputs_B=[torch.tensor([w2i_B[token] for token in sentence_pair[1].lower().split()]) for sentence_pair in sentence_pairs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7a7e40-3f4f-44d1-81da-5bfbe4d7ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences to create a batch\n",
    "padded_sequences_A = torch.nn.utils.rnn.pad_sequence(inputs_A, padding_value=w2i_A['<pad>'], batch_first=True)\n",
    "\n",
    "# Padding sequences to create a batch\n",
    "padded_sequences_B = torch.nn.utils.rnn.pad_sequence(inputs_B, padding_value=w2i_B['<pad>'], batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb828a-2e31-486f-a3a0-3fa996d9c533",
   "metadata": {},
   "source": [
    "#### Encoder: Bi-LSTM with Multiple Layers\n",
    "The encoder processes the input sequence $ X = \\{x_1, x_2, \\ldots, x_T\\} $ and converts it into a context vector that summarizes the sequence.\n",
    "\n",
    "1. **Bidirectional LSTM**:\n",
    "   - In a **Bi-LSTM**, two LSTM networks are used: one processes the input sequence in the forward direction, and the other processes it in the reverse direction.\n",
    "   - At each time step $ t $, the hidden states are concatenated:\n",
    "     $$\n",
    "     h_t = [h_t^{\\text{forward}}; h_t^{\\text{backward}}]\n",
    "     $$\n",
    "\n",
    "2. **Multiple Layers**:\n",
    "   - By stacking multiple Bi-LSTM layers, the encoder can learn more complex hierarchical representations of the input data.\n",
    "   - The output of one layer serves as the input for the next layer.\n",
    "\n",
    "The final outputs of the encoder are:\n",
    "- A concatenated context vector $ h_T $, summarizing the input sequence.\n",
    "- A hidden state and cell state passed to the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a56544-7116-4b50-b159-6871132985ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmadali/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_embeddings_A=len(w2i_A)\n",
    "embedding_dim = 300\n",
    "padding_idx_A=w2i_A['<pad>']\n",
    "\n",
    "hidden_size= 256\n",
    "dropout_rate=0.25\n",
    "\n",
    "embd_A= torch.nn.Embedding(num_embeddings_A, embedding_dim, padding_idx=padding_idx_A)\n",
    "encoder=torch.nn.LSTM(num_embeddings_A ,hidden_size, num_layers=1, bias=True, batch_first=True, dropout=dropout_rate, bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1f5889-7243-4aae-b786-6b7810a3aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=embd_A(padded_sequences_A)\n",
    "hidden_states, (hn,cn)= encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36b2d25-5c48-4be2-8738-d7b2357aa9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 256]), torch.Size([1, 3, 256]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape,cn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768f0cd-d86c-49c3-bf26-29f31545ed72",
   "metadata": {},
   "source": [
    "#### Decoder: LSTM\n",
    "The decoder generates the output sequence $ Y = \\{y_1, y_2, \\ldots, y_{T'}\\} $ one token at a time, based on the context vector from the encoder.\n",
    "\n",
    "1. **Input to the Decoder**:\n",
    "   - At the first time step, the decoder takes the **start-of-sequence token** (<SOS>) as input.\n",
    "   - For subsequent steps, the decoder uses either the ground truth token (during training) or its predicted token (during inference).\n",
    "\n",
    "2. **LSTM Decoder**:\n",
    "   - The decoder is a unidirectional LSTM that predicts the next token based on its current hidden state, cell state, and the previously generated token.\n",
    "   - At each time step $ t' $:\n",
    "     $$\n",
    "     h_{t'}, c_{t'} = \\text{LSTM}(y_{t'-1}, (h_{t'-1}, c_{t'-1}))\n",
    "     $$\n",
    "\n",
    "3. **Output Layer**:\n",
    "   - A fully connected layer projects the hidden state $ h_{t'} $ to the size of the target vocabulary, followed by a softmax function to produce a probability distribution over possible next tokens:\n",
    "     $$\n",
    "     o_{t'} = \\text{softmax}(W \\cdot h_{t'} + b)\n",
    "     $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8530e1f6-dc41-4a96-9020-f635b4b5b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings_B=len(w2i_B)\n",
    "padding_idx_B=w2i_B['<pad>']\n",
    "\n",
    "\n",
    "\n",
    "embd_B= torch.nn.Embedding(num_embeddings_B,  embedding_dim, padding_idx=padding_idx_B)\n",
    "decoder=torch.nn.LSTM(embedding_dim ,hidden_size, num_layers=1, bias=True, batch_first=True, dropout=dropout_rate, bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6439a3-19e9-4a05-b864-cd2b0a6ea33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455ac9fe-21da-45b3-8693-380e7f375e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=embd_B(padded_sequences_B)\n",
    "\n",
    "_,(h,c)= decoder(y,(hn,cn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3d1cd-7e6a-45d8-af2a-fd0f39aa1718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edbc2b5b-8cd9-42fe-9dea-2e45a480bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Seq2Seq(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings_A, num_embeddings_B, embedding_dim ,hidden_size,padding_idx_A, padding_idx_B,dropout_rate=0):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.embd_A= torch.nn.Embedding(num_embeddings_A, embedding_dim, padding_idx=padding_idx_A)\n",
    "        self.encoder=torch.nn.LSTM(embedding_dim ,hidden_size, num_layers=1, bias=True, batch_first=True, dropout=dropout_rate, bidirectional=False)\n",
    "\n",
    "        self.embd_B= torch.nn.Embedding(num_embeddings_B,  embedding_dim, padding_idx=padding_idx_B)\n",
    "        self.decoder=torch.nn.LSTM(embedding_dim ,hidden_size, num_layers=1, bias=True, batch_first=True, dropout=dropout_rate, bidirectional=False)\n",
    "    def forward(self, padded_sequences_A, padded_sequences_B):\n",
    "        \n",
    "        x=self.embd_A(padded_sequences_A)\n",
    "        \n",
    "        _, (hn,cn)= self.encoder(x)\n",
    "       \n",
    "        \n",
    "        y=self.embd_B(padded_sequences_B)\n",
    "        hidden_states,_= self.decoder(y,(hn,cn))\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b006c69-9c0f-4e23-ad42-6b332514c2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1398,  0.0054, -0.0953,  ...,  0.1783, -0.1887,  0.0801],\n",
       "         [ 0.0886,  0.0235, -0.1270,  ...,  0.1277,  0.0654, -0.0631],\n",
       "         [ 0.0864,  0.0385, -0.0349,  ...,  0.0870,  0.0469, -0.0298],\n",
       "         [ 0.0536,  0.0399, -0.0320,  ...,  0.0691,  0.0215, -0.0258]],\n",
       "\n",
       "        [[-0.2406,  0.0935,  0.3156,  ...,  0.2072, -0.2333, -0.1745],\n",
       "         [ 0.0865,  0.0797,  0.2805,  ...,  0.0488, -0.0515, -0.3543],\n",
       "         [-0.1559, -0.2132,  0.1505,  ..., -0.1188, -0.0030,  0.1048],\n",
       "         [-0.0455,  0.0320, -0.0599,  ..., -0.0428,  0.0447,  0.2450]],\n",
       "\n",
       "        [[-0.0681,  0.1804,  0.1041,  ..., -0.2098, -0.1247, -0.0469],\n",
       "         [-0.1781,  0.1230, -0.0583,  ..., -0.1554, -0.2372, -0.0904],\n",
       "         [-0.0829, -0.0630,  0.0046,  ..., -0.2933,  0.0115,  0.1354],\n",
       "         [-0.0460, -0.0196, -0.0365,  ..., -0.0743,  0.0064,  0.0865]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Seq2Seq(num_embeddings_A, num_embeddings_B, embedding_dim ,hidden_size,padding_idx_A, padding_idx_B,dropout_rate)\n",
    "model(padded_sequences_A, padded_sequences_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bcb1b1-1aba-4343-973b-0acca0056d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec021ce2-1ed7-4e9f-83e9-08fc6af66ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50e8ca-fe94-416e-8c59-bb1a0877b349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
