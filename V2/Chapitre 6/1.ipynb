{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd4ea0d-67a9-447d-a97c-202f4e0dc355",
   "metadata": {},
   "source": [
    "## Additive Attention Mechanism\n",
    "Bahdanau Attention, also known as Additive Attention, is a mechanism introduced by Dzmitry Bahdanau in the paper \"Neural Machine Translation by Jointly Learning to Align and Translate.\" It computes the attention weights by comparing the current decoder state with the encoder states, producing a context vector. Here's the equation for Bahdanau Attention:\n",
    "\n",
    "### 1. **Score Function**:\n",
    "The score function computes a similarity measure between the decoder hidden state ($ s_t $) and each encoder hidden state ($ h_i $):\n",
    "$$\n",
    "e_{t,i} = v_a^\\top \\tanh(W_a h_i + U_a s_t + b_a)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_i $: Encoder hidden state at step $ i $\n",
    "- $ s_t $: Decoder hidden state at step $ t $\n",
    "- $ W_a, U_a $: Weight matrices\n",
    "- $ b_a $: Bias vector\n",
    "- $ v_a $: Weight vector for producing a scalar score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240a08b5-20ea-4c2f-94ff-1f05a52dd9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "bs,seq_len, hidden_dim= 2, 10, 256\n",
    "encoder_hidden_states= torch.rand((bs,seq_len, hidden_dim))\n",
    "decoder_hn= torch.rand((bs,hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315892b9-2d0e-4b5f-b8c4-4db2c02aec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_a=nn.Linear(hidden_dim, 1)\n",
    "Wa=nn.Linear(hidden_dim, hidden_dim)\n",
    "Ua=nn.Linear(hidden_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efe9294-9479-4a2b-a539-3dec76245ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=v_a(torch.tanh(Wa(encoder_hidden_states)+Ua(decoder_hn).unsqueeze(1))).squeeze(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f26b22-ebd0-4475-837c-f43449243382",
   "metadata": {},
   "source": [
    "### 2. **Attention Weights**:\n",
    "The attention weights are computed by applying a softmax function over the scores to ensure they sum to 1:\n",
    "$$\n",
    "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^{T_x} \\exp(e_{t,j})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha_{t,i} $: Attention weight for encoder hidden state $ h_i $ at time $ t $\n",
    "- $ T_x $: Number of encoder time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6acc3251-0b8f-4bd3-b660-950ac8ec19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= torch.nn.functional.softmax(scores,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01e560b-97af-4b97-93b0-7c4a81c43041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2f591-c038-4468-9eef-61f48d430fcd",
   "metadata": {},
   "source": [
    "### 3. **Context Vector**:\n",
    "The context vector $ c_t $ is a weighted sum of the encoder hidden states, where the weights are the attention scores:\n",
    "$$\n",
    "c_t = \\sum_{i=1}^{T_x} \\alpha_{t,i} h_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23658d21-3d45-4f1d-bf35-b54b19913a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bef6fc-0603-4016-9a2e-1b57db738929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d93712-25c8-45e5-959b-b0429bd0bfd0",
   "metadata": {},
   "source": [
    "(1 x 10) (10 x 256) -> (1 x 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43162312-fa2a-4d94-bfa3-45356ae237de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b097af2c-60e9-4462-8fc1-9542a42dd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context= torch.bmm(weights.unsqueeze(1),encoder_hidden_states ).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ad4a5c-7315-4f9a-bf5a-6b16fdf0fcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46603e-f21c-4980-8b54-80dc3ffad042",
   "metadata": {},
   "source": [
    "### Old Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f34db4d-d058-425f-b5b3-7889d25857e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59769232-4c80-4141-9748-bebca8571241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English-French sentence pairs\n",
    "sentence_pairs = [\n",
    "    (\"I love programming.\", \"J'aime programmer.\"),\n",
    "    (\"The weather is nice today.\", \"Il fait beau aujourd'hui.\"),\n",
    "    (\"Can you help me?\", \"Peux-tu m'aider ?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e68c5d-0ef0-47ce-a6e3-f8925de31111",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_A=[token for sentence_pair in  sentence_pairs for token in sentence_pair[0].lower().split()]\n",
    "train_tokens_B=[token for sentence_pair in  sentence_pairs for token in sentence_pair[1].lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "787c5de8-7a44-4826-a318-06da97295859",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_A=Counter(train_tokens_A)\n",
    "vocab_B=Counter(train_tokens_B)\n",
    "\n",
    "vocab_A['<pad>']=1\n",
    "vocab_B['<pad>']=1\n",
    "\n",
    "vocab_A['<bos>']=1\n",
    "vocab_B['<bos>']=1\n",
    "\n",
    "vocab_A['<eos>']=1\n",
    "vocab_B['<eos>']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d43cc591-eb53-461e-8702-b39899f3f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_A={k:i for i, (k,v) in enumerate(vocab_A.items())}\n",
    "w2i_B={k:i for i, (k,v) in enumerate(vocab_B.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "655a9f69-8255-4511-97a4-4b2131d4e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_A=[torch.tensor([w2i_A[token] for token in ['<bos>']+ sentence_pair[0].lower().split()+['<eos>'] ]) for sentence_pair in sentence_pairs ]\n",
    "inputs_B=[torch.tensor([w2i_B[token] for token in ['<bos>']+sentence_pair[1].lower().split()+['<eos>'] ]) for sentence_pair in sentence_pairs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c4b131-7dce-4fdd-b3a0-508c50015aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences to create a batch\n",
    "padded_sequences_A = torch.nn.utils.rnn.pad_sequence(inputs_A, padding_value=w2i_A['<pad>'], batch_first=True)\n",
    "\n",
    "# Padding sequences to create a batch\n",
    "padded_sequences_B = torch.nn.utils.rnn.pad_sequence(inputs_B, padding_value=w2i_B['<pad>'], batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "467ae31d-5c60-4691-bee6-04c758ee9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d455a06-755d-4aa8-a28b-172383f88489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, padding_idx_A, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=padding_idx_A)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71c7525a-b562-4692-86c5-85ac60bead2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, num_embeddings_A, num_embeddings_B, hidden_size, padding_idx_A, padding_idx_B , dropout_rate=0.1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder= EncoderRNN(num_embeddings_A, hidden_size, padding_idx_A, dropout_rate)\n",
    "        self.embedding = nn.Embedding(num_embeddings_B, hidden_size, padding_idx=padding_idx_B)\n",
    "        \n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        \n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, num_embeddings_B)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "\n",
    "        encoder_outputs, encoder_hidden=self.encoder(input_tensor)\n",
    "        batch_size,seq_len = target_tensor.shape\n",
    "       \n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            \n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1029a31a-eff9-42dc-88e9-47cc1c4048cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings_B=len(w2i_B)\n",
    "padding_idx_B=w2i_B['<pad>']\n",
    "\n",
    "\n",
    "num_embeddings_A=len(w2i_A)\n",
    "embedding_dim = 300\n",
    "padding_idx_A=w2i_A['<pad>']\n",
    "\n",
    "hidden_size= 256\n",
    "dropout_rate=0.25\n",
    "\n",
    "model= Seq2Seq(num_embeddings_A, num_embeddings_B ,hidden_size,padding_idx_A, padding_idx_B,dropout_rate)\n",
    "outputs, attention_weights=model(padded_sequences_A, padded_sequences_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9cf7b-bc55-4eed-a4a3-1856ff510954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
