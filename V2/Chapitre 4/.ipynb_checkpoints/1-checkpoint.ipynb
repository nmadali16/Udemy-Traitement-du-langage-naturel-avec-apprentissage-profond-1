{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b489409-de82-4521-bbdb-4fca6a845f0b",
   "metadata": {},
   "source": [
    "# Word2Vec with Negative Sampling Implementation\n",
    "\n",
    "In this Jupyter notebook, we will implement the Word2Vec model using the technique of **negative sampling**. Word2Vec is a popular algorithm for learning word representations (embeddings) from large text corpora. Negative sampling is an optimization technique used to efficiently train the model by approximating the softmax function with a binary classification task.\n",
    "\n",
    "We will go through the process of:\n",
    "\n",
    "1. Preparing the text data for training.\n",
    "2. Implementing the negative sampling objective function.\n",
    "3. Training the Word2Vec model.\n",
    "4. Evaluating the learned word embeddings.\n",
    "\n",
    "Let's get started with building the model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f367f4-b091-4415-816e-a9d225f6a39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c900e35-c60c-4b3b-96c3-6cbb6688dbe4",
   "metadata": {},
   "source": [
    "To train Word2Vec with negative sampling, the formula for the objective function involves maximizing the likelihood of the context words while minimizing the likelihood of randomly sampled negative words. Here's the formula for training Word2Vec with negative sampling:\n",
    "\n",
    "### Objective Function for Word2Vec with Negative Sampling\n",
    "\n",
    "Given a target word $w_t $ and a context word $w_c $, the objective is to maximize the probability of the context word given the target word using a logistic regression model. The model outputs a probability for the pair of words to be a valid context-target pair.\n",
    "\n",
    "1. **Positive Pairs (True Context)**\n",
    "   The probability of a valid context word $w_c $ given the target word $w_t $ is:\n",
    "\n",
    "  $$\n",
    "   P(w_c | w_t) = \\sigma(v_{w_c}^T v_{w_t})\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $\\sigma(x) $ is the sigmoid function, $\\sigma(x) = \\frac{1}{1 + e^{-x}} $,\n",
    "   - $v_{w_c} $ is the vector representation of the context word $w_c $,\n",
    "   - $v_{w_t} $ is the vector representation of the target word $w_t $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5f040-3680-4c43-8058-badc07199710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8fb08ce-91b2-48c2-8bf9-c5ccd0b6e1fe",
   "metadata": {},
   "source": [
    "2. **Negative Sampling**\n",
    "   Negative sampling introduces random negative samples $w_n $ to train the model to distinguish valid word pairs from random noise. For each positive word pair $(w_t, w_c) $, we sample $k $ negative samples $w_n $. The objective function also includes the probability of the context word $w_n $ being sampled:\n",
    "\n",
    "  $$\n",
    "   P(w_n | w_t) = \\sigma(-v_{w_n}^T v_{w_t})\n",
    "   $$\n",
    "\n",
    "   The negative sign is used to push the model to reduce the similarity between negative samples and the target word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d4403-1aae-4c60-9687-d252767d9734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe4fc51e-ef67-4c34-a222-63073e32d5ee",
   "metadata": {},
   "source": [
    "3. **Final Objective Function**\n",
    "   The final objective function to maximize is:\n",
    "\n",
    "  $$\n",
    "   J(w_t, w_c) = \\log \\sigma(v_{w_c}^T v_{w_t}) + \\sum_{n=1}^k \\mathbb{E}_{w_n \\sim P(w)} \\left[ \\log \\sigma(-v_{w_n}^T v_{w_t}) \\right]\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - The first term corresponds to the positive sample (context word),\n",
    "   - The second term sums over the negative samples, where each negative sample $w_n $ is drawn from a distribution $P(w) $ (often a unigram distribution raised to a power, e.g., $P(w) = \\frac{p(w)^\\alpha}{\\sum_{w'} p(w')^\\alpha} $).\n",
    "\n",
    "By maximizing this objective, the model learns to increase the similarity between the target word vector $v_{w_t} $ and the context word vectors $v_{w_c} $, while decreasing the similarity with negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b00017-ce7a-4dab-9fe9-7a01760258f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1901ebfe-fd7d-46f5-ae49-62921bfd86c2",
   "metadata": {},
   "source": [
    "# Importing Pretrained Word2Vec Using Gensim\n",
    "\n",
    "The Gensim library is a popular Python package for natural language processing tasks, particularly for working with word embeddings such as Word2Vec. Gensim provides a straightforward way to load pretrained Word2Vec models, including Google's pretrained Word2Vec model or others in the `.bin` or `.txt` format.\n",
    "\n",
    "Hereâ€™s a step-by-step guide to import a pretrained Word2Vec model:\n",
    "\n",
    "## Steps to Import Pretrained Word2Vec\n",
    "\n",
    "1. **Install Gensim**  \n",
    "   If you haven't installed Gensim, you can install it using pip:\n",
    "   ```bash\n",
    "   pip install gensim\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3245fa-4f6f-4906-9dac-09c969240139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3e8c28c-f82c-4fb1-b64b-f64fbab15acf",
   "metadata": {},
   "source": [
    "2. **Download a Pretrained Word2Vec Model**  \n",
    "   Commonly used pretrained models include:\n",
    "   - Google's pretrained Word2Vec model: [Google News vectors](https://code.google.com/archive/p/word2vec/)\n",
    "   - Other links https://huggingface.co/fse/word2vec-google-news-300\n",
    "   - Other embeddings such as FastText, Glove, or models trained on specific datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88618b75-1ff1-44f8-be03-6f37ef312bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4f356f2-4232-42a2-817b-c888f357ed64",
   "metadata": {},
   "source": [
    "3. **Load the Pretrained Model**  \n",
    "   Use the `KeyedVectors` module from Gensim to load the pretrained model. If the model is in binary format, set `binary=True`. Otherwise, leave it as `binary=False`.\n",
    "\n",
    "   ```python\n",
    "   from gensim.models import KeyedVectors\n",
    "\n",
    "   # Path to the pretrained model\n",
    "   model_path = \"path/to/pretrained/word2vec.bin\"\n",
    "\n",
    "   # Load the model\n",
    "   word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361a75f-9fb4-463c-be8a-5964eb52396d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "847fdd2d-ea04-430a-ab42-3a74d5a33ef8",
   "metadata": {},
   "source": [
    "4. **Using the Loaded Model**  \n",
    "   Once loaded, you can use the model to:\n",
    "   - Retrieve vector representation of words:\n",
    "     ```python\n",
    "     vector = word2vec_model[\"example\"]\n",
    "     print(vector)\n",
    "     ```\n",
    "   - Find most similar words:\n",
    "     ```python\n",
    "     similar_words = word2vec_model.most_similar(\"king\", topn=5)\n",
    "     print(similar_words)\n",
    "     ```\n",
    "   - Compute similarity between words:\n",
    "     ```python\n",
    "     similarity = word2vec_model.similarity(\"king\", \"queen\")\n",
    "     print(similarity)\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1552fb-cb9b-43d9-806b-d24661df3f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd740df-08a0-4de8-8bae-4c04d9ef0284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "169ad62a-2b7c-44f9-8d23-207c587bff5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7414e-a1a6-4248-9dba-a4ed7f04db91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
