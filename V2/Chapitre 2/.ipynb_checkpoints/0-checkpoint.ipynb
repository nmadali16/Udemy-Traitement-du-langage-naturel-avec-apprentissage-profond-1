{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67ff562-efe1-405d-9eb5-ebdc7f5c7650",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Text Classification\n",
    "\n",
    "### 1. **Introduction to Naive Bayes**\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on **Bayes' Theorem**. It is called \"naive\" because it assumes that the features (words in our case) are conditionally independent given the class label. Despite this strong assumption, Naive Bayes performs surprisingly well, especially in text classification tasks.\n",
    "\n",
    "### 2. **Bayes' Theorem**\n",
    "\n",
    "At the heart of the Naive Bayes classifier is **Bayes' Theorem**, which describes the probability of a class $C$ given a set of features $X$. The theorem is expressed as:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(C|X)$ is the **posterior** probability of class $C$ given the features $X$.\n",
    "- $P(X|C)$ is the **likelihood** of observing features $X$ given the class $C$.\n",
    "- $P(C)$ is the **prior** probability of the class $C$.\n",
    "- $P(X)$ is the **evidence** or the probability of observing the features $X$ (which remains constant for all classes).\n",
    "\n",
    "### 3. **Simplification Using Naive Assumption**\n",
    "\n",
    "In the case of Naive Bayes, we make the assumption that the features are conditionally independent, given the class. This means that the probability of a feature $x_1, x_2, ..., x_n$ occurring together is the product of the individual probabilities of each feature. This simplifies our likelihood term:\n",
    "\n",
    "$$\n",
    "P(X|C) = P(x_1, x_2, ..., x_n | C) = \\prod_{i=1}^{n} P(x_i | C)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_1, x_2, ..., x_n$ are the features (in our case, the words in the document).\n",
    "- $P(x_i | C)$ is the probability of feature $x_i$ occurring given class $C$.\n",
    "\n",
    "Thus, the posterior probability becomes:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(C) \\cdot \\prod_{i=1}^{n} P(x_i | C)}{P(X)}\n",
    "$$\n",
    "\n",
    "We can ignore $P(X)$ because it is constant for all classes and doesnâ€™t affect the decision of which class is most likely. Therefore, we only need to compute:\n",
    "\n",
    "$$\n",
    "P(C|X) \\propto P(C) \\cdot \\prod_{i=1}^{n} P(x_i | C)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0c88c0-9236-4569-8c72-c2b0570442ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmadali/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f65abee-ebce-42d4-a015-6616e2272983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d76b633-8a2b-4004-bdb2-53281903e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download and load the IMDb dataset from Hugging Face\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8023f6-f4a0-4c9c-a55b-be074c38bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nmadali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Preprocess the data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe5fb5a-0b4d-4e13-bfe3-242cc06844d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text (lowercasing, removing punctuation and stopwords)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    tokens = text.split()  # Tokenize by splitting by whitespace\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4c9e9f-bc90-42bb-9def-9dfbfd539b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the training and test data\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "train_text = [preprocess_text(text) for text in train_data['text'] ]\n",
    "test_text = [preprocess_text(text) for text in test_data['text'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aea3832-83d3-40a8-8f62-f1958af1e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert text to Bag of Words\n",
    "def build_vocab(corpus):\n",
    "    vocab = defaultdict(int)\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "def vectorize_data(text, vocab):\n",
    "    \n",
    "    vector = np.zeros(len(vocab))\n",
    "    for word in text:\n",
    "            if word in vocab:\n",
    "                vector[list(vocab.keys()).index(word)] += 1\n",
    "        \n",
    "    return vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf9ca85-b65c-4314-85eb-2bf11eb362d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from training data\n",
    "vocab = build_vocab(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab499e-f6cb-4f23-aca5-31d67ff360cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. **Class Probabilities**\n",
    "\n",
    "For a given class $C$, the class probability $P(C)$ is simply the relative frequency of that class in the training data. If we have a dataset with $N$ total samples and $N_C$ samples of class $C$, the class probability is:\n",
    "\n",
    "$$\n",
    "P(C) = \\frac{N_C}{N}\n",
    "$$\n",
    "\n",
    "### 5. **Feature Likelihoods (Word Probabilities)**\n",
    "\n",
    "The likelihood $P(x_i | C)$ represents the probability of observing the word $x_i$ in class $C$. This is calculated by counting how often the word $x_i$ appears in documents of class $C$, and then dividing by the total number of words in class $C$.\n",
    "\n",
    "To avoid the issue of zero probabilities (when a word doesn't appear in the training data for a given class), we use **Laplace smoothing**, which ensures that every word has a non-zero probability. The smoothed probability of a word $x_i$ given class $C$ is:\n",
    "\n",
    "$$\n",
    "P(x_i | C) = \\frac{count(x_i, C) + 1}{|V| + count(C)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $count(x_i, C)$ is the count of how many times the word $x_i$ appears in documents of class $C$.\n",
    "- $|V|$ is the size of the vocabulary (the number of distinct words).\n",
    "- $count(C)$ is the total number of words in class $C$.\n",
    "\n",
    "This is the probability of a word $x_i$ occurring in class $C$ after smoothing.\n",
    "\n",
    "### 6. **Prediction**\n",
    "\n",
    "To classify a new document, we compute the posterior probability $P(C|X)$ for each class and choose the class with the highest posterior probability:\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\arg\\max_{C} P(C) \\cdot \\prod_{i=1}^{n} P(x_i | C)\n",
    "$$\n",
    "\n",
    "This means we calculate the posterior probabilities for each class and select the class with the highest value. The class with the highest score is the predicted label.\n",
    "\n",
    "### 7. **Final Formula for Naive Bayes Classification**\n",
    "\n",
    "Putting everything together, the Naive Bayes classifier predicts the class $C$ for a document $X = (x_1, x_2, ..., x_n)$ by maximizing the following expression:\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\arg\\max_{C} \\left( P(C) \\cdot \\prod_{i=1}^{n} P(x_i | C) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(C)$ is the class prior.\n",
    "- $P(x_i | C)$ is the likelihood of the word $x_i$ given the class $C$, smoothed using Laplace smoothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7566e0ee-fcc1-400e-910c-4b8203d133c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Naive Bayes Classifier\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_probs = {}\n",
    "        self.word_probs = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Step 4.1: Calculate class probabilities\n",
    "        class_counts = Counter(y)\n",
    "        total_count = len(y)\n",
    "        for label, count in class_counts.items():\n",
    "            self.class_probs[label] = count / total_count\n",
    "        \n",
    "        # Step 4.2: Calculate word probabilities for each class\n",
    "        word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        for (tokens, label) in tqdm(zip(X, y)):\n",
    "                    for word, count in Counter(tokens).items():\n",
    "                            word_counts[label][word] += count\n",
    "\n",
    "        \n",
    "        # Step 4.3: Apply Laplace smoothing and calculate probabilities\n",
    "        for label in word_counts:\n",
    "            total_words_in_class = sum(word_counts[label].values()) + len(vocab)\n",
    "            for word in word_counts[label]:\n",
    "                self.word_probs[label][word] = (word_counts[label][word] + 1) / total_words_in_class\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for tokens in tqdm(X):\n",
    "            class_scores = {}\n",
    "            for label, class_prob in self.class_probs.items():\n",
    "                score = np.log(class_prob)\n",
    "                for word, count in Counter(tokens).items():\n",
    "                        \n",
    "                        if word in self.word_probs[label]:\n",
    "                            score += np.log(self.word_probs[label][word])\n",
    "                        else:\n",
    "                            score += np.log(1 / (sum(self.class_probs.values()) + len(vocab)))  # Smoothing\n",
    "                class_scores[label] = score\n",
    "            predictions.append(max(class_scores, key=class_scores.get))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f072a0-627d-4482-abee-59f72451d821",
   "metadata": {},
   "source": [
    "### 8. **Example: Classification of IMDb Reviews**\n",
    "\n",
    "For example, if we have a review with the words \"great movie\", we compute the posterior probabilities for both classes \"positive\" and \"negative\" based on:\n",
    "- The prior probabilities $P(\\text{positive})$ and $P(\\text{negative})$,\n",
    "- The likelihoods $P(\\text{great} | \\text{positive})$, $P(\\text{movie} | \\text{positive})$, $P(\\text{great} | \\text{negative})$, and $P(\\text{movie} | \\text{negative})$.\n",
    "\n",
    "The class with the higher posterior probability is the predicted label for the review.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f600aa01-2034-4e12-b65f-f132e6f0004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [00:00, 38177.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train Naive Bayes classifier\n",
    "nb = NaiveBayes()\n",
    "y_train = train_data['label']\n",
    "nb.fit(train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72374886-6aa2-4e44-898d-5bcbaddcd118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:07<00:00, 3349.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Predict on the test data\n",
    "y_test = test_data['label']\n",
    "y_pred = nb.predict(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e56d4d1-6dd5-43b6-b64c-949d59a93f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate the model\n",
    "def evaluate(y_true, y_pred):\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report (Precision, Recall, F1)\n",
    "    tp, fp, fn, tn = 0, 0, 0, 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == 1 and pred == 1:\n",
    "            tp += 1\n",
    "        elif true == 0 and pred == 1:\n",
    "            fp += 1\n",
    "        elif true == 1 and pred == 0:\n",
    "            fn += 1\n",
    "        elif true == 0 and pred == 0:\n",
    "            tn += 1\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d1f89-fb93-4e2d-9ab9-55b1f12671d2",
   "metadata": {},
   "source": [
    "### 9. **Evaluation**\n",
    "\n",
    "To evaluate the performance of the Naive Bayes classifier, we use metrics such as **accuracy**, **precision**, **recall**, and **F1-score**:\n",
    "\n",
    "- **Accuracy**: Measures the overall correctness of the model.\n",
    "- **Precision**: The proportion of positive predictions that were actually positive.\n",
    "- **Recall**: The proportion of actual positives that were correctly predicted.\n",
    "- **F1-score**: The harmonic mean of precision and recall, balancing both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8979ac56-17b1-44dd-8192-d19f27719305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8288\n",
      "Precision: 0.8642\n",
      "Recall: 0.7802\n",
      "F1-Score: 0.8200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "evaluate(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775bc0e-e0a9-4478-9f68-1e6715729042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
