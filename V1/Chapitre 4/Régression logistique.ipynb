{"cells":[{"cell_type":"markdown","metadata":{"id":"4_Di-dGqJA94"},"source":["# Régression logistique\n","Dans ce cahier, nous implémenterons la régression logistique pour l'analyse des sentiments sur les tweets. Étant donné un tweet, vous déciderez s’il a un sentiment positif ou négatif. Plus précisément, vous allez :\n","\n","* Apprenez à extraire des fonctionnalités pour la régression logistique à partir de texte\n","* Implémenter la régression logistique à partir de zéro\n","* Appliquer la régression logistique sur une tâche de traitement du langage naturel\n","* Testez en utilisant votre régression logistique\n","* Effectuer une analyse des erreurs"]},{"cell_type":"markdown","metadata":{"id":"0lxUl1IvJA96"},"source":["## Importer des fonctions et des données"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Q04PTsMVJA96","executionInfo":{"status":"ok","timestamp":1698344347624,"user_tz":-120,"elapsed":8,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"baf9c40e-c84f-4011-d2e3-d4f56c595fbc","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":25}],"source":["import re\n","import nltk\n","import string\n","import numpy as np\n","import pandas as pd\n","\n","\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer\n","from nltk.corpus import twitter_samples\n","\n","\n","\n","nltk.download('twitter_samples')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"N9NFcr14JA97","executionInfo":{"status":"ok","timestamp":1698344347625,"user_tz":-120,"elapsed":5,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["def process_tweet(tweet):\n","    \"\"\"Traitement de la fonction tweet.\n","     Saisir:\n","         tweet : une chaîne contenant un tweet\n","     Sortir:\n","         tweets_clean : une liste de mots contenant le tweet traité\n","     \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    # supprimer les tickers boursiers comme $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # supprimer l'ancien texte de retweet \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # supprimer les hyperliens\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # supprimer les hashtags\n","    # en supprimant uniquement le signe dièse # du mot\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokeniser les tweets\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and  # supprimer les mots vides\n","                word not in string.punctuation):  # supprimer la ponctuation\n","\n","            stem_word = stemmer.stem(word)  # mot dérivé\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean\n","\n","\n","def build_freqs(tweets, ys):\n","    \"\"\"Construisez des fréquences.\n","     Saisir:\n","         tweets : une liste de tweets\n","         ys : un tableau m x 1 avec l'étiquette de sentiment de chaque tweet\n","             (soit 0 ou 1)\n","     Sortir:\n","         freqs : un dictionnaire mappant chaque paire (mot, sentiment) à son\n","         fréquence\n","     \"\"\"\n","    # Convertissez le tableau np en liste car zip a besoin d'un itérable.\n","    # Le squeeze est nécessaire ou la liste se termine avec un élément.\n","    # Notez également qu'il ne s'agit que d'un NOP si ys est déjà une liste.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Commencez avec un dictionnaire vide et remplissez-le en parcourant tous les tweets\n","    # et sur tous les mots traités dans chaque tweet.\n","    freqs = {}\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            if pair in freqs:\n","                freqs[pair] += 1\n","            else:\n","                freqs[pair] = 1\n","\n","    return freqs"]},{"cell_type":"markdown","metadata":{"id":"Pil44LZiJA98"},"source":["### Préparer les données\n","* Le `twitter_samples` contient des sous-ensembles de cinq mille tweets_positifs, cinq mille tweets_négatifs et l'ensemble complet de 10 000 tweets.\n","     * Si vous utilisiez les trois ensembles de données, nous introduirions des doublons des tweets positifs et des tweets négatifs.\n","     * Vous sélectionnerez uniquement les cinq mille tweets positifs et les cinq mille tweets négatifs."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"Oh7R3-noJA98","executionInfo":{"status":"ok","timestamp":1698344348634,"user_tz":-120,"elapsed":1013,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["# sélectionnez l'ensemble des tweets positifs et négatifs\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"]},{"cell_type":"markdown","metadata":{"id":"8TAzC-VmJA98"},"source":["* Répartition des tests d'entraînement : 20 % seront dans l'ensemble de tests et 80 % dans l'ensemble d'entraînement."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Vgem0URyJA98","executionInfo":{"status":"ok","timestamp":1698344348958,"user_tz":-120,"elapsed":6,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["# diviser les données en deux parties, une pour la formation et une pour les tests (ensemble de validation)\n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]\n","\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"]},{"cell_type":"markdown","metadata":{"id":"cHVNvlaKJA98"},"source":["* Créez le tableau numpy d'étiquettes positives et d'étiquettes négatives."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"BmCQF8WLJA98","executionInfo":{"status":"ok","timestamp":1698344348958,"user_tz":-120,"elapsed":5,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["# combiner des étiquettes positives et négatives\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"g6akhEknJA98","executionInfo":{"status":"ok","timestamp":1698344348958,"user_tz":-120,"elapsed":5,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"7a6d4b6e-5ca9-4027-8fcc-0357db6c897f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["train_y.shape = (8000, 1)\n","test_y.shape = (2000, 1)\n"]}],"source":["# Imprimer le train de formes et les ensembles de tests\n","print(\"train_y.shape = \" + str(train_y.shape))\n","print(\"test_y.shape = \" + str(test_y.shape))"]},{"cell_type":"markdown","metadata":{"id":"Sz9kasstJA98"},"source":["* Créez le dictionnaire de fréquences à l'aide de la fonction build_freqs importée.\n","     * Nous vous recommandons fortement d'ouvrir utils.py et de lire la fonction build_freqs pour comprendre ce qu'elle fait.\n","     * Pour afficher le répertoire de fichiers, allez dans le menu et cliquez sur Fichier->Ouvrir.\n","\n","```Python\n","    for y,tweet in zip(ys, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            if pair in freqs:\n","                freqs[pair] += 1\n","            else:\n","                freqs[pair] = 1\n","```\n","* Remarquez comment la boucle for externe parcourt chaque tweet et la boucle for interne parcourt chaque mot d'un tweet.\n","* Le dictionnaire 'freqs' est le dictionnaire de fréquences en cours de construction.\n","* La clé est le tuple (mot, étiquette), tel que (\"happy\",1) ou (\"happy\",0). La valeur stockée pour chaque clé correspond au nombre de fois où le mot « heureux » a été associé à une étiquette positive, ou combien de fois « heureux » a été associé à une étiquette négative."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"usaWpOOEJA99","executionInfo":{"status":"ok","timestamp":1698344355399,"user_tz":-120,"elapsed":6445,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"09d7adaa-3ff4-4756-e1cc-c3d5f55580a8","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 11337\n"]}],"source":["# créer un dictionnaire de fréquences\n","freqs = build_freqs(train_x, train_y)\n","\n","# vérifie le résultat\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"]},{"cell_type":"markdown","metadata":{"id":"J7svfAM3JA99"},"source":["### Traiter le tweet\n","La fonction donnée 'process_tweet' tokenise le tweet en mots individuels, supprime les mots vides et applique la radicalisation."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"1TIQQHvpJA99","executionInfo":{"status":"ok","timestamp":1698344355399,"user_tz":-120,"elapsed":33,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"9bfea7c2-7a07-4a6a-de27-817e337a3319","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Ceci est un exemple de tweet positif: \n"," #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n","\n","Ceci est un exemple de la version traitée du tweet: \n"," ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"]}],"source":["# testez la fonction ci-dessous\n","print('Ceci est un exemple de tweet positif: \\n', train_x[0])\n","print('\\nCeci est un exemple de la version traitée du tweet: \\n', process_tweet(train_x[0]))"]},{"cell_type":"markdown","metadata":{"id":"PFvABhAWJA99"},"source":["\n","## 1 – Régression logistique\n","\n","### 1.1 - Sigmoïde\n","Vous apprendrez à utiliser la régression logistique pour la classification de textes.\n","* La fonction sigmoïde est définie comme :\n","\n","$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n","\n","Il mappe l'entrée « z » à une valeur comprise entre 0 et 1 et peut donc être traitée comme une probabilité.\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"B9nL4bsZJA99","executionInfo":{"status":"ok","timestamp":1698344355399,"user_tz":-120,"elapsed":28,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["\n","def sigmoid(z):\n","    '''\n","     Saisir:\n","         z : est l'entrée (peut être un scalaire ou un tableau)\n","     Sortir:\n","         h : le sigmoïde de z\n","     '''\n","    # calculer la sigmoïde de z\n","    h = 1/(1+np.exp(-z))\n","    return h"]},{"cell_type":"markdown","metadata":{"id":"A494JN_JJA99"},"source":["#### Régression logistique : régression et sigmoïde\n","\n","La régression logistique utilise une régression linéaire régulière et applique une sigmoïde au résultat de la régression linéaire.\n","\n","Régression:\n","$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n","Notez que les valeurs $\\theta$ sont des « poids ». Si vous avez suivi la spécialisation deep learning, nous avons fait référence aux poids avec le vecteur « w ». Dans ce cours, nous utilisons une variable différente $\\theta$ pour faire référence aux poids.\n","\n","Régression logistique\n","$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n","$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n","We will refer to 'z' as the 'logits'."]},{"cell_type":"markdown","metadata":{"id":"jmjWKeJSJA99"},"source":["\n","### 1.2 - Fonction de coût et gradient\n","\n","La fonction de coût utilisée pour la régression logistique est la moyenne de la perte logarithmique pour tous les exemples de formation :\n","\n","$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n","* $m$ est le nombre d'exemples de formation\n","* $y^{(i)}$ est l'étiquette réelle de l'exemple de formation « i ».\n","* $h(z^{(i)})$ est la prédiction du modèle pour l'exemple d'entraînement « i ».\n","\n","La fonction de perte pour un seul exemple de formation est\n","$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n","\n","* Toutes les valeurs $h$ sont comprises entre 0 et 1, les logs seront donc négatifs. C'est la raison pour laquelle le facteur -1 est appliqué à la somme des deux termes de perte.\n","* Notez que lorsque le modèle prédit 1 ($h(z(\\theta)) = 1$) et que l'étiquette « y » est également 1, la perte pour cet exemple de formation est de 0.\n","* De même, lorsque le modèle prédit 0 ($h(z(\\theta)) = 0$) et que l'étiquette réelle est également 0, la perte pour cet exemple de formation est de 0.\n","* Cependant, lorsque la prédiction du modèle est proche de 1 ($h(z(\\theta)) = 0,9999$) et que l'étiquette est 0, le deuxième terme de la perte log devient un grand nombre négatif, qui est ensuite multiplié par le facteur global de -1 pour le convertir en une valeur de perte positive. $-1 \\times (1 - 0) \\times log(1 - 0,9999) \\approx 9,2$ Plus la prédiction du modèle se rapproche de 1, plus la perte est importante."]},{"cell_type":"markdown","metadata":{"id":"9yaMJqGpJA99"},"source":["* De même, si le modèle prédit proche de 0 ($h(z) = 0,0001$) mais que l'étiquette réelle est 1, le premier terme de la fonction de perte devient un grand nombre : $-1 \\times log(0,0001) \\approx 9,2$. Plus la prédiction est proche de zéro, plus la perte est importante."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"ddCRiGtoJA99","executionInfo":{"status":"ok","timestamp":1698344355399,"user_tz":-120,"elapsed":27,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"ab1d9ec0-64e8-4664-9ac1-1c01a483cafa","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.210340371976294"]},"metadata":{},"execution_count":34}],"source":["# vérifier que lorsque le modèle prédit proche de 1, mais que l'étiquette réelle est 0, la perte est une grande valeur positive\n","-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"x1k-LQn5JA9-","executionInfo":{"status":"ok","timestamp":1698344355400,"user_tz":-120,"elapsed":26,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"f4a1cdb3-b26b-44ea-90e5-00e47460bf67","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.210340371976182"]},"metadata":{},"execution_count":35}],"source":["# vérifier que lorsque le modèle prédit proche de 0 mais que l'étiquette réelle est 1, la perte est une grande valeur positive\n","-1 * np.log(0.0001) # loss is about 9.2"]},{"cell_type":"markdown","metadata":{"id":"-nrWLNsaJA9-"},"source":["#### Mettre à jour les poids\n","\n","Pour mettre à jour votre vecteur de poids $\\theta$, vous appliquerez une descente de gradient pour améliorer de manière itérative les prédictions de votre modèle.\n","Le gradient de la fonction de coût $J$ par rapport à l'un des poids $\\theta_j$ est :\n","\n","$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j \\tag{5}$$\n","* « i » est l'index de tous les exemples de formation « m ».\n","* 'j' est l'indice du poids $\\theta_j$, donc $x^{(i)}_j$ est la caractéristique associée au poids $\\theta_j$\n","\n","* Pour mettre à jour le poids $\\theta_j$, on l'ajuste en soustrayant une fraction du gradient déterminé par $\\alpha$ :\n","$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n","* Le taux d'apprentissage $\\alpha$ est une valeur que nous choisissons pour contrôler la taille d'une seule mise à jour.\n"]},{"cell_type":"markdown","metadata":{"id":"ticSdJnDJA9-"},"source":["### Exercice 2 - gradientDescent\n","Implémentez la fonction de descente de gradient.\n","* Le nombre d'itérations « num_iters » est le nombre de fois que vous utiliserez l'intégralité de l'ensemble d'entraînement.\n","* Pour chaque itération, vous calculerez la fonction de coût en utilisant tous les exemples de formation (il existe « m » exemples de formation) et pour toutes les fonctionnalités.\n","* Au lieu de mettre à jour un seul poids $\\theta_i$ à la fois, nous pouvons mettre à jour tous les poids du vecteur colonne :  \n","$$\\mathbf{\\theta} = \\begin{pmatrix}\n","\\theta_0\n","\\\\\n","\\theta_1\n","\\\\\n","\\theta_2\n","\\\\\n","\\vdots\n","\\\\\n","\\theta_n\n","\\end{pmatrix}$$\n","* $\\mathbf{\\theta}$ a des dimensions (n+1, 1), où 'n' est le nombre de fonctionnalités, et il y a un élément supplémentaire pour le terme de biais $\\theta_0$ (notez que la valeur de fonctionnalité correspondante $\\mathbf{x_0}$ est 1).\n","* Les « logits », « z », sont calculés en multipliant la matrice de caractéristiques « x » par le vecteur de poids « thêta ».  $z = \\mathbf{x}\\mathbf{\\theta}$\n","    * $\\mathbf{x}$ has dimensions (m, n+1)\n","    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n","    * $\\mathbf{z}$: has dimensions (m, 1)\n","* La prédiction 'h', est calculée en appliquant la sigmoïde à chaque élément de 'z' : $h(z) = sigmoïde(z)$, et a des dimensions (m,1).\n","* La fonction de coût $J$ est calculée en prenant le produit scalaire des vecteurs 'y' et 'log(h)'. Puisque « y » et « h » sont tous deux des vecteurs colonnes (m,1), transposez le vecteur vers la gauche, de sorte que la multiplication matricielle d'un vecteur ligne avec un vecteur colonne effectue le produit scalaire.\n","$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n","* La mise à jour de theta est également vectorisée. Parce que les dimensions de $\\mathbf{x}$ sont (m, n+1), et que $\\mathbf{h}$ et $\\mathbf{y}$ sont (m, 1), nous devons transposer le $ \\mathbf{x}$ et placez-le à gauche afin d'effectuer une multiplication matricielle, qui donne alors la réponse (n+1, 1) dont nous avons besoin :\n","$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"tVbXgRp-JA9-","executionInfo":{"status":"ok","timestamp":1698344355400,"user_tz":-120,"elapsed":25,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["\n","def gradientDescent(x, y, theta, alpha, num_iters):\n","    '''\n","     Saisir:\n","         x : matrice de caractéristiques qui est (m,n+1)\n","         y : étiquettes correspondantes de la matrice d'entrée x, dimensions (m,1)\n","         thêta : vecteur poids de dimension (n+1,1)\n","         alpha : taux d'apprentissage\n","         num_iters : nombre d'itérations pour lesquelles vous souhaitez entraîner votre modèle\n","     Sortir:\n","         J : le coût final\n","         thêta : votre vecteur de poids final\n","     Astuce : vous souhaiterez peut-être imprimer le coût pour vous assurer qu'il diminue.\n","     '''\n","\n","    # récupère 'm', le nombre de lignes dans la matrice x\n","    m = x.shape[0]\n","\n","    for i in range(0, num_iters):\n","\n","        # obtenir z, le produit scalaire de x et thêta\n","        z = np.dot(x, theta)\n","\n","        # obtenir le sigmoïde de z\n","        h = sigmoid(z)\n","\n","        # calculer la fonction de coût\n","        J = (-1/m)*(np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))\n","\n","        # mettre à jour les poids thêta\n","        theta = theta - (alpha/m)*np.dot(x.T, (h-y))\n","\n","    J = float(J)\n","    return J, theta"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"b5xr3zZCJA9-","executionInfo":{"status":"ok","timestamp":1698344355400,"user_tz":-120,"elapsed":24,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"f0bb0785-ec36-4819-bfca-66ea6da1d309","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["The cost after training is 0.67094970.\n","The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"]}],"source":["# Vérifiez la fonction\n","# Construire un scénario de test synthétique en utilisant les fonctions numpy PRNG\n","np.random.seed(1)\n","# L'entrée X est 10 x 3 avec des uns pour les termes de biais\n","tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n","# Les étiquettes Y sont 10 x 1\n","tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n","\n","# Appliquer une descente de dégradé\n","tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n","print(f\"The cost after training is {tmp_J:.8f}.\")\n","print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"]},{"cell_type":"markdown","metadata":{"id":"36hO6qCEJA9-"},"source":["<a name='2'></a>\n","## 2 - Extraire les fonctionnalités\n","\n","* Étant donné une liste de tweets, extrayez les fonctionnalités et stockez-les dans une matrice. Vous allez extraire deux fonctionnalités.\n","     * La première caractéristique est le nombre de mots positifs dans un tweet.\n","     * La deuxième caractéristique est le nombre de mots négatifs dans un tweet.\n","* Entraînez ensuite votre classificateur de régression logistique sur ces fonctionnalités.\n","* Testez le classificateur sur un ensemble de validation.\n","\n","\n","### Exercice 3 - extract_features\n","Implémentez la fonction extract_features.\n","* Cette fonction prend en compte un seul tweet.\n","* Traitez le tweet à l'aide de la fonction importée `process_tweet` et enregistrez la liste des mots du tweet.\n","* Parcourez chaque mot de la liste des mots traités\n","     * Pour chaque mot, vérifiez le dictionnaire « freqs » pour connaître le nombre de mots lorsque ce mot a une étiquette « 1 » positive. (Vérifiez la clé (mot, 1.0)\n","     * Faites de même pour le décompte lorsque le mot est associé à l'étiquette négative « 0 ». (Vérifiez la clé (mot, 0.0).)\n","\n","**Remarque :** Dans les instructions d'implémentation fournies ci-dessus, la prédiction d'être positif ou négatif dépend du vecteur de caractéristiques qui compte les mots en double - ceci est différent de ce que vous avez vu dans les vidéos de conférence."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"-0ILbfsNJA9-","executionInfo":{"status":"ok","timestamp":1698344355400,"user_tz":-120,"elapsed":23,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["\n","def extract_features(tweet, freqs, process_tweet=process_tweet):\n","    '''\n","     Saisir:\n","         tweet : une liste de mots pour un tweet\n","         freqs : un dictionnaire correspondant aux fréquences de chaque tuple (mot, label)\n","     Sortir:\n","         x : un vecteur caractéristique de dimension (1,3)\n","     '''\n","    # process_tweet tokenise, racine et supprime les mots vides\n","    word_l = process_tweet(tweet)\n","\n","    # 3 éléments pour les comptes [biais, positifs, négatifs]\n","    x = np.zeros(3)\n","\n","    # terme de biais est défini sur 1\n","    x[0] = 1\n","\n","\n","\n","    # boucle sur chaque mot de la liste de mots\n","    for word in word_l:\n","\n","        # incrémente le nombre de mots pour l'étiquette positive 1\n","        if (word, 1) in freqs:\n","            x[1] += freqs[(word, 1)]\n","\n","        # incrémente le nombre de mots pour l'étiquette négative 0\n","        if (word, 0) in freqs:\n","            x[2] += freqs[(word, 0)]\n","\n","\n","\n","    x = x[None, :]  # ajout d'une dimension de lot pour un traitement ultérieur\n","    assert(x.shape == (1, 3))\n","    return x"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"JGgMkDOYJA9-","executionInfo":{"status":"ok","timestamp":1698344355719,"user_tz":-120,"elapsed":342,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"f5358d1c-ab75-4165-d26b-599effd442d9","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.00e+00 3.02e+03 6.10e+01]]\n"]}],"source":["\n","tmp1 = extract_features(train_x[0], freqs)\n","print(tmp1)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"ri0k67LIJA9-","executionInfo":{"status":"ok","timestamp":1698344355719,"user_tz":-120,"elapsed":4,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"689a1910-b3f3-4a78-ef8e-a70730c736dd","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1. 0. 0.]]\n"]}],"source":["\n","# vérifie quand les mots ne sont pas dans le dictionnaire de fréquences\n","tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n","print(tmp2)"]},{"cell_type":"markdown","metadata":{"id":"DFJF8UqQJA9_"},"source":["\n","## 3 – Entraîner votre modèle\n","\n","Pour entraîner le modèle :\n","* Empilez les fonctionnalités de tous les exemples de formation dans une matrice X.\n","* Appelez `gradientDescent`, que vous avez implémenté ci-dessus.\n","\n","Cette rubrique vous est donnée. Veuillez le lire pour comprendre et exécuter la cellule."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"K80LS1pCJA9_","executionInfo":{"status":"ok","timestamp":1698344367312,"user_tz":-120,"elapsed":11595,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"443e36f2-2633-4265-fe1f-87b2eef10b91","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["The cost after training is 0.24215478.\n","The resulting vector of weights is [7e-08, 0.00052391, -0.00055517]\n"]}],"source":["# collecter les caractéristiques 'x' et les empiler dans une matrice 'X'\n","X = np.zeros((len(train_x), 3))\n","for i in range(len(train_x)):\n","    X[i, :]= extract_features(train_x[i], freqs)\n","\n","# libellés de formation correspondant à X\n","Y = train_y\n","\n","# Appliquer une descente de dégradé\n","J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n","print(f\"The cost after training is {J:.8f}.\")\n","print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"]},{"cell_type":"markdown","metadata":{"id":"MGUnLWrQJA9_"},"source":["\n","## 4 – Testez votre régression logistique\n","\n","Il est temps pour vous de tester votre fonction de régression logistique sur de nouvelles entrées que votre modèle n'a jamais vues auparavant.\n","\n","### Exercice 4 - prédict_tweet\n","Implémentez `predict_tweet`.\n","Prédisez si un tweet est positif ou négatif.\n","\n","* Étant donné un tweet, traitez-le, puis extrayez les fonctionnalités.\n","* Appliquez les poids appris du modèle sur les fonctionnalités pour obtenir les logits.\n","* Appliquez le sigmoïde aux logits pour obtenir la prédiction (une valeur comprise entre 0 et 1).\n","\n","$$y_{pred} = sigmoïde(\\mathbf{x} \\cdot \\theta)$$"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"-3S0ur0XJA9_","executionInfo":{"status":"ok","timestamp":1698344367313,"user_tz":-120,"elapsed":10,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["\n","def predict_tweet(tweet, freqs, theta):\n","    '''\n","     Saisir:\n","         tweet : une chaîne\n","         freqs : un dictionnaire correspondant aux fréquences de chaque tuple (mot, label)\n","         thêta : (3,1) vecteur de poids\n","     Sortir:\n","         y_pred : la probabilité qu'un tweet soit positif ou négatif\n","     '''\n","\n","\n","    # extraire les fonctionnalités du tweet et le stocker dans x\n","    x = extract_features(tweet, freqs)\n","\n","    # faire la prédiction en utilisant x et theta\n","    z = np.dot(x, theta)\n","    h = sigmoid(z)\n","    y_pred = h\n","\n","\n","\n","    return y_pred"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"wFN9hd4aJA9_","executionInfo":{"status":"ok","timestamp":1698344367313,"user_tz":-120,"elapsed":8,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"5901ef0f-f690-4dd0-dd12-19257060d37a","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["I am happy -> 0.518581\n","I am bad -> 0.494339\n","this movie should have been great. -> 0.515331\n","great -> 0.515464\n","great great -> 0.530899\n","great great great -> 0.546274\n","great great great great -> 0.561562\n"]}],"source":["# Exécutez cette cellule pour tester votre fonction\n","for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n","    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))\n"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"Sg748_owJA9_","executionInfo":{"status":"ok","timestamp":1698344367313,"user_tz":-120,"elapsed":6,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"7720f48b-3851-4357-fba7-5e11519c6154","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.8163691]])"]},"metadata":{},"execution_count":44}],"source":["# N'hésitez pas à vérifier le sentiment de votre propre tweet ci-dessous\n","my_tweet = 'I am learning :)'\n","predict_tweet(my_tweet, freqs, theta)"]},{"cell_type":"markdown","metadata":{"id":"kjRA4FVeJA9_"},"source":["<a name='4-1'></a>\n","### 4.1 - Vérifier les performances à l'aide de l'ensemble de test\n","Après avoir entraîné votre modèle à l'aide de l'ensemble d'entraînement ci-dessus, vérifiez ses performances sur des données réelles et invisibles, en le testant par rapport à l'ensemble de test.\n","\n","<a name='ex-5'></a>\n","### Exercice 5 - test_logistic_regression\n","Implémentez `test_logistic_regression`.\n","* Compte tenu des données de test et des poids de votre modèle entraîné, calculez la précision de votre modèle de régression logistique.\n","* Utilisez votre fonction 'predict_tweet' pour faire des prédictions sur chaque tweet de l'ensemble de test.\n","* Si la prédiction est > 0,5, définissez la classification « y_hat » du modèle sur 1, sinon définissez la classification « y_hat » du modèle sur 0.\n","* Une prédiction est exacte lorsque le y_hat est égal au test_y. Résumez toutes les instances où elles sont égales et divisez par m."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"cPxETNqDJA9_","executionInfo":{"status":"ok","timestamp":1698344367313,"user_tz":-120,"elapsed":4,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":["\n","def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n","    \"\"\"\n","     Saisir:\n","         test_x : une liste de tweets\n","         test_y : (m, 1) vecteur avec les labels correspondants pour la liste des tweets\n","         freqs : un dictionnaire avec la fréquence de chaque paire (ou tuple)\n","         thêta : vecteur poids de dimension (3, 1)\n","     Sortir:\n","         précision : (nombre de tweets classés correctement) / (nombre total de tweets)\n","     \"\"\"\n","\n","\n","    # la liste de stockage des prédictions\n","    y_hat = []\n","\n","    for tweet in test_x:\n","        # obtenir la prédiction du label pour le tweet\n","        y_pred = predict_tweet(tweet, freqs, theta)\n","\n","        if y_pred > 0.5:\n","            # ajouter 1.0 à la liste\n","            y_hat.append(1.0)\n","        else:\n","            # ajouter 0 à la liste\n","            y_hat.append(0.0)\n","\n","    # Avec l'implémentation ci-dessus, y_hat est une liste, mais test_y est un tableau (m,1)\n","     # convertir les deux en tableaux unidimensionnels afin de les comparer en utilisant l'opérateur '=='\n","    accuracy = np.sum(np.array(y_hat).flatten() == test_y.flatten())/len(y_hat)\n","\n","\n","\n","    return accuracy"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"6nKwQ5aiJA9_","executionInfo":{"status":"ok","timestamp":1698344368364,"user_tz":-120,"elapsed":1055,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"74005ed2-cd55-452b-8e91-b550f8b66bab","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Précision du modèle de régression logistique = 0.9950\n"]}],"source":["tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n","print(f\"Précision du modèle de régression logistique = {tmp_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"gwQO3j62JA-F"},"source":["<a name='6'></a>\n","## 6 - Prédisez avec votre propre Tweet"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"jRgCchokJA-F","executionInfo":{"status":"ok","timestamp":1698344370939,"user_tz":-120,"elapsed":5,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"6c894db3-c19e-4404-a34e-e0a02dcdf50b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n","[[0.48139084]]\n","Sentiment négatif\n"]}],"source":["# N'hésitez pas à modifier le tweet ci-dessous\n","my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n","print(process_tweet(my_tweet))\n","y_hat = predict_tweet(my_tweet, freqs, theta)\n","print(y_hat)\n","if y_hat > 0.5:\n","    print('Sentiment positif')\n","else:\n","    print('Sentiment négatif')"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"nEQVrbgcJA-F","executionInfo":{"status":"ok","timestamp":1698344370939,"user_tz":-120,"elapsed":3,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}