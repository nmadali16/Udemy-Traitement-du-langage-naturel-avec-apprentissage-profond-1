{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4521ddbd",
   "metadata": {},
   "source": [
    "Voici comment utiliser `CountVectorizer` du module `sklearn.feature_extraction.text` pour effectuer l'encodage de sac de mots (BoW) :\n",
    "\n",
    "1. **Importer les bibliothèques nécessaires** :\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "```\n",
    "\n",
    "2. **Préparer vos données textuelles** :\n",
    "\n",
    "```python\n",
    "# Exemple de données textuelles\n",
    "documents = [\n",
    "    'Ceci est le premier document.',\n",
    "    'Ce document est le deuxième document.',\n",
    "    'Et ceci est le troisième.',\n",
    "    'Est-ce le premier document ?',\n",
    "]\n",
    "```\n",
    "\n",
    "3. **Initialiser le `CountVectorizer`** :\n",
    "\n",
    "```python\n",
    "# Initialiser CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "```\n",
    "\n",
    "4. **Ajuster et transformer les données** :\n",
    "\n",
    "```python\n",
    "# Ajuster le vectorizer aux données et transformer les documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "```\n",
    "\n",
    "- `X` sera une matrice creuse où chaque ligne représente un document, et chaque colonne représente un mot unique dans le corpus.\n",
    "\n",
    "5. **Inspecter le vocabulaire** :\n",
    "\n",
    "```python\n",
    "# Obtenir le vocabulaire (mots uniques)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "print(vocabulary)\n",
    "```\n",
    "\n",
    "6. **Convertir la représentation BoW en un tableau dense (optionnel)** :\n",
    "\n",
    "```python\n",
    "# Convertir la matrice creuse en un tableau dense pour une manipulation plus facile\n",
    "X_dense = X.toarray()\n",
    "```\n",
    "\n",
    "Maintenant, `X_dense` sera un tableau numpy bidimensionnel où chaque ligne correspond à un document, et chaque colonne correspond à un mot dans le vocabulaire.\n",
    "\n",
    "7. **Convertir de nouveaux textes** :\n",
    "\n",
    "Si vous avez de nouveaux textes, vous pouvez utiliser le `CountVectorizer` déjà ajusté pour les transformer :\n",
    "\n",
    "```python\n",
    "nouveaux_documents = ['Ceci est un nouveau document.']\n",
    "X_nouveau = vectorizer.transform(nouveaux_documents)\n",
    "```\n",
    "\n",
    "8. **Utilisation de la représentation BoW** :\n",
    "\n",
    "Vous pouvez maintenant utiliser la représentation BoW résultante `X` pour des tâches comme la classification de texte, le regroupement, ou toute autre tâche d'apprentissage automatique nécessitant une entrée numérique.\n",
    "\n",
    "N'oubliez pas que le `CountVectorizer` fournit une représentation BoW basique où les comptes de chaque mot sont utilisés comme caractéristiques. Selon votre tâche spécifique et votre jeu de données, vous pourriez avoir besoin de prétraiter davantage les données textuelles (par exemple, mise en minuscule, suppression de la ponctuation, suppression des mots vides, etc.).\n",
    "\n",
    "Gardez à l'esprit que cet exemple est relativement simple. Dans des scénarios réels, vous pourriez avoir besoin de techniques plus avancées, comme l'utilisation de TF-IDF ou la considération de n-grammes, en fonction de la nature de vos données textuelles et de la tâche à accomplir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef29fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06562dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nmadali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nmadali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convertir le texte en minuscules\n",
    "    text = text.lower()\n",
    "\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenizer le texte\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "\n",
    "    # Enlever les mots vides (stopwords)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Appliquer la racinisation (stemming)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a16db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/shakespeare.txt') as f:\n",
    "\n",
    "    text = f.read()\n",
    "    #print(contents[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a686ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vect = CountVectorizer(tokenizer=preprocess_text) \n",
    "sentences=sent_tokenize(text[:30000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea160e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(tokenizer=<function preprocess_text at 0x0000026A5AF3BC10>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74495876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0indexgut', '1', '10', ..., 'youth', 'zealous', 'zip'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8179cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 1233\n",
      "Sentence 1: [[0 0 0 ... 0 0 0]]\n",
      "Sentence 1 shape:  (1, 1233)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence1 = vect.transform(sentences[:1])\n",
    "\n",
    "print('Vocabulary: %d' %len(vect.get_feature_names()))\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 1 shape: ' ,sentence1.toarray().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83f307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
