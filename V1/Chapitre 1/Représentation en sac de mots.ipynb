{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWKVfN1vbxfd"
   },
   "source": [
    "# Sac de mots\n",
    "La représentation du sac de mots (ou Bag of Words en anglais) est une méthode utilisée en traitement du langage naturel et en recherche d'informations. C'est une manière simplifiée de représenter des données textuelles où la fréquence de chaque mot dans un document est utilisée comme une caractéristique.\n",
    "\n",
    "Voici comment cela fonctionne :\n",
    "\n",
    "1. **Création du Vocabulaire** :\n",
    "   - Un vocabulaire est créé en compilant une liste de tous les mots uniques dans l'ensemble des données. Chaque mot se voit attribuer un index unique.\n",
    "\n",
    "2. **Comptage des Occurrences** :\n",
    "   - Pour chaque document, la fréquence de chaque mot dans le vocabulaire est comptée. Ce comptage est ensuite utilisé pour représenter le document.\n",
    "\n",
    "3. **Vectorisation** :\n",
    "   - La représentation obtenue est un vecteur, où chaque élément correspond au décompte d'un mot du vocabulaire. La taille du vecteur est égale au nombre de mots uniques dans le vocabulaire.\n",
    "\n",
    "4. **Représentation Creuse** :\n",
    "   - Étant donné que la plupart des documents n'utilisent qu'une petite fraction du vocabulaire disponible, les vecteurs obtenus sont généralement très creux (c'est-à-dire qu'ils sont composés principalement de zéros).\n",
    "\n",
    "Par exemple, supposons que nous ayons un petit vocabulaire : `[pomme, banane, orange, chat, chien]`, et que nous voulions représenter la phrase \"J'ai une pomme et une banane.\" La représentation en sac de mots pourrait être `[1, 1, 0, 0, 0]`, ce qui indique que les mots \"pomme\" et \"banane\" apparaissent chacun une fois, tandis que les autres mots n'apparaissent pas.\n",
    "\n",
    "La représentation en sac de mots ignore l'ordre des mots dans le texte et se concentre uniquement sur leur fréquence. Bien que ce soit une représentation simple, elle peut être utile dans diverses applications telles que la classification de texte, la recherche d'informations et l'analyse de sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1696167820249,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "libUrerSdhy1",
    "outputId": "f1183b14-ec2a-4355-ae46-e83cf6dab701"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nmadali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nmadali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OixziqRkeKpM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1696167821262,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "WWDFqJgTdjGQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convertir le texte en minuscules\n",
    "    text = text.lower()\n",
    "\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenizer le texte\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "\n",
    "    # Enlever les mots vides (stopwords)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Appliquer la racinisation (stemming)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1696167822526,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "AnUIQKTpeOLj"
   },
   "outputs": [],
   "source": [
    "with open('../data/shakespeare.txt') as f:\n",
    "\n",
    "    text = f.read()\n",
    "    #print(contents[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6563,
     "status": "ok",
     "timestamp": 1696167830854,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "CYEn5icmeWWs"
   },
   "outputs": [],
   "source": [
    "# Tokenizer et prétraiter le texte\n",
    "tokens = preprocess_text(text)\n",
    "\n",
    "# Construire le vocabulaire\n",
    "vocabulary = set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2307,
     "status": "ok",
     "timestamp": 1696167909455,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "6iSLsKhjdb3c",
    "outputId": "d0e9a5c2-4e9f-467a-f778-ed0525867cc7"
   },
   "outputs": [],
   "source": [
    "# Tokenizer la première phrase\n",
    "sentences = sent_tokenize(text, language='english')\n",
    "\n",
    "\n",
    "# Extraire la représentation sac de mots (BoW)\n",
    "sentence_tokens = preprocess_text(sentences[0])\n",
    "bow = {word: sentence_tokens.count(word) for word in vocabulary}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulaire:\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Représentation BoW:\", bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696167927514,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "TcGEvoiEfGjC",
    "outputId": "902f081c-4bb8-4844-c68c-a070f8fbd870"
   },
   "outputs": [],
   "source": [
    "sentence_tokens.count('100th')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPitK6lksFAHyh/rDX/irf+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
