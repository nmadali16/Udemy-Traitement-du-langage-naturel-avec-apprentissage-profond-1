{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c6f37a",
   "metadata": {
    "id": "c6c6f37a"
   },
   "source": [
    "# Le stemming et la lemmatisation\n",
    "\n",
    "En français, le stemming et la lemmatisation sont des techniques de traitement de texte visant à réduire les mots à leur forme de base (ou forme canonique) pour faciliter l'analyse et la recherche de texte. Voici les définitions :\n",
    "\n",
    "1. **Stemming (Racinisation)** :\n",
    "   Le stemming est le processus de réduction des mots à leur radical ou \"stem\". Cela signifie qu'il supprime les préfixes et les suffixes des mots pour obtenir une forme de base. Par exemple, le mot \"mangerait\" serait ramené à \"mange\" en utilisant le stemming.\n",
    "\n",
    "2. **Lemmatization (Lemmatisation)** :\n",
    "   La lemmatisation, quant à elle, est un processus similaire au stemming, mais plus sophistiqué. Il tient compte de la morphologie des mots et essaie de les ramener à leur forme canonique, le \"lemme\". Par exemple, le mot \"mangerait\" serait lemmatisé en \"manger\".\n",
    "\n",
    "Maintenant, pour effectuer le stemming et la lemmatisation en français avec la bibliothèque NLTK, voici comment procéder :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c236a6",
   "metadata": {
    "id": "72c236a6"
   },
   "source": [
    "Pour effectuer le stemming et la lemmatisation en français avec la bibliothèque SpaCy, suivez ces étapes :\n",
    "\n",
    "\n",
    "1. **Stemming en français avec SpaCy** :\n",
    "\n",
    "   SpaCy n'inclut pas une fonction de stemming, mais il offre une lemmatisation performante qui couvre déjà une grande partie des besoins.\n",
    "\n",
    "2. **Lemmatisation en français avec SpaCy** :\n",
    "\n",
    "   ```python\n",
    "   import spacy\n",
    "\n",
    "   # Chargez le modèle pour le français\n",
    "   nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "   mot = \"mangerait\"\n",
    "   doc = nlp(mot)\n",
    "\n",
    "   # Accédez à la lemmatisation\n",
    "   lemme = doc[0].lemma_\n",
    "   print(lemme)\n",
    "   ```\n",
    "\n",
    "   Cela affichera \"manger\" car c'est la forme canonique de \"mangerait\".\n",
    "\n",
    "Avec ces étapes, vous serez en mesure d'appliquer la lemmatisation en français à l'aide de la bibliothèque SpaCy. Pour le stemming, vous devrez utiliser une autre bibliothèque spécialisée, car SpaCy est principalement conçu pour la lemmatisation et l'analyse syntaxique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68ec6f0f",
   "metadata": {
    "executionInfo": {
     "elapsed": 855,
     "status": "ok",
     "timestamp": 1696097647501,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "68ec6f0f"
   },
   "outputs": [],
   "source": [
    "# Effectuer des importations standards :\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7qfvfl-BTsBS",
   "metadata": {
    "id": "7qfvfl-BTsBS"
   },
   "outputs": [],
   "source": [
    "are, is, -> be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e02f3d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696097647502,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "9e02f3d6",
    "outputId": "245b4cbb-d4d1-4cef-abe3-f2f83789c770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John \t PROPN \t 11174346320140919546 \t John\n",
      "Adam \t PROPN \t 14264057329400597350 \t Adam\n",
      "  \t SPACE \t 8532415787641010193 \t  \n",
      "is \t AUX \t 10382539506755952630 \t be\n",
      "one \t NUM \t 17454115351911680600 \t one\n",
      "the \t DET \t 7425985699627899538 \t the\n",
      "researcher \t NOUN \t 1317581537614213870 \t researcher\n",
      "who \t PRON \t 3876862883474502309 \t who\n",
      "invent \t VERB \t 5373681334090504585 \t invent\n",
      "the \t DET \t 7425985699627899538 \t the\n",
      "direction \t NOUN \t 895834437038626927 \t direction\n",
      "of \t ADP \t 886050111519832510 \t of\n",
      "way \t NOUN \t 6878210874361030284 \t way\n",
      "towards \t ADP \t 9315050841437086371 \t towards\n",
      "success \t NOUN \t 16089821935113899987 \t success\n",
      "! \t PUNCT \t 17494803046312582752 \t !\n"
     ]
    }
   ],
   "source": [
    "var1 = nlp(u\"John Adam  is one the researcher who invent the direction of way towards success!\")\n",
    "\n",
    "for token in var1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "XR5IHi4hSoP6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1696097648007,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "XR5IHi4hSoP6",
    "outputId": "a6164b4d-ad73-4d8d-f9bc-38ae42a8dd48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "meeting \t VERB \t 6880656908171229526 \t meet\n",
      "him \t PRON \t 1655312771067108281 \t he\n",
      "tomorrow \t NOUN \t 3573583789758258062 \t tomorrow\n",
      "at \t ADP \t 11667289587015813222 \t at\n",
      "the \t DET \t 7425985699627899538 \t the\n",
      "meeting \t NOUN \t 14798207169164081740 \t meeting\n",
      ". \t PUNCT \t 12646065887601541794 \t .\n"
     ]
    }
   ],
   "source": [
    "var1 = nlp(u\"I am meeting him tomorrow at the meeting.\")\n",
    "\n",
    "for token in var1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917aebc6",
   "metadata": {
    "id": "917aebc6"
   },
   "source": [
    "\n",
    "\n",
    "### Utilisation de NLTK pour le stemming et la lemmatisation en français\n",
    "\n",
    "1. **Installation** :\n",
    "\n",
    "   Assurez-vous d'avoir la bibliothèque NLTK installée dans votre environnement Python.\n",
    "\n",
    "   ```python\n",
    "   !pip install nltk\n",
    "   ```\n",
    "\n",
    "2. **Téléchargement des ressources spécifiques au français** :\n",
    "\n",
    "   Pour utiliser les fonctionnalités de stemming et de lemmatisation en français, vous devez télécharger les ressources spécifiques. Vous pouvez le faire avec les commandes suivantes :\n",
    "\n",
    "   ```python\n",
    "   import nltk\n",
    "   nltk.download('punkt')\n",
    "   nltk.download('averaged_perceptron_tagger')\n",
    "   nltk.download('wordnet')\n",
    "   nltk.download('omw')\n",
    "   ```\n",
    "\n",
    "3. **Stemming en français** :\n",
    "\n",
    "   ```python\n",
    "   from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "   stemmer = FrenchStemmer()\n",
    "   mot = \"mangerait\"\n",
    "   stem = stemmer.stem(mot)\n",
    "   print(stem)\n",
    "   ```\n",
    "\n",
    "   Cela affichera \"mang\" car il s'agit du radical de \"mangerait\".\n",
    "\n",
    "4. **Lemmatisation en français** :\n",
    "\n",
    "   ```python\n",
    "   from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "   lemmatizer = WordNetLemmatizer()\n",
    "   mot = \"mangerait\"\n",
    "   lemme = lemmatizer.lemmatize(mot)\n",
    "   print(lemme)\n",
    "   ```\n",
    "\n",
    "   Cela affichera \"manger\" car c'est la forme canonique de \"mangerait\".\n",
    "\n",
    "Avec ces étapes, vous serez en mesure d'appliquer le stemming et la lemmatisation en français à l'aide de la bibliothèque NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5aff16ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1696097648007,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "5aff16ce",
    "outputId": "1a0a0426-3e08-4091-c703-f03b3d055c34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('popular')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "073a31b4",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1696097648007,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "073a31b4"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcc983ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1696097648008,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "dcc983ca",
    "outputId": "36af9313-9b84-4102-b110-43e1a399f916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e8ca5a9",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1696097648008,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "8e8ca5a9"
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
    "               the world have come and invaded us, captured our lands, conquered our minds.\n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
    "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
    "               We have not grabbed their land, their culture,\n",
    "               their history and tried to enforce our way of life on them.\n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "               I see four milestones in my career\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bf9f4b6",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1696097648008,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "6bf9f4b6"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "GG9lOvDSUdGb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1696098017428,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "GG9lOvDSUdGb",
    "outputId": "d27ad1f2-1b7f-42b5-be3f-aecf2eaab51a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
       " 'from alexander onwards , greeks , turks , moguls , portuguese , british , french , dutch , came looted u , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquered anyone .',\n",
       " 'we grabbed land , culture , history tried enforce way life .',\n",
       " 'my good fortune worked three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeeded dr. brahm prakash , father nuclear material .',\n",
       " 'i lucky worked three closely consider great opportunity life .',\n",
       " 'i see four milestone career']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45cfea53",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1696097648008,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "45cfea53"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4b85791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1696097648008,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "f4b85791",
    "outputId": "17d06671-6902-4c59-b0e0-2ac7d1f87f6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c6f71",
   "metadata": {
    "id": "cf5c6f71"
   },
   "source": [
    "**Problème :** La représentation intermédiaire produite du mot peut n'avoir aucune signification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8280ed6",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1696097648008,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "d8280ed6"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "NU3V4bJ7S7oJ",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1696097648008,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "NU3V4bJ7S7oJ"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "UJ4rBK6MS6MO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1696097648009,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "UJ4rBK6MS6MO",
    "outputId": "de327db3-d94e-49a3-a102-7faaa9c2b2f0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I have three visions for India.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1e5bbf6",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1696097648009,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "b1e5bbf6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word).lower() for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "084e9310",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1696097648009,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "084e9310",
    "outputId": "ee482422-15a4-4328-ce25-6970a84a87ef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'i three vision india .'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
