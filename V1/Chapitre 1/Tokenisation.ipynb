{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ZiMiVUKdITdS",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1696094978146,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "ZiMiVUKdITdS"
   },
   "outputs": [],
   "source": [
    "#!gdown --id 1YAbKs98vGWgJqL6ZNN0W1-Pa-ar4ZrGV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00721884",
   "metadata": {
    "id": "00721884"
   },
   "source": [
    "# 01 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92784c59",
   "metadata": {
    "id": "92784c59"
   },
   "source": [
    "*  La tokenization en traitement du langage naturel (NLP) fait référence au processus de diviser un texte en unités plus petites, appelées \"tokens\".\n",
    "\n",
    "* Un token peut être soit un mot, soit un caractère, selon le niveau de granularité choisi.\n",
    "\n",
    "* La tokenisation est une étape fondamentale dans le traitement automatique du langage naturel, car elle permet de traiter et d'analyser le texte de manière plus précise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08eb2b03",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1696094979104,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "08eb2b03"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2821c470",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1696095313340,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "2821c470",
    "outputId": "10df4254-25c7-4b91-f18c-7bf7746d4e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the 100th Etext file presented by Project Gutenberg, and\\n', 'is presented in cooperation with World Library, Inc., from their\\n', 'Library of the Future and Shakespeare CDROMS.  Project Gutenberg\\n', 'often releases Etexts that are NOT placed in the Public Domain!!\\n', '\\n', 'Shakespeare\\n', '\\n', '*This Etext has certain copyright implications you should read!*\\n', '\\n', '<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\\n', 'SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\\n', 'PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\\n', 'WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\\n', 'DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\\n', 'PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\\n', 'COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\\n', 'SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\\n', '\\n', '*Project Gutenberg is proud to cooperate with The World Library*\\n', 'in the presentation of The Complete Works of William Shakespeare\\n', 'for your reading for education and entertainment.  HOWEVER, THIS\\n', 'IS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARY\\n', 'OF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAY\\n', 'BE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!\\n', 'TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED!!\\n', '\\n', '\\n', '**Welcome To The World of Free Plain Vanilla Electronic Texts**\\n', '\\n', '**Etexts Readable By Both Humans and By Computers, Since 1971**\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('shakespeare.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    print(lines[:30])\n",
    "    #contents = f.read()\n",
    "    #print(contents[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "D-pKtvJ8KOkc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696095338669,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "D-pKtvJ8KOkc",
    "outputId": "df4724c5-2aba-4036-833e-0b03ce679cad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124456"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1aaf579",
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1696095349621,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "e1aaf579"
   },
   "outputs": [],
   "source": [
    "text=lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ncaE0UIMKTdk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1696095354232,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "ncaE0UIMKTdk",
    "outputId": "24faf5b8-5762-43c7-df30-e9e8823d2cd1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This is the 100th Etext file presented by Project Gutenberg, and\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cb51c57",
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1696095409303,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "3cb51c57"
   },
   "outputs": [],
   "source": [
    "# divisé en mots par espace blanc\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "R8lpF1G_Klns",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1696095433022,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "R8lpF1G_Klns",
    "outputId": "52eb2ed5-b889-4a14-a68c-7fe521fa3fe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'the',\n",
       " '100th',\n",
       " 'Etext',\n",
       " 'file',\n",
       " 'presented',\n",
       " 'by',\n",
       " 'Project',\n",
       " 'Gutenberg,',\n",
       " 'and']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac0e88",
   "metadata": {
    "id": "b1ac0e88"
   },
   "source": [
    "# Expression régulière\n",
    "Une expression régulière, souvent abrégée en regex, est un outil puissant pour la recherche de motifs dans les chaînes de caractères. Elle vous permet de chercher, de faire correspondre et de manipuler du texte en fonction de motifs spécifiques. Les expressions régulières sont largement utilisées dans des tâches telles que le traitement de texte, la validation de données et le raclage web.\n",
    "\n",
    "En Python, vous pouvez travailler avec des expressions régulières en utilisant le module `re`. Voici un bref aperçu de comment utiliser les expressions régulières en Python :\n",
    "\n",
    "1. **Importer le module `re`** :\n",
    "\n",
    "   ```python\n",
    "   import re\n",
    "   ```\n",
    "\n",
    "2. **Compiler une Expression Régulière** :\n",
    "\n",
    "   Avant d'utiliser une expression régulière, il est recommandé de la compiler d'abord. Ce processus permet à Python d'optimiser et de sauvegarder l'expression régulière pour une utilisation ultérieure.\n",
    "\n",
    "   ```python\n",
    "   motif = re.compile(r'motif_ici')\n",
    "   ```\n",
    "\n",
    "   Le `r` avant la chaîne de motif indique une \"chaîne brute\", ce qui permet d'éviter les problèmes avec les barres obliques inverses.\n",
    "\n",
    "3. **Utilisation de `re.search()`** :\n",
    "\n",
    "   La fonction `re.search()` parcourt une chaîne de caractères, cherchant tout emplacement où le motif d'expression régulière correspond.\n",
    "\n",
    "   ```python\n",
    "   correspondance = motif.search(texte)\n",
    "   if correspondance:\n",
    "       print(\"Motif trouvé\")\n",
    "   else:\n",
    "       print(\"Motif non trouvé\")\n",
    "   ```\n",
    "\n",
    "4. **Utilisation de `re.findall()`** :\n",
    "\n",
    "   Cette fonction trouve toutes les occurrences du motif dans une chaîne de caractères et les renvoie sous forme de liste de chaînes de caractères.\n",
    "\n",
    "   ```python\n",
    "   correspondances = motif.findall(texte)\n",
    "   ```\n",
    "\n",
    "5. **Utilisation de `re.sub()`** :\n",
    "\n",
    "   La fonction `re.sub()` vous permet de remplacer les occurrences du motif par une chaîne de caractères spécifiée.\n",
    "\n",
    "   ```python\n",
    "   nouveau_texte = re.sub(motif, remplacement, texte)\n",
    "   ```\n",
    "\n",
    "   Par exemple :\n",
    "\n",
    "   ```python\n",
    "   nouveau_texte = re.sub(r'\\d', 'X', 'Bonjour 123')\n",
    "   # Résultat : 'Bonjour XXX'\n",
    "   ```\n",
    "\n",
    "6. **Syntaxe des Motifs** :\n",
    "\n",
    "   Les expressions régulières utilisent des caractères spéciaux pour définir des motifs. Par exemple :\n",
    "   - `.` correspond à n'importe quel caractère sauf un saut de ligne.\n",
    "   - `*` correspond à 0 ou plusieurs occurrences du caractère précédent.\n",
    "   - `+` correspond à 1 ou plusieurs occurrences du caractère précédent.\n",
    "   - `^` correspond au début d'une chaîne de caractères.\n",
    "   - `$` correspond à la fin d'une chaîne de caractères.\n",
    "   - `[ ]` correspond à l'un des caractères inclus.\n",
    "   - `( )` groupe les motifs ensemble.\n",
    "\n",
    "7. **Indicateurs** :\n",
    "\n",
    "   La fonction `re.compile()` peut prendre en option des indicateurs pour modifier le comportement du motif. Par exemple, `re.IGNORECASE` rend le motif insensible à la casse.\n",
    "\n",
    "   ```python\n",
    "   motif = re.compile(r'motif_ici', re.IGNORECASE)\n",
    "   ```\n",
    "\n",
    "Rappelez-vous, les expressions régulières peuvent être très puissantes mais peuvent aussi devenir complexes, surtout pour des motifs compliqués. Il est souvent conseillé de tester vos motifs avec diverses entrées pour vous assurer qu'ils se comportent comme prévu. De plus, il existe de nombreux outils et ressources en ligne disponibles pour vous aider à construire et tester des expressions régulières."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "azaOMuiBLY0q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1696095638450,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "azaOMuiBLY0q",
    "outputId": "e59f92ad-5f97-49a0-e934-d8389b0bba33"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This is the 100th Etext file presented by Project Gutenberg, and\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2edfefdc",
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1696095644970,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "2edfefdc"
   },
   "outputs": [],
   "source": [
    "#créer une expression régulière\n",
    "re_line_break = re.compile('\\n' )\n",
    "#applique l'expression régulière\n",
    "text=re_line_break.sub('',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "jzWBW6OdLYb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1696095695272,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "jzWBW6OdLYb7",
    "outputId": "94da4497-e29a-4016-f885-d31a4587cee0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[%s]' % re.escape(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd56dea7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1696095735090,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "cd56dea7",
    "outputId": "49c7a03a-039e-4b5a-bf69-f9a5de910e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', '100th', 'Etext', 'file', 'presented', 'by', 'Project', 'Gutenberg', 'and']\n"
     ]
    }
   ],
   "source": [
    "# préparer l'expression régulière pour le filtrage des caractères\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# supprimer la ponctuation de chaque mot\n",
    "stripped = [re_punc.sub('', w) for w in words]\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f115621e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1696095807930,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "f115621e",
    "outputId": "09ffd5c5-16dc-4a06-9e26-ce50c5a0128e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', '100th', 'Etext', 'file', 'presented', 'by', 'Project', 'Gutenberg,', 'and']\n"
     ]
    }
   ],
   "source": [
    "# string.printable inverse de string.ponctuation\n",
    "\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "result = [re_print.sub('', w) for w in words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "X-6iyuZHMJBi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1696095847332,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "X-6iyuZHMJBi",
    "outputId": "6ca67a77-e163-4349-8a49-91bc8d9a411a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'THe'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7faf6ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1696095833611,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "e7faf6ea",
    "outputId": "7cf6121c-c972-49de-cd6f-64e81486dcef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', '100th', 'etext', 'file', 'presented', 'by', 'project', 'gutenberg,', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Cas de normalisation\n",
    "# divisé en mots par espace blanc\n",
    "words = text.split()\n",
    "# convertir en minuscule\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b407ff",
   "metadata": {
    "id": "03b407ff"
   },
   "source": [
    "# Utiliser une bibliothèque tierce pour la tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3dc57a",
   "metadata": {
    "id": "1c3dc57a"
   },
   "source": [
    "La bibliothèque NLTK (Natural Language Toolkit) est une bibliothèque Python qui fournit des outils pour travailler avec des données textuelles. Elle offre un large éventail de fonctionnalités pour le traitement du langage naturel, telles que la tokenisation, l'analyse syntaxique, la lemmatisation, la classification de texte, etc.\n",
    "\n",
    "Pour effectuer la tokenisation en utilisant NLTK en français, vous pouvez suivre ces étapes :\n",
    "\n",
    "1. **Installer NLTK** : Si vous ne l'avez pas déjà installé, vous pouvez le faire en exécutant la commande suivante dans votre terminal :\n",
    "\n",
    "```python\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "2. **Importer la bibliothèque** : Dans votre script Python, importez la bibliothèque NLTK.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "```\n",
    "\n",
    "3. **Télécharger les ressources nécessaires** : NLTK fournit un certain nombre de ressources (comme des corpus, des modèles de tokenization, etc.) qui peuvent être téléchargées pour une utilisation ultérieure. Pour la tokenization en français, vous pouvez télécharger les modèles spécifiques en exécutant :\n",
    "\n",
    "```python\n",
    "nltk.download('punkt')\n",
    "```\n",
    "\n",
    "4. **Utiliser la tokenization** : Une fois que vous avez téléchargé les ressources, vous pouvez utiliser la fonction `word_tokenize` pour effectuer la tokenization d'une chaîne de texte en mots.\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texte = \"La tokenisation est une étape importante dans le traitement du langage naturel.\"\n",
    "\n",
    "mots = word_tokenize(texte, language='french')\n",
    "```\n",
    "\n",
    "`mots` sera alors une liste de mots dans la chaîne de texte.\n",
    "\n",
    "N'oubliez pas que la tokenization est une tâche complexe et peut nécessiter des ajustements en fonction du texte que vous traitez. Il est toujours bon de vérifier et de peaufiner les résultats en fonction de vos besoins spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa7adf59",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1696094979424,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "aa7adf59"
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5610e5",
   "metadata": {
    "id": "bb5610e5"
   },
   "outputs": [],
   "source": [
    "# Tokenisation des paragraphes/phrases\n",
    "import nltk\n",
    "# nltk.download(\"popular\") # utilisez ceci pour télécharger toutes les bibliothèques populaires dans nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e32b8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696095035908,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "03e32b8d",
    "outputId": "2c004460-1c61-4f5e-c7b7-5e827f490d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 100th Etext file presented by Project Gutenberg, and\n",
      "is presented in cooperation with World Library, Inc., from their\n",
      "Library of the Future and Shakespeare CDROMS.  Project Gutenberg\n",
      "often releases Etexts that are NOT placed in the Public Domain!!\n",
      "\n",
      "Shakespeare\n",
      "\n",
      "*This Etext has certain co\n"
     ]
    }
   ],
   "source": [
    "with open('shakespeare.txt') as f:\n",
    "\n",
    "    contents = f.read()\n",
    "    print(contents[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b426bf93",
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1696095038906,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "b426bf93"
   },
   "outputs": [],
   "source": [
    "# Phrases tokenisées\n",
    "sentences = nltk.sent_tokenize(contents[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "815b0233",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1696095094390,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "815b0233",
    "outputId": "df643fe8-4c9b-469a-b853-ce9d9145a8b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "iiMGukAXJSjl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1696095091003,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "iiMGukAXJSjl",
    "outputId": "65a7d7ec-8944-4b78-89e3-a5a630b4584f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Project Gutenberg\\noften releases Etexts that are NOT placed in the Public Domain!!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f83a5fa6",
   "metadata": {
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1696095112368,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "f83a5fa6"
   },
   "outputs": [],
   "source": [
    "# Mots tokenisés\n",
    "words = nltk.word_tokenize(contents[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f33a539e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696095113429,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "f33a539e",
    "outputId": "a4a61a3b-859f-4ceb-f55e-b7e67f0855ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', '100th', 'Etext', 'file', 'presented', 'by', 'Project', 'Gutenberg', ',', 'and', 'is', 'presented', 'in', 'cooperation', 'with', 'World', 'Library', ',', 'Inc.', ',', 'from', 'their', 'Library', 'of', 'the', 'Future', 'and', 'Shakespeare', 'CDROMS', '.', 'Project', 'Gutenberg', 'often', 'releases', 'Etexts', 'that', 'are', 'NOT', 'placed', 'in', 'the', 'Public', 'Domain', '!', '!', 'Shakespeare', '*', 'This', 'Etext', 'has', 'certain', 'copyright', 'implications', 'you', 'should', 'read', '!', '*', '<', '<', 'THIS', 'ELECTRONIC', 'VERSION', 'OF', 'THE', 'COMPLETE', 'WORKS', 'OF', 'WILLIAM', 'SHAKESPEARE', 'IS', 'COPYRIGHT', '1990-1993', 'BY', 'WORLD', 'LIBRARY', ',', 'INC.', ',', 'AND', 'IS', 'PROVIDED', 'BY', 'PROJECT', 'GUTENBERG', 'ETEXT', 'OF', 'ILLINOIS', 'BENEDICTINE', 'COLLEGE', 'WITH', 'PERMISSION', '.', 'ELECTRONIC', 'AND', 'MACHINE', 'READABLE', 'COPIES']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1958, None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d5f0bc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696095113429,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "2d5f0bc9",
    "outputId": "c3e1fec5-363e-4fc8-8c19-dacee2bca14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', '100th', 'etext', 'file', 'presented', 'by', 'project', 'gutenberg', '', 'and', 'is', 'presented', 'in', 'cooperation', 'with', 'world', 'library', '', 'inc', '', 'from', 'their', 'library', 'of', 'the', 'future', 'and', 'shakespeare', 'cdroms', '', 'project', 'gutenberg', 'often', 'releases', 'etexts', 'that', 'are', 'not', 'placed', 'in', 'the', 'public', 'domain', '', '', 'shakespeare', '', 'this', 'etext', 'has', 'certain', 'copyright', 'implications', 'you', 'should', 'read', '', '', '', '', 'this', 'electronic', 'version', 'of', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'is', 'copyright', '19901993', 'by', 'world', 'library', '', 'inc', '', 'and', 'is', 'provided', 'by', 'project', 'gutenberg', 'etext', 'of', 'illinois', 'benedictine', 'college', 'with', 'permission', '', 'electronic', 'and', 'machine', 'readable', 'copies']\n"
     ]
    }
   ],
   "source": [
    "stripped = [re_punc.sub('', w.lower()) for w in words]\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97dcca",
   "metadata": {
    "id": "ae97dcca"
   },
   "source": [
    "spaCy\n",
    "\n",
    "1. **Installer spaCy** : Si vous ne l'avez pas déjà installé, vous pouvez le faire en exécutant la commande suivante dans votre terminal :\n",
    "\n",
    "```python\n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "2. **Télécharger le modèle en français** : spaCy propose des modèles de traitement du langage naturel pour différentes langues. Vous pouvez télécharger le modèle en français avec la commande suivante :\n",
    "\n",
    "```python\n",
    "python -m spacy download fr_core_news_sm\n",
    "```\n",
    "\n",
    "3. **Importer spaCy** : Dans votre script Python, importez spaCy.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "```\n",
    "\n",
    "4. **Charger le modèle** : Chargez le modèle en français que vous venez de télécharger.\n",
    "\n",
    "```python\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "```\n",
    "\n",
    "5. **Utiliser la tokenisation** : Utilisez le modèle pour effectuer la tokenisation.\n",
    "\n",
    "```python\n",
    "texte = \"La tokenisation est une étape importante dans le traitement du langage naturel.\"\n",
    "\n",
    "doc = nlp(texte)\n",
    "\n",
    "# Accédez aux tokens\n",
    "mots = [token.text for token in doc]\n",
    "```\n",
    "\n",
    "`mots` contiendra une liste de mots dans la chaîne de texte.\n",
    "\n",
    "spaCy va plus loin en fournissant également d'autres informations sur les tokens, telles que le lemme, la partie du discours, etc., qui peuvent être très utiles dans des tâches de traitement du langage naturel plus avancées.\n",
    "\n",
    "N'oubliez pas d'installer le modèle approprié en fonction de la langue que vous souhaitez traiter. Dans cet exemple, nous avons utilisé `fr_core_news_sm` pour le français, mais spaCy propose également des modèles pour d'autres langues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5c5e9",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1696095022704,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "8dc5c5e9"
   },
   "outputs": [],
   "source": [
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8003a915",
   "metadata": {
    "executionInfo": {
     "elapsed": 13928,
     "status": "ok",
     "timestamp": 1696095144058,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "8003a915"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dKs7sQyFNmwr",
   "metadata": {
    "id": "dKs7sQyFNmwr"
   },
   "outputs": [],
   "source": [
    "are,is -> be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a214c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1696095144059,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -120
    },
    "id": "5a214c49",
    "outputId": "b9a7591c-15db-43be-fdd2-a4891b63f30a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John \t PROPN \t 11174346320140919546 \t John\n",
      "Adam \t PROPN \t 14264057329400597350 \t Adam\n",
      "is \t AUX \t 10382539506755952630 \t be\n",
      "one \t NUM \t 17454115351911680600 \t one\n",
      "the \t DET \t 7425985699627899538 \t the\n",
      "researcher \t NOUN \t 1317581537614213870 \t researcher\n",
      "who \t PRON \t 3876862883474502309 \t who\n",
      "invent \t VERB \t 5373681334090504585 \t invent\n",
      "the \t DET \t 7425985699627899538 \t the\n",
      "direction \t NOUN \t 895834437038626927 \t direction\n",
      "of \t ADP \t 886050111519832510 \t of\n",
      "way \t NOUN \t 6878210874361030284 \t way\n",
      "towards \t ADP \t 9315050841437086371 \t towards\n",
      "success \t NOUN \t 16089821935113899987 \t success\n",
      "! \t PUNCT \t 17494803046312582752 \t !\n"
     ]
    }
   ],
   "source": [
    "var1 = nlp(u\"John Adam is one the researcher who invent the direction of way towards success!\")\n",
    "\n",
    "for token in var1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
