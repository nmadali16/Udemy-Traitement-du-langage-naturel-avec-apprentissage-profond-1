{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP08KoX3ZlrIc1mtWTvLkJz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mécanisme d'attention\n","En traitement du langage naturel (NLP), l'attention fait référence à un mécanisme permettant à un modèle de se concentrer sur des parties spécifiques de l'entrée lors de la prise de décision. Cela est particulièrement important dans les tâches de compréhension de texte, de traduction automatique, et d'autres applications NLP.\n","\n","L'attention dans NLP est souvent réalisée à l'aide de mécanismes tels que l'attention basée sur les transformateurs, qui ont révolutionné de nombreuses tâches de NLP.\n","\n","L'équation de l'attention dans le contexte des transformers peut être exprimée en utilisant LaTeX comme suit :\n","\n","$$\n","\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{{QK^T}}{\\sqrt{d_k}}\\right) V\n","$$\n","\n","Dans cette équation :\n","\n","- $Q$ représente la matrice de requête (Query).\n","- $K$ représente la matrice de clé (Key).\n","- $V$ représente la matrice de valeur (Value).\n","- $\\text{softmax}$ est la fonction de softmax qui normalise les scores d'attention.\n","- $d_k$ est la dimension de la clé.\n","\n","Cette équation calcule les poids d'attention pour chaque paire de requête-clé. Ces poids sont utilisés pour pondérer les valeurs, donnant ainsi une représentation pondérée des informations.\n","\n","Cela permet au modèle d'apprendre à se concentrer sur différentes parties de l'entrée en fonction de la tâche à accomplir, améliorant ainsi les performances dans de nombreuses applications NLP."],"metadata":{"id":"ec25cjRDYeZY"}},{"cell_type":"markdown","source":["\n","Explication :\n","\n","1. Nous définissons une classe `ModuleAttention` personnalisée qui hérite de `nn.Module`. Ce module contient trois couches linéaires (`self.W_q`, `self.W_k`, et `self.W_v`) qui seront utilisées pour transformer les matrices de requête, de clé et de valeur en entrée.\n","\n","2. Dans la méthode `forward`, nous appliquons les transformations linéaires aux matrices d'entrée `Q`, `K`, et `V` pour obtenir `q`, `k`, et `v`.\n","\n","3. Ensuite, nous calculons les scores d'attention, les poids d'attention et la sortie finale en utilisant la même procédure que précédemment.\n","\n","4. Dans le code principal, nous générons des données aléatoires (`Q`, `K`, et `V`) et créons une instance de `ModuleAttention` avec des dimensions d'entrée et cachées spécifiées.\n","\n","5. Nous appliquons le mécanisme d'attention en appelant le module avec `Q`, `K`, et `V`.\n","\n","6. Enfin, nous imprimons l'entrée (`Q`) et la sortie."],"metadata":{"id":"VLD47FxyZXTG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"k26b2TAsZjIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","class ModuleAttention(nn.Module):\n","    def __init__(self, dim_entree, dim_cachee):\n","        super(ModuleAttention, self).__init__()\n","\n","        self.W_q = nn.Linear(dim_entree, dim_cachee, bias=False)\n","        self.W_k = nn.Linear(dim_entree, dim_cachee, bias=False)\n","        self.W_v = nn.Linear(dim_entree, dim_cachee, bias=False)\n","\n","    def forward(self, X):\n","        q = self.W_q(X)\n","        k = self.W_k(X)\n","        v = self.W_v(X)\n","\n","        scores = torch.matmul(q, k.transpose(-2, -1)) / (K.size(-1)** 0.5)\n","        poids_attention = F.softmax(scores, dim=-1)\n","        sortie = torch.matmul(poids_attention, v)\n","\n","        return sortie\n"],"metadata":{"id":"XD53G_h0ZdWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ndTybuOIYTdH","executionInfo":{"status":"ok","timestamp":1698515620175,"user_tz":-120,"elapsed":8,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"adc91851-b4f5-4ff4-ffd9-17c683a9d78b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entrée (Q): tensor([[[-0.5295,  2.0963, -1.9470, -1.1883,  1.5356],\n","         [ 0.3852, -0.5441, -1.6336, -0.7697, -0.6062],\n","         [-0.8084, -0.4302, -0.9308, -1.2673,  2.9480],\n","         [ 1.5320, -1.8932, -0.2014, -0.6770,  0.0478]],\n","\n","        [[-0.1531, -0.2689,  0.0730,  0.5146, -0.2539],\n","         [ 0.6599,  0.1553, -0.1086, -0.0092,  2.1382],\n","         [-0.6784, -0.3757,  0.7186, -0.6309,  0.0193],\n","         [ 0.2035,  2.0154,  2.2795,  1.7233, -0.1711]],\n","\n","        [[ 1.3183, -0.5741,  1.3496, -0.3333, -0.2220],\n","         [ 1.1988, -0.6285,  0.9968, -1.4363, -1.6455],\n","         [-1.5933, -0.2748, -0.5862,  1.6317,  0.6865],\n","         [-0.1581,  0.9783, -0.0180,  0.4559, -2.3155]]])\n","Sortie: tensor([[[ 0.2254, -0.2288, -0.6786,  0.3408,  0.2478, -0.9811, -0.1463,\n","           0.9622,  0.1015, -0.4047],\n","         [ 0.2450, -0.1526, -0.9531,  0.2715,  0.4129, -1.1471, -0.1552,\n","           1.0772,  0.3018, -0.7277],\n","         [-0.0146, -0.2051, -0.5113,  0.2539,  0.1269, -0.3210,  0.1744,\n","           0.4662, -0.4005, -0.1045],\n","         [ 0.1963, -0.1503, -0.9108,  0.2564,  0.3841, -1.0092, -0.0906,\n","           0.9739,  0.1950, -0.6579]],\n","\n","        [[ 0.0535,  0.0549,  0.2856, -0.0808,  0.3263,  0.3397,  0.0106,\n","          -0.4186,  0.2199, -0.2505],\n","         [ 0.0072,  0.0815,  0.0717, -0.1058,  0.2261,  0.2074,  0.0361,\n","          -0.2603,  0.1665, -0.2355],\n","         [ 0.0705,  0.0246,  0.1093, -0.0547,  0.3837,  0.2114,  0.0536,\n","          -0.2744,  0.1695, -0.3114],\n","         [-0.0795,  0.1561,  0.0020, -0.1780,  0.0068,  0.1930,  0.0242,\n","          -0.2275,  0.1434, -0.1196]],\n","\n","        [[-0.1458,  0.0847,  0.3300, -0.1002, -0.2922,  0.3997,  0.0178,\n","          -0.3593, -0.1150,  0.3212],\n","         [-0.0634,  0.1769, -0.1309, -0.3752, -0.1405,  0.1816,  0.2399,\n","          -0.1810, -0.1347,  0.2707],\n","         [-0.2765, -0.0202,  0.8564,  0.2450, -0.4585,  0.6945, -0.2399,\n","          -0.5927, -0.0996,  0.3245],\n","         [-0.0713,  0.1272,  0.1486, -0.2503, -0.2769,  0.2369,  0.0931,\n","          -0.2345, -0.1027,  0.3886]]], grad_fn=<UnsafeViewBackward0>)\n"]}],"source":["\n","# Générer des données aléatoires\n","X = torch.randn(3, 4, 5)  # Taille de lot de 3, 4 requêtes, chacune avec une dimension de 5\n","\n","\n","# Créer le module d'attention\n","module_attention = ModuleAttention(dim_entree=5, dim_cachee=10)\n","\n","# Appliquer le mécanisme d'attention\n","sortie = module_attention(X)\n","\n","print(\"Entrée (X):\", X)\n","print(\"Sortie:\", sortie)\n"]}]}